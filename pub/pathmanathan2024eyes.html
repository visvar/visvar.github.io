<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=100%, initial-scale=1">
    
    <title>Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks</title>
    
    <link rel="stylesheet" href="../style.css">
    <link rel="shortcut icon" href="../assets/img/misc/favicon.png">
    <link rel="icon" type="image/png" href="../assets/img/misc/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../assets/img/misc/favicon.png">

    <!-- OG Metadata -->
    <meta property="og:site_name" content="HCI VISUS" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="The HCI Research Group in Stuttgart" />
    <meta property="og:url" content="https://visvar.github.io/" />
    <meta property="og:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta property="og:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
    <meta property="og:locale" content="EN" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title"  content="The HCI Research Group in Stuttgart" />
    <meta name="twitter:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta name="twitter:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
  </head>
  
    <body>
      <main>
        
  <div>
  <header>
    <div>
      <a href="../index.html">
        <img class="logo" src="../assets/img/misc/hci.svg" />
      </a>
    </div>
    <div>
      <nav>
        <ul>
          <li><a href="../index.html">home</a></li>
        </ul>
      </nav>
    </div>
  </header>
</div>
        <div>
          <article>
            <h1>Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks</h1>
            <div class="pubPageContent">
              
              <a href="../assets/img/teaser/pathmanathan2024eyes.png" target="_blank" title="show image full size">
                <img class="teaser" id="imagepathmanathan2024eyes" src="../assets/img/teaser/pathmanathan2024eyes.png"/>
              </a>
              <div>
                <div class="authors">
                  <b>Authors.</b> Nelusa Pathmanathan, <a href="../members/tobias_rau.html" target="_blank">Tobias Rau</a>, Xiliu Yang, <a href="../members/aimee_sousa_calepso.html" target="_blank">Aimée Sousa Calepso</a>, Felix Amtsberg, Achim Menges, <a href="../members/michael_sedlmair.html" target="_blank">Michael Sedlmair</a>, Kuno Kurzhals
                </div>
                  
                <div>
                  <b>Venue.</b> VR (2024)
                </div>
                <div class="materials">
                  <b>Materials.</b>
                  <a href="https://doi.org/10.1109/VR58804.2024.00098" target="_blank" rel="noreferrer">DOI</a>
                  
                  
                  <a href="../assets/pdf/pathmanathan2024eyes.pdf" target="_blank" rel="noreferrer">PDF</a>
                  
                  
                  
                </div>
                <div class="abstract"><b>Abstract.</b> The use of augmented reality technology to support humans with situated visualization in complex tasks such as navigation or assembly has gained increasing importance in research and industrial applications. One important line of research regards supporting and understanding collaborative tasks. Analyzing collaboration patterns is usually done by conducting observations and interviews. To expand these methods, we argue that eye tracking can be used to extract further insights and quantify behavior. To this end, we contribute a study that uses eye tracking to investigate participant strategies for solving collaborative sorting and assembly tasks. We compare participants’ visual attention during situated instructions in AR and traditional paper-based instructions as a baseline. By investigating the performance and gaze behavior of the participants, different strategies for solving the provided tasks are revealed. Our results show that with situated visualization, participants focus more on task-relevant areas and require less discussion between collaboration partners to solve the task at hand.</div>
                <div class="bibtex"><textarea>@inproceedings{pathmanathan2024eyes,
    title         = {Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks},
    author        = {Nelusa Pathmanathan, Tobias Rau, Xiliu Yang, Aim\'{e}e Sousa Calepso, Felix Amtsberg, Achim Menges, Michael Sedlmair, Kuno Kurzhals},
    year          = {2024},
    month         = {4},
    booktitle     = {Conf. Virtual Reality and 3D User Interfaces},
    publisher     = {IEEE},
    pages         = {785--795},
    doi           = {https://doi.org/10.1109/VR58804.2024.00098},
}
</textarea></div>
                
                
                <div class="qrcontainer">
                  <div class="qrtitle">Link to this page:</div>
                  <img class="qrimage" src="../assets/img/qr/pathmanathan2024eyes.png"/>
                </div>
            </div>
          </article>
          
<div style="text-align: center">
  <a href="../imprint.html">Imprint / Legal Notice</a>
</div>

        </div>
      </main>
    </body>
    </html>