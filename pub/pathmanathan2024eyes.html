<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=100%, initial-scale=1">
    <title>Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks | HCI Stuttgart</title>
    <link rel="stylesheet" href="../style.css">
    <link rel="shortcut icon" href="../assets/img/misc/favicon.png">
    <link rel="icon" type="image/png" href="../assets/img/misc/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../assets/img/misc/favicon.png">
  </head>
  
    <body>
      <a class="anchor" name="top"></a>
      <main>
        
<div>
  <header>
    <div>
      <a href="../index.html">
        <img class="logo" src="../assets/img/misc/hci.svg" />
      </a>
    </div>
    <div>
      <nav>
      <ul>
        <li><a href="../index.html#aboutus">about us</a></li>
        <li><a href="../index.html#members">members</a></li>
        <li><a href="../index.html#publications">publications</a></li>
      </ul>
      </nav>
    </div>
  </header>
</div>

        <div>
          <article><a class="anchor" name="publications"></a>
            <h1>Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks</h1>
            <div class="pubPageContent">
              
              <a href="../assets/img/teaser/pathmanathan2024eyes.png" target="_blank" title="show image full size">
                <img id="imagepathmanathan2024eyes" src="../assets/img/teaser/pathmanathan2024eyes.png"/>
              </a>
              <div>
                <div>
                  <b>Authors.</b> Nelusa Pathmanathan, Tobias Rau, Xiliu Yang, Aimée Sousa Calepso, Felix Amtsberg, Achim Menges, Michael Sedlmair, Kuno Kurzhals
                </div>
                <div>
                  <b>Venue.</b> <a href="../venue/vr.html" target="_blank" title="IEEE Virtual Reality Conference">VR</a> (2024) Full Paper
                </div>
                
                <div>
                  <b>Type.</b> Full Paper
                </div>
                
                <div>
                  <b>Materials.</b>
                  <a href="https://doi.org/10.1109/VR58804.2024.00098" target="_blank" rel="noreferrer">DOI</a>
                  
                  
                  
                  
                </div>
                <div class="abstract"><b>Abstract.</b> The use of augmented reality technology to support humans with situated visualization in complex tasks such as navigation or assembly has gained increasing importance in research and industrial applications. One important line of research regards supporting and understanding collaborative tasks. Analyzing collaboration patterns is usually done by conducting observations and interviews. To expand these methods, we argue that eye tracking can be used to extract further insights and quantify behavior. To this end, we contribute a study that uses eye tracking to investigate participant strategies for solving collaborative sorting and assembly tasks. We compare participants’ visual attention during situated instructions in AR and traditional paper-based instructions as a baseline. By investigating the performance and gaze behavior of the participants, different strategies for solving the provided tasks are revealed. Our results show that with situated visualization, participants focus more on task-relevant areas and require less discussion between collaboration partners to solve the task at hand.</div>
                <div class="bibtex"><textarea>@inproceedings{10494083,
    title        = {Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks},
    author       = {N. Pathmanathan and T. Rau and X. Yang and A. Calepso and F. Amtsberg and A. Menges and M. Sedlmair and K. Kurzhals},
    year         = {2024},
    month        = {mar},
    booktitle    = {2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
    publisher    = {IEEE Computer Society},
    pages        = {785--795},
    doi          = {10.1109/VR58804.2024.00098},
    abstract     = {The use of augmented reality technology to support humans with situated visualization in complex tasks such as navigation or assembly has gained increasing importance in research and industrial applications. One important line of research regards supporting and understanding collaborative tasks. Analyzing collaboration patterns is usually done by conducting observations and interviews. To expand these methods, we argue that eye tracking can be used to extract further insights and quantify behavior. To this end, we contribute a study that uses eye tracking to investigate participant strategies for solving collaborative sorting and assembly tasks. We compare participants' visual attention during situated instructions in AR and traditional paper-based instructions as a baseline. By investigating the performance and gaze behavior of the participants, different strategies for solving the provided tasks are revealed. Our results show that with situated visualization, participants focus more on task-relevant areas and require less discussion between collaboration partners to solve the task at hand.},
    keywords     = {visualization;three-dimensional displays;collaboration;gaze tracking;user interfaces;user experience;task analysis}
}
</textarea></div>
                
                
                <img class="qr" src="../assets/img/qr/pathmanathan2024eyes.png"/>
            </div>
          </article>
          
<div style="text-align: center">
  <a href="../imprint.html">Imprint / Legal Notice</a>
</div>

        </div>
      </main>
    </body>
    </html>