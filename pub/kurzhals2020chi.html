<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=100%, initial-scale=1">
    
    <title>A View on the Viewer: Gaze-Adaptive Captions for Videos</title>
    
    <link rel="stylesheet" href="../style.css">
    <link rel="shortcut icon" href="../assets/img/misc/favicon.png">
    <link rel="icon" type="image/png" href="../assets/img/misc/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../assets/img/misc/favicon.png">

    <!-- OG Metadata -->
    <meta property="og:site_name" content="HCI VISUS" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="The HCI Research Group in Stuttgart" />
    <meta property="og:url" content="https://visvar.github.io/" />
    <meta property="og:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta property="og:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
    <meta property="og:locale" content="EN" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title"  content="The HCI Research Group in Stuttgart" />
    <meta name="twitter:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta name="twitter:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
  </head>
  
    <body>
      <a class="anchor" name="top"></a>
      <main>
        
<div>
  <header>
    <div>
      <a href="../index.html">
        <img class="logo" src="../assets/img/misc/hci.svg" />
      </a>
    </div>
    <div>
      <nav>
      <ul>
        <li><a href="../index.html#aboutus">about us</a></li>
        <li><a href="../index.html#members">members</a></li>
        <li><a href="../index.html#publications">publications</a></li>
      </ul>
      </nav>
    </div>
  </header>
</div>

        <div>
          <article><a class="anchor" name="publications"></a>
            <h1>A View on the Viewer: Gaze-Adaptive Captions for Videos</h1>
            <div class="pubPageContent">
              
              <a href="../assets/img/teaser/kurzhals2020chi.png" target="_blank" title="show image full size">
                <img class="teaser" id="imagekurzhals2020chi" src="../assets/img/teaser/kurzhals2020chi.png"/>
              </a>
              <div>
                <div class="authors">
                  <b>Authors.</b> Kuno Kurzhals, Fabian Göbel, <a href="../members/katrin_angerbauer.html" target="_blank">Katrin Angerbauer</a>, <a href="../members/michael_sedlmair.html" target="_blank">Michael Sedlmair</a>, Martin Raubal
                </div>
                  
                <div>
                  <b>Venue.</b> CHI (2020)
                </div>
                <div class="materials">
                  <b>Materials.</b>
                  <a href="https://doi.org/10.1145/3313831.3376266" target="_blank" rel="noreferrer">DOI</a>
                  
                  
                  <a href="../assets/pdf/kurzhals2020chi.pdf" target="_blank" rel="noreferrer">PDF</a>
                  
                  
                  
                </div>
                <div class="abstract"><b>Abstract.</b> Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.</div>
                <div class="bibtex"><textarea>@inproceedings{kurzhals2020chi,
    title         = {A View on the Viewer: Gaze-Adaptive Captions for Videos},
    author        = {Kuno Kurzhals, Fabian G\"{o}bel, Katrin Angerbauer, Michael Sedlmair, Martin Raubal},
    year          = {2020},
    month         = {4},
    booktitle     = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
    location      = {Honolulu, HI, USA},
    publisher     = {Association for Computing Machinery},
    series        = {CHI '20},
    pages         = {1–12},
    doi           = {https://doi.org/10.1145/3313831.3376266},
    isbn          = {9781450367080},
    numpages      = {12},
    keywords      = {eye tracking, gaze input, gaze-responsive display, multimedia, subtitles, video captions},
}
</textarea></div>
                
                
                <div class="qrcontainer">
                  <div class="qrtitle">Link to this page:</div>
                  <img class="qrimage" src="../assets/img/qr/kurzhals2020chi.png"/>
                </div>
            </div>
          </article>
          
<div style="text-align: center">
  <a href="../imprint.html">Imprint / Legal Notice</a>
</div>

        </div>
      </main>
    </body>
    </html>