<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=100%, initial-scale=1">
    
    <title>Interpretable Visualizations of Deep Neural Networks for Domain Generation Algorithm Detection</title>
    
    <link rel="stylesheet" href="../style.css">
    <link rel="shortcut icon" href="../assets/img/misc/favicon.png">
    <link rel="icon" type="image/png" href="../assets/img/misc/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../assets/img/misc/favicon.png">

    <!-- OG Metadata -->
    <meta property="og:site_name" content="HCI VISUS" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="The HCI Research Group in Stuttgart" />
    <meta property="og:url" content="https://visvar.github.io/" />
    <meta property="og:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta property="og:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
    <meta property="og:locale" content="EN" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title"  content="The HCI Research Group in Stuttgart" />
    <meta name="twitter:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta name="twitter:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
  </head>
  
    <body>
      <main>
        
  <div>
  <header>
    <div>
      <a href="../index.html">
        <img class="logo" src="../assets/img/misc/hci.svg" />
      </a>
    </div>
    <div>
      <nav>
        <ul>
          <li><a href="../index.html">home</a></li>
        </ul>
      </nav>
    </div>
  </header>
</div>
        <div>
          <article>
            <h1>Interpretable Visualizations of Deep Neural Networks for Domain Generation Algorithm Detection</h1>
            <div class="pubPageContent">
              
              <a href="../assets/img/teaser/becker2020interpretable.png" target="_blank" title="show image full size">
                <img class="teaser" id="imagebecker2020interpretable" src="../assets/img/teaser/becker2020interpretable.png"/><span class='sr-only'>(opens in new tab)</span>
              </a>
              <div>
                <div class="authors">
                  <b>Authors.</b> <a href="../members/franziska_becker.html">Franziska Becker</a>, Arthur Drichel, Christoph Müller, Thomas Ertl
                </div>
                  
                <div>
                  <b>Venue.</b> VizSec (2020)
                </div>
                <div class="materials">
                  <b>Materials.</b>
                  <a href="https://doi.org/10.1109/VizSec51108.2020.00010" target="_blank" rel="noreferrer">DOI<span class='sr-only'>(opens in new tab)</span></a>
                  
                  
                  <a href="../assets/pdf/becker2020interpretable.pdf" target="_blank" rel="noreferrer">PDF<span class='sr-only'>(opens in new tab)</span></a>
                  
                  
                  
                </div>
                <div class="abstract"><b>Abstract.</b> Due to their success in many application areas, deep learning models have found wide adoption for many problems. However, their blackbox nature makes it hard to trust their decisions and to evaluate their line of reasoning. In the field of cybersecurity, this lack of trust and understanding poses a significant challenge for the utilization of deep learning models. Thus, we present a visual analytics system that provides designers of deep learning models for the classification of domain generation algorithms with understandable interpretations of their model. We cluster the activations of the model’s nodes and leverage decision trees to explain these clusters. In combination with a 2D projection, the user can explore how the model views the data at different layers. In a preliminary evaluation of our system, we show how it can be employed to better understand misclassifications, identify potential biases and reason about the role different layers in a model may play.</div>
                <div class="bibtex"><textarea>@inproceedings{becker2020interpretable,
    title         = {Interpretable Visualizations of Deep Neural Networks for Domain Generation Algorithm Detection},
    author        = {Franziska Becker, Arthur Drichel, Christoph M\"{u}ller, Thomas Ertl},
    year          = {2020},
    month         = {10},
    booktitle     = {2020 IEEE Symposium on Visualization for Cyber Security (VizSec)},
    publisher     = {IEEE},
    doi           = {https://doi.org/10.1109/VizSec51108.2020.00010},
    isbn          = {978-1-7281-8262-9},
}
</textarea></div>
                
                
                <div class="qrcontainer">
                  <div class="qrtitle">Link to this page:</div>
                  <img class="qrimage" src="../assets/img/qr/becker2020interpretable.png"/>
                </div>
            </div>
          </article>
          
<div style="text-align: center">
  <a href="../imprint.html">Imprint / Legal Notice</a>
</div>

        </div>
      </main>
    </body>
    </html>