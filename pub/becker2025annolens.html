<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=100%, initial-scale=1">
    
    <title>AnnoLens: Exploration and Annotation through Lens-Based Guidance</title>
    
    <link rel="stylesheet" href="../style.css">
    <link rel="shortcut icon" href="../assets/img/misc/favicon.png">
    <link rel="icon" type="image/png" href="../assets/img/misc/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../assets/img/misc/favicon.png">

    <!-- OG Metadata -->
    <meta property="og:site_name" content="HCI VISUS" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="The HCI Research Group in Stuttgart" />
    <meta property="og:url" content="https://visvar.github.io/" />
    <meta property="og:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta property="og:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
    <meta property="og:locale" content="EN" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title"  content="The HCI Research Group in Stuttgart" />
    <meta name="twitter:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta name="twitter:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
  </head>
  
    <body>
      <main>
        
  <div>
  <header>
    <div>
      <a href="../index.html">
        <img class="logo" src="../assets/img/misc/hci.svg" />
      </a>
    </div>
    <div>
      <nav>
        <ul>
          <li><a href="../index.html">home</a></li>
        </ul>
      </nav>
    </div>
  </header>
</div>
        <div>
          <article>
            <h1>AnnoLens: Exploration and Annotation through Lens-Based Guidance</h1>
            <div class="pubPageContent">
              
              <a href="../assets/img/teaser/becker2025annolens.png" target="_blank" title="show image full size">
                <img class="teaser" id="imagebecker2025annolens" src="../assets/img/teaser/becker2025annolens.png"/><span class='sr-only'>(opens in new tab)</span>
              </a>
              <div>
                <div class="authors">
                  <b>Authors.</b> <a href="../members/franziska_becker.html">Franziska Becker</a>, Steffen Koch, <a href="../members/tanja_blascheck.html">Tanja Blascheck</a>
                </div>
                  
                <div>
                  <b>Venue.</b> VIS (2025)
                </div>
                <div class="materials">
                  <b>Materials.</b>
                  <a href="https://doi.org/10.1109/VIS60296.2025.00054" target="_blank" rel="noreferrer">DOI<span class='sr-only'>(opens in new tab)</span></a>
                  
                  
                  <a href="../assets/pdf/becker2025annolens.pdf" target="_blank" rel="noreferrer">PDF<span class='sr-only'>(opens in new tab)</span></a>
                  
                  
                  
                </div>
                <div class="abstract"><b>Abstract.</b> Annotation is often a time-consuming but fruitful activity in data analysis contexts. The manual labor required to create useful annotations is a barrier that keeps users from documenting their analysis, especially intermediate results. To address the needs of exploration and annotation alike, we propose integrating annotation with lens-based interactions, combining both with guidance. We investigate the exploration-annotation requirement space, identifying challenges and extracting five design requirements for annotation in exploration contexts. Based on this investigation, we designed ANNOLENSâ€”a concrete instantiation of such a system that lets users explore and annotate dimensionality-reduced multivariate data. It employs a dual-lens approach for contrastive exploration, using guidance to steer users toward interesting data subsets and attributes. Annotation is directly integrated into the lenses, letting users quickly annotate hunches and discoveries. Automated merging and linking serve to simplify annotation management and reduce disruptions. In a pilot study, we conducted a preliminary evaluation of our approach, which indicated that users find it easy to annotate data and were able to incorporate their knowledge and unique perspective into the process. A free copy of this paper and all supplemental materials are available at https://osf.io/zpu6c/.</div>
                <div class="bibtex"><textarea>@inproceedings{becker2025annolens,
    title         = {AnnoLens: Exploration and Annotation through Lens-Based Guidance},
    author        = {Franziska Becker, Steffen Koch, Tanja Blascheck},
    year          = {2025},
    month         = {11},
    booktitle     = {2025 IEEE Visualization and Visual Analytics (VIS)},
    pages         = {241--245},
    doi           = {https://doi.org/10.1109/VIS60296.2025.00054},
}
</textarea></div>
                
                
                <div class="qrcontainer">
                  <div class="qrtitle">Link to this page:</div>
                  <img class="qrimage" src="../assets/img/qr/becker2025annolens.png"/>
                </div>
            </div>
          </article>
          
<div style="text-align: center">
  <a href="../imprint.html">Imprint / Legal Notice</a>
</div>

        </div>
      </main>
    </body>
    </html>