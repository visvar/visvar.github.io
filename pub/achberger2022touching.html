<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=100%, initial-scale=1">
    
    <title>Touching Data with PropellerHand</title>
    
    <link rel="stylesheet" href="../style.css">
    <link rel="shortcut icon" href="../assets/img/misc/favicon.png">
    <link rel="icon" type="image/png" href="../assets/img/misc/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../assets/img/misc/favicon.png">

    <!-- OG Metadata -->
    <meta property="og:site_name" content="HCI VISUS" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="The HCI Research Group in Stuttgart" />
    <meta property="og:url" content="https://visvar.github.io/" />
    <meta property="og:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta property="og:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
    <meta property="og:locale" content="EN" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title"  content="The HCI Research Group in Stuttgart" />
    <meta name="twitter:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta name="twitter:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
  </head>
  
    <body>
      <a class="anchor" name="top"></a>
      <main>
        
<div>
  <header>
    <div>
      <a href="../index.html">
        <img class="logo" src="../assets/img/misc/hci.svg" />
      </a>
    </div>
    <div>
      <nav>
      <ul>
        <li><a href="../index.html#aboutus">about us</a></li>
        <li><a href="../index.html#members">members</a></li>
        <li><a href="../index.html#publications">publications</a></li>
      </ul>
      </nav>
    </div>
  </header>
</div>

        <div>
          <article><a class="anchor" name="publications"></a>
            <h1>Touching Data with PropellerHand</h1>
            <div class="pubPageContent">
              
              <a href="../assets/img/teaser/achberger2022touching.png" target="_blank" title="show image full size">
                <img class="teaser" id="imageachberger2022touching" src="../assets/img/teaser/achberger2022touching.png"/>
              </a>
              <div>
                <div class="authors">
                  <b>Authors.</b> <a href="../members/alexander_achberger.html" target="_blank">Alexander Achberger</a>, <a href="../members/frank_heyen.html" target="_blank">Frank Heyen</a>, Kresimir Vidackovic, <a href="../members/michael_sedlmair.html" target="_blank">Michael Sedlmair</a>
                </div>
                  
                <div>
                  <b>Venue.</b> Journal of Visualization (2022)
                </div>
                <div class="materials">
                  <b>Materials.</b>
                  <a href="https://doi.org/10.1007/s12650-022-00859-2" target="_blank" rel="noreferrer">DOI</a>
                  
                  
                  <a href="../assets/pdf/achberger2022touching.pdf" target="_blank" rel="noreferrer">PDF</a>
                  
                  
                  
                </div>
                <div class="abstract"><b>Abstract.</b> Immersive analytics often takes place in virtual environments which promise the users immersion. To fulfill this promise, sensory feedback, such as haptics, is an important component, which is however not well supported yet. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our prototype through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality. Additionally, we conducted a second user study in the field of immersive visualization to investigate the potential benefits of PropellerHand there.</div>
                <div class="bibtex"><textarea>@article{achberger2022touching,
    title         = {Touching Data with PropellerHand},
    author        = {Alexander Achberger, Frank Heyen, Kresimir Vidackovic, Michael Sedlmair},
    year          = {2022},
    month         = {6},
    journal       = {Journal of Visualization},
    doi           = {https://doi.org/10.1007/s12650-022-00859-2},
    funding       = {Open Access funding enabled and organized by Projekt DEAL},
}
</textarea></div>
                <div class="abstract"><b>Acknowledgements.</b> Partially supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy – EXC 2120/1 – 390831618</div>
                
                <div class="qrcontainer">
                  <div class="qrtitle">Link to this page:</div>
                  <img class="qrimage" src="../assets/img/qr/achberger2022touching.png"/>
                </div>
            </div>
          </article>
          
<div style="text-align: center">
  <a href="../imprint.html">Imprint / Legal Notice</a>
</div>

        </div>
      </main>
    </body>
    </html>