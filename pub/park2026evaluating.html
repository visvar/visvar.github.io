<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=100%, initial-scale=1">
    
    <title>Evaluating Generative AI in the Lab: Methodological Challenges and Guidelines</title>
    
    <link rel="stylesheet" href="../style.css">
    <link rel="shortcut icon" href="../assets/img/misc/favicon.png">
    <link rel="icon" type="image/png" href="../assets/img/misc/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../assets/img/misc/favicon.png">

    <!-- OG Metadata -->
    <meta property="og:site_name" content="HCI VISUS" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="The HCI Research Group in Stuttgart" />
    <meta property="og:url" content="https://visvar.github.io/" />
    <meta property="og:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta property="og:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
    <meta property="og:locale" content="EN" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title"  content="The HCI Research Group in Stuttgart" />
    <meta name="twitter:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta name="twitter:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
  </head>
  
    <body>
      <main>
        
  <div>
  <header>
    <div>
      <a href="../index.html">
        <img class="logo" src="../assets/img/misc/hci.svg" />
      </a>
    </div>
    <div>
      <nav>
        <ul>
          <li><a href="../index.html">home</a></li>
        </ul>
      </nav>
    </div>
  </header>
</div>
        <div>
          <article>
            <h1>Evaluating Generative AI in the Lab: Methodological Challenges and Guidelines</h1>
            <div class="pubPageContent">
              
              <a href="../assets/img/teaser/park2026evaluating.png" target="_blank" title="show image full size">
                <img class="teaser" id="imagepark2026evaluating" src="../assets/img/teaser/park2026evaluating.png"/><span class='sr-only'>(opens in new tab)</span>
              </a>
              <div>
                <div class="authors">
                  <b>Authors.</b> <a href="../members/hyerim_park.html">Hyerim Park</a>, Khanh Huynh, Malin Eiband, Jeremy Dillmann, Sven Mayer, <a href="../members/michael_sedlmair.html">Michael Sedlmair</a>
                </div>
                  
                <div>
                  <b>Venue.</b> arXiv (2026)
                </div>
                <div class="materials">
                  <b>Materials.</b>
                  <a href="https://doi.org/10.48550/arXiv.2601.16740" target="_blank" rel="noreferrer">DOI<span class='sr-only'>(opens in new tab)</span></a>
                  
                  
                  <a href="../assets/pdf/park2026evaluating.pdf" target="_blank" rel="noreferrer">PDF<span class='sr-only'>(opens in new tab)</span></a>
                  
                  
                  
                </div>
                <div class="abstract"><b>Abstract.</b> Generative AI (GenAI) systems are inherently non-deterministic, producing varied outputs even for identical inputs. While this variability is central to their appeal, it challenges established HCI evaluation practices that typically assume consistent and predictable system behavior. Designing controlled lab studies under such conditions therefore remains a key methodological challenge. We present a reflective multi-case analysis of four lab-based user studies with GenAI-integrated prototypes, spanning conversational in-car assistant systems and image generation tools for design workflows. Through cross-case reflection and thematic analysis across all study phases, we identify five methodological challenges and propose eighteen practice-oriented recommendations, organized into five guidelines. These challenges represent methodological constructs that are either amplified, redefined, or newly introduced by GenAI's stochastic nature: (C1) reliance on familiar interaction patterns, (C2) fidelity-control trade-offs, (C3) feedback and trust, (C4) gaps in usability evaluation, and (C5) interpretive ambiguity between interface and system issues. Our guidelines address these challenges through strategies such as reframing onboarding to help participants manage unpredictability, extending evaluation with constructs such as trust and intent alignment, and logging system events, including hallucinations and latency, to support transparent analysis. This work contributes (1) a methodological reflection on how GenAI's stochastic nature unsettles lab-based HCI evaluation and (2) eighteen recommendations that help researchers design more transparent, robust, and comparable studies of GenAI systems in controlled settings.</div>
                <div class="bibtex"><textarea>@article{park2026evaluating,
    title         = {Evaluating Generative AI in the Lab: Methodological Challenges and Guidelines},
    author        = {Hyerim Park, Khanh Huynh, Malin Eiband, Jeremy Dillmann, Sven Mayer, Michael Sedlmair},
    year          = {2026},
    month         = {1},
    journal       = {Int. Conf. Intelligent User Interfaces},
    doi           = {https://doi.org/10.48550/arXiv.2601.16740},
    eprinttype    = {arXiv},
}
</textarea></div>
                
                
                <div class="qrcontainer">
                  <div class="qrtitle">Link to this page:</div>
                  <img class="qrimage" src="../assets/img/qr/park2026evaluating.png"/>
                </div>
            </div>
          </article>
          
<div style="text-align: center">
  <a href="../imprint.html">Imprint / Legal Notice</a>
</div>

        </div>
      </main>
    </body>
    </html>