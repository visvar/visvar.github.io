<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=100%, initial-scale=1">
    <title>Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study | HCI Stuttgart</title>
    <link rel="stylesheet" href="../style.css">
    <link rel="shortcut icon" href="../assets/img/misc/favicon.png">
    <link rel="icon" type="image/png" href="../assets/img/misc/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../assets/img/misc/favicon.png">
  </head>
  
    <body>
      <a class="anchor" name="top"></a>
      <main>
        
<div>
  <header>
    <div>
      <a href="../index.html">
        <img class="logo" src="../assets/img/misc/hci.svg" />
      </a>
    </div>
    <div>
      <nav>
      <ul>
        <li><a href="../index.html#aboutus">about us</a></li>
        <li><a href="../index.html#members">members</a></li>
        <li><a href="../index.html#publications">publications</a></li>
      </ul>
      </nav>
    </div>
  </header>
</div>

        <div>
          <article><a class="anchor" name="publications"></a>
            <h1>Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study</h1>
            <div class="pubPageContent">
              
              <a href="../assets/img/teaser/hube2022using.png" target="_blank" title="show image full size">
                <img id="imagehube2022using" src="../assets/img/teaser/hube2022using.png"/>
              </a>
              <div>
                <div>
                  <b>Authors.</b> Natalie Hube,  Kresimir Vidackovic, Michael Sedlmair
                </div>
                <div>
                  <b>Venue.</b> <a href="../venue/chi.html" target="_blank" title="Conference on Human Factors in Computing Systems">CHI</a> (2022) Late-Breaking Work
                </div>
                
                <div>
                  <b>Type.</b> Late-Breaking Work
                </div>
                
                <div>
                  <b>Materials.</b>
                  <a href="https://doi.org/10.1145/3491101.3519822" target="_blank" rel="noreferrer">DOI</a>
                  
                  <a href="../assets/pdf/hube2022using.pdf" target="_blank" rel="noreferrer">PDF</a>
                  
                  
                </div>
                <div class="abstract"><b>Abstract.</b> Virtual avatars are widely used for collaborating in virtual environments. Yet, often these avatars lack expressiveness to determine a state of mind. Prior work has demonstrated efective usage of determining emotions and animated lip movement through analyzing mere audio tracks of spoken words. To provide this information on a virtual avatar, we created a natural audio data set consisting of 17 audio fles from which we then extracted the underlying emotion and lip movement. To conduct a pilot study, we developed a prototypical system that displays the extracted visual parameters and then maps them on a virtual avatar while playing the corresponding audio fle. We tested the system with 5 participants in two conditions: (i) while seeing the virtual avatar only an audio fle was played. (ii) In addition to the audio fle, the extracted facial visual parameters were displayed on the virtual avatar. Our results suggest the validity of using additional visual parameters in the avatars face as it helps to determine emotions. We conclude with a brief discussion on the outcomes and their implications on future work.</div>
                <div class="bibtex"><textarea>@inproceedings{10.1145/3491101.3519822,
    title        = {Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study},
    author       = {Hube, Natalie and Vidackovic, Kresimir and Sedlmair, Michael},
    year         = {2022},
    booktitle    = {CHI Conference on Human Factors in Computing Systems Extended Abstracts},
    publisher    = {Association for Computing Machinery},
    series       = {CHI EA '22},
    doi          = {https://doi.org/10.1145/3491101.3519822},
    abstract     = {Virtual avatars are widely used for collaborating in virtual environments. Yet, often these avatars lack expressiveness to determine a state of mind. Prior work has demonstrated effective usage of determining emotions and animated lip movement through analyzing mere audio tracks of spoken words. To provide this information on a virtual avatar, we created a natural audio data set consisting of 17 audio files from which we then extracted the underlying emotion and lip movement. To conduct a pilot study, we developed a prototypical system that displays the extracted visual parameters and then maps them on a virtual avatar while playing the corresponding audio file. We tested the system with 5 participants in two conditions: (i) while seeing the virtual avatar only an audio file was played. (ii) In addition to the audio file, the extracted facial visual parameters were displayed on the virtual avatar. Our results suggest the validity of using additional visual parameters in the avatars' face as it helps to determine emotions. We conclude with a brief discussion on the outcomes and their implications on future work.},
    articleno    = {260},
    numpages     = {7},
    keywords     = {emotion, avatars, lip synchronization, virtual reality}
}
</textarea></div>
                
                
                <img class="qr" src="../assets/img/qr/hube2022using.png"/>
            </div>
          </article>
          
<div style="text-align: center">
  <a href="../imprint.html">Imprint / Legal Notice</a>
</div>

        </div>
      </main>
    </body>
    </html>