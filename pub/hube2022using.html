<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=100%, initial-scale=1">
    
    <title>Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study</title>
    
    <link rel="stylesheet" href="../style.css">
    <link rel="shortcut icon" href="../assets/img/misc/favicon.png">
    <link rel="icon" type="image/png" href="../assets/img/misc/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../assets/img/misc/favicon.png">

    <!-- OG Metadata -->
    <meta property="og:site_name" content="HCI VISUS" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="The HCI Research Group in Stuttgart" />
    <meta property="og:url" content="https://visvar.github.io/" />
    <meta property="og:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta property="og:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
    <meta property="og:locale" content="EN" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title"  content="The HCI Research Group in Stuttgart" />
    <meta name="twitter:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta name="twitter:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
  </head>
  
    <body>
      <a class="anchor" name="top"></a>
      <main>
        
<div>
  <header>
    <div>
      <a href="../index.html">
        <img class="logo" src="../assets/img/misc/hci.svg" />
      </a>
    </div>
    <div>
      <nav>
      <ul>
        <li><a href="../index.html#aboutus">about us</a></li>
        <li><a href="../index.html#members">members</a></li>
        <li><a href="../index.html#publications">publications</a></li>
      </ul>
      </nav>
    </div>
  </header>
</div>

        <div>
          <article><a class="anchor" name="publications"></a>
            <h1>Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study</h1>
            <div class="pubPageContent">
              
              <a href="../assets/img/teaser/hube2022using.png" target="_blank" title="show image full size">
                <img class="teaser" id="imagehube2022using" src="../assets/img/teaser/hube2022using.png"/>
              </a>
              <div>
                <div class="authors">
                  <b>Authors.</b> <a href="../members/natalie_hube.html" target="_blank">Natalie Hube</a>, Kresimir Vidackovic, <a href="../members/michael_sedlmair.html" target="_blank">Michael Sedlmair</a>
                </div>
                  
                <div>
                  <b>Venue.</b> CHI (2022)
                </div>
                <div class="materials">
                  <b>Materials.</b>
                  <a href="https://doi.org/10.1145/3491101.3519822" target="_blank" rel="noreferrer">DOI</a>
                  
                  
                  <a href="../assets/pdf/hube2022using.pdf" target="_blank" rel="noreferrer">PDF</a>
                  
                  
                  
                </div>
                <div class="abstract"><b>Abstract.</b> Virtual avatars are widely used for collaborating in virtual environments. Yet, often these avatars lack expressiveness to determine a state of mind. Prior work has demonstrated efective usage of determining emotions and animated lip movement through analyzing mere audio tracks of spoken words. To provide this information on a virtual avatar, we created a natural audio data set consisting of 17 audio fles from which we then extracted the underlying emotion and lip movement. To conduct a pilot study, we developed a prototypical system that displays the extracted visual parameters and then maps them on a virtual avatar while playing the corresponding audio fle. We tested the system with 5 participants in two conditions: (i) while seeing the virtual avatar only an audio fle was played. (ii) In addition to the audio fle, the extracted facial visual parameters were displayed on the virtual avatar. Our results suggest the validity of using additional visual parameters in the avatars face as it helps to determine emotions. We conclude with a brief discussion on the outcomes and their implications on future work.</div>
                <div class="bibtex"><textarea>@inproceedings{hube2022using,
    title         = {Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study},
    author        = {Natalie Hube,  Kresimir Vidackovic, Michael Sedlmair},
    year          = {2022},
    month         = {4},
    booktitle     = {CHI Conference on Human Factors in Computing Systems Extended Abstracts},
    location      = {New Orleans, LA, USA},
    publisher     = {ACM},
    series        = {CHI EA '22},
    doi           = {https://doi.org/10.1145/3491101.3519822},
    isbn          = {9781450391566},
    articleno     = {260},
    numpages      = {7},
}
</textarea></div>
                
                
                <div class="qrcontainer">
                  <div class="qrtitle">Link to this page:</div>
                  <img class="qrimage" src="../assets/img/qr/hube2022using.png"/>
                </div>
            </div>
          </article>
          
<div style="text-align: center">
  <a href="../imprint.html">Imprint / Legal Notice</a>
</div>

        </div>
      </main>
    </body>
    </html>