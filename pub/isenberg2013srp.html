<!DOCTYPE html>
  <html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=100%, initial-scale=1">
    
    <title>A Systematic Review on the Practice of Evaluating Visualization</title>
    
    <link rel="stylesheet" href="../style.css">
    <link rel="shortcut icon" href="../assets/img/misc/favicon.png">
    <link rel="icon" type="image/png" href="../assets/img/misc/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../assets/img/misc/favicon.png">

    <!-- OG Metadata -->
    <meta property="og:site_name" content="HCI VISUS" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="The HCI Research Group in Stuttgart" />
    <meta property="og:url" content="https://visvar.github.io/" />
    <meta property="og:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta property="og:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
    <meta property="og:locale" content="EN" />

    <!-- Twitter card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title"  content="The HCI Research Group in Stuttgart" />
    <meta name="twitter:description" content="All about the people and their research at the HCI group at VISUS, University of Stuttgart." />
    <meta name="twitter:image" content="https://visvar.github.io/assets/img/misc/hcivisus.png" />
  </head>
  
    <body>
      <main>
        
  <div>
  <header>
    <div>
      <a href="../index.html">
        <img class="logo" src="../assets/img/misc/hci.svg" />
      </a>
    </div>
    <div>
      <nav>
        <ul>
          <li><a href="../index.html">home</a></li>
        </ul>
      </nav>
    </div>
  </header>
</div>
        <div>
          <article>
            <h1>A Systematic Review on the Practice of Evaluating Visualization</h1>
            <div class="pubPageContent">
              
              <a href="../assets/img/teaser/isenberg2013srp.png" target="_blank" title="show image full size">
                <img class="teaser" id="imageisenberg2013srp" src="../assets/img/teaser/isenberg2013srp.png"/>
              </a>
              <div>
                <div class="authors">
                  <b>Authors.</b> Tobias Isenberg, Petra Isenberg, Jian Chen, <a href="../members/michael_sedlmair.html" target="_blank">Michael Sedlmair</a>, Torsten MÃ¶ller
                </div>
                  
                <div>
                  <b>Venue.</b> TVCG (2013)
                </div>
                <div class="materials">
                  <b>Materials.</b>
                  <a href="https://doi.org/10.1109/TVCG.2013.126" target="_blank" rel="noreferrer">DOI</a>
                  
                  
                  <a href="../assets/pdf/isenberg2013srp.pdf" target="_blank" rel="noreferrer">PDF</a>
                  <a href="http://tobias.isenberg.cc/VideosAndDemos/Isenberg2013SRP" target="_blank" rel="noreferrer">supplemental [link]</a>
                  
                  
                </div>
                <div class="abstract"><b>Abstract.</b> We present an assessment of the state and historic development of evaluation practices as reported in papers published at the IEEE Visualization conference. Our goal is to reflect on a meta-level about evaluation in our community through a systematic understanding of the characteristics and goals of presented evaluations. For this purpose we conducted a systematic review of ten years of evaluations in the published papers using and extending a coding scheme previously established by Lam et al. [2012]. The results of our review include an overview of the most common evaluation goals in the community, how they evolved over time, and how they contrast or align to those of the IEEE Information Visualization conference. In particular, we found that evaluations specific to assessing resulting images and algorithm performance are the most prevalent (with consistently 80-90% of all papers since 1997). However, especially over the last six years there is a steady increase in evaluation methods that include participants, either by evaluating their performances and subjective feedback or by evaluating their work practices and their improved analysis and reasoning capabilities using visual tools. Up to 2010, this trend in the IEEE Visualization conference was much more pronounced than in the IEEE Information Visualization conference which only showed an increasing percentage of evaluation through user performance and experience testing. Since 2011, however, also papers in IEEE Information Visualization show such an increase of evaluations of work practices and analysis as well as reasoning using visual tools. Further, we found that generally the studies reporting requirements analyses and domain-specific work practices are too informally reported which hinders cross-comparison and lowers external validity.</div>
                <div class="bibtex"><textarea>@article{isenberg2013srp,
    title         = {A Systematic Review on the Practice of Evaluating Visualization},
    author        = {Tobias Isenberg, Petra Isenberg, Jian Chen, Michael Sedlmair, Torsten M\"{o}ller},
    year          = {2013},
    month         = {10},
    journal       = {IEEE Trans. Visualization Computer Graphics},
    volume        = {19},
    number        = {12},
    pages         = {2818--2827},
    doi           = {https://doi.org/10.1109/TVCG.2013.126},
}
</textarea></div>
                
                
                <div class="qrcontainer">
                  <div class="qrtitle">Link to this page:</div>
                  <img class="qrimage" src="../assets/img/qr/isenberg2013srp.png"/>
                </div>
            </div>
          </article>
          
<div style="text-align: center">
  <a href="../imprint.html">Imprint / Legal Notice</a>
</div>

        </div>
      </main>
    </body>
    </html>