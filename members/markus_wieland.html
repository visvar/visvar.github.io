<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Markus Wieland, M.Sc. | VISVAR Research Group, University of Stuttgart</title>
  <link rel="stylesheet" href="../style.css">
  <script src="../script.js"></script>
  <link rel="shortcut icon" href="../img/favicon.png">
  <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
  <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
</head>
<body>
  <a class="anchor" name="top"></a>
  <main>
    
<div>
<header>
<div>
<a href="https://visvar.github.io/">
<h1 class="h1desktop"><div>VISVAR</div><div>Research</div><div>Group</div></h1>
<h1 class="h1mobile">VISVAR</h1>
</a>
</div>
<div>
<nav>
<ul>
<li><a href="https://visvar.github.io/#aboutus">about VISVAR</a></li>
<li><a href="https://visvar.github.io/#publications">publications</a></li>
<li class="memberNav"><a href="https://visvar.github.io/#members">members</a></li>
<ul class="memberNav">

<li><a href="https://visvar.github.io/members/michael_sedlmair.html">Michael Sedlmair</a></li>

<li><a href="https://visvar.github.io/members/quynh_quang_ngo.html">Quynh Quang Ngo</a></li>

<li><a href="https://visvar.github.io/members/aimee_sousa_calepso.html">Aimee Sousa Calepso</a></li>

<li><a href="https://visvar.github.io/members/alexander_achberger.html">Alexander Achberger</a></li>

<li><a href="https://visvar.github.io/members/frank_heyen.html">Frank Heyen</a></li>

<li><a href="https://visvar.github.io/members/jonas_haischt.html">Jonas Haischt</a></li>

<li><a href="https://visvar.github.io/members/katrin_angerbauer.html">Katrin Angerbauer</a></li>

<li><a href="https://visvar.github.io/members/markus_wieland.html">Markus Wieland</a></li>

<li><a href="https://visvar.github.io/members/melissa_reinelt.html">Melissa Reinelt</a></li>

<li><a href="https://visvar.github.io/members/natalie_hube.html">Natalie Hube</a></li>

<li><a href="https://visvar.github.io/members/nina_doerr.html">Nina Dörr</a></li>

<li><a href="https://visvar.github.io/members/rene_cutura.html">Rene Cutura</a></li>

<li><a href="https://visvar.github.io/members/ruben_bauer.html">Ruben Bauer</a></li>

<li><a href="https://visvar.github.io/members/sebastian_rigling.html">Sebastian Rigling</a></li>

<li><a href="https://visvar.github.io/members/simeon_rau.html">Simeon Rau</a></li>

<li><a href="https://visvar.github.io/members/tobias_rau.html">Tobias Rau</a></li>

<li><a href="https://visvar.github.io/members/xingyao_yu.html">Xingyao Yu</a></li>

</ul>
</ul>
</nav>
</div>
</header>
</div>
    <div>
      <article><a class="anchor" name="aboutus"></a>
        <h1>Markus Wieland, M.Sc.</h1>
        <div class="aboutMember">
          <div class="avatarAndBio">
            <img class="avatar" src="../img/people/markus_wieland.jpg" />
            <div class="bio">
    <p>
      My research focuses on the accessible design of VR for people with visual impairments.
      In the context of social VR spaces, I try to make non-verbal signals (e.g., eye contact, facial expressions, and gestures) perceivable during a conversation in VR for people with visual impairments via other sensory modalities.
      Of course, the substituted non-verbal signals must always be adapted to the context and be unobtrusive so that they do not override other sensory modalities of people with visual impairment in a conversation.
      This requires a user-centric approach to develop usable solutions.
      The overall goal of my research is always to increase the accessibility of VR.
    </p>
    </div>
          </div>
          <div class="furtherInfo">
            <div>
              <h2>Research Interests</h2>
              <ul>
                <li>HCI</li>
<li>Accessibility</li>
<li>Cognitive Ergonomics</li>
<li>Human Factors</li>
<li>VR/AR</li>
              </ul>
            </div>
            <div>
              <h2>Links</h2>
              <ul>
                <li><a href="https://www.visus.uni-stuttgart.de/en/institute/team/Wieland-00007/" target="_blank" rel="noreferrer">University of Stuttgart website</a></li>
<li><a href="https://scholar.google.de/citations?user=3Ya8r3gAAAAJ" target="_blank" rel="noreferrer">Google Scholar</a></li>
<li><a href="https://de.linkedin.com/in/markus-wieland" target="_blank" rel="noreferrer">LinkedIn</a></li>
              </ul>
            </div>
            
          </div>
        </div>
      </article>
      <article> <a class="anchor" name="publications"></a>
        <h1>Publications</h1>
        
  <h2 class="yearHeading">2022</h2>
  <div class="paper small" id="paperwieland2022towards">
    
      <img
        id="imagewieland2022towards"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperwieland2022towards', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/wieland2022towards.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperwieland2022towards', 'small'); toggleImageSize(imagewieland2022towards)"
        title="Click to show details"
      >
        Towards Inclusive Conversations in Virtual Reality for People with Visual Impairments<a class="anchor" name="wieland2022towards"></a>
      </h3>
      <div>
        Markus Wieland, Tonja Machulla
      </div>
      <div>
        MuC (2022) Workshop Paper
        <a href="https://visvar.github.io/pub/wieland2022towards.html" target="_blank">direct link</a>
        <a href="https://dx.doi.org/10.18420/muc2022-mci-ws11-467" target="_blank">DOI</a>
        
        <a href="../pdf/wieland2022towards.pdf" target="_blank">PDF</a>
        
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Current mainstream social Virtual Reality (VR) spaces pose barriers to the equal participation of people with visual impairments (PVI) in social interactions. At present, VR is first and primarily a visual medium with a strong emphasis on the visual design of the VR scene and the available avatars. If social communication cues, such as non-verbal communication, are available at all, they are often not provided in a form accessible to PVI. Such cues are essential in social interactions to successfully participate in social interactions and experience a conversation in VR as realistic. Here, we summarize previous research regarding specific requirements for social VR spaces to be accessible to PVIs. We describe how people with disabilities recognize and identify potential conversational partners and how non-verbal communication works between PVI and sighted people. Our goal was to provide an overview of valuable features that can be implemented for inclusive conversations in a social VR space.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{mci/Wieland2022,
    title        = {Towards Inclusive Conversations in Virtual Reality for People with Visual Impairments},
    author       = {Wieland, Markus AND Machulla, Tonja},
    year         = {2022},
    booktitle    = {Mensch und Computer 2022 - Workshopband},
    publisher    = {Gesellschaft f\"{u}r Informatik e.V.},
    doi          = {10.18420/muc2022-mci-ws11-467},
    editor       = {Marky, Karola AND Gr\"{u}nefeld, Uwe AND Kosch, Thomas}
}
</textarea></div>
      
    </div>
  </div>
  
  
  <div class="paper small" id="paperwieland2022nonverbal">
    
      <img
        id="imagewieland2022nonverbal"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperwieland2022nonverbal', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/wieland2022nonverbal.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperwieland2022nonverbal', 'small'); toggleImageSize(imagewieland2022nonverbal)"
        title="Click to show details"
      >
        Non-verbal Communication and Joint Attention Between People with and Without Visual Impairments: Deriving Guidelines for Inclusive Conversations in Virtual Realities<a class="anchor" name="wieland2022nonverbal"></a>
      </h3>
      <div>
        Markus Wieland, Lauren Thevin, Albrecht Schmidt, Tonja Machulla
      </div>
      <div>
        Lecture Notes in Computer Science (2022) Full Paper
        <a href="https://visvar.github.io/pub/wieland2022nonverbal.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1007/978-3-031-08648-9_34" target="_blank">DOI</a>
        
        <a href="../pdf/wieland2022nonverbal.pdf" target="_blank">PDF</a>
        
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">With the emergence of mainstream virtual reality (VR) platforms for social interactions, non-verbal communicative cues are increasingly being transmitted into the virtual environment. Since VR is primarily a visual medium, accessible VR solutions are required for people with visual impairments (PVI). However, existing propositions do not take into account social interactions, and therefore PVI are excluded from this type of experience. To address this issue, we conducted semi-structured interviews with eleven participants, seven of whom were PVI and four of whom were partners or close friends without visual impairments, to explore how non-verbal cues and joint attention are used and perceived in everyday social situations and conversations. Our goal was to provide guidelines for inclusive conversations in virtual environments for PVI. Our findings suggest that gaze, head direction, head movements, and facial expressions are important for both groups in conversations but often difficult to identify visually for PVI. From our findings, we provide concrete suggestions for the design of social VR spaces, inclusive to PVI.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1007/978-3-031-08648-9_34,
    title        = {Non-verbal Communication and Joint Attention Between People with and Without Visual Impairments: Deriving Guidelines for Inclusive Conversations in Virtual Realities},
    author       = {Wieland, Markus and Thevin, Lauren and Schmidt, Albrecht and Machulla, Tonja},
    year         = {2022},
    booktitle    = {Computers Helping People with Special Needs},
    publisher    = {Springer International Publishing},
    pages        = {295--304},
    abstract     = {With the emergence of mainstream virtual reality (VR) platforms for social interactions, non-verbal communicative cues are increasingly being transmitted into the virtual environment. Since VR is primarily a visual medium, accessible VR solutions are required for people with visual impairments (PVI). However, existing propositions do not take into account social interactions, and therefore PVI are excluded from this type of experience. To address this issue, we conducted semi-structured interviews with eleven participants, seven of whom were PVI and four of whom were partners or close friends without visual impairments, to explore how non-verbal cues and joint attention are used and perceived in everyday social situations and conversations. Our goal was to provide guidelines for inclusive conversations in virtual environments for PVI. Our findings suggest that gaze, head direction, head movements, and facial expressions are important for both groups in conversations but often difficult to identify visually for PVI. From our findings, we provide concrete suggestions for the design of social VR spaces, inclusive to PVI.}
}
</textarea></div>
      
    </div>
  </div>
  
  <h2 class="yearHeading">2018</h2>
  <div class="paper small" id="paperbejan2018a">
    
    <div class="metaData noImage">
      <h3
        onclick="toggleClass('paperbejan2018a', 'small'); toggleImageSize(imagebejan2018a)"
        title="Click to show details"
      >
        A Virtual Environment Gesture Interaction System for People with Dementia<a class="anchor" name="bejan2018a"></a>
      </h3>
      <div>
        Alexander Bejan, Markus Wieland, Patrizia Murko, Christophe Kunze
      </div>
      <div>
        DIS (2018) 
        <a href="https://visvar.github.io/pub/bejan2018a.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3197391.3205440" target="_blank">DOI</a>
        
        
        
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">As dementia will most likely become an impactful challenge for our future society, it is imperative to maintain the well-being of the diverse group of people with dementia (PwD). Thus, appropriate interventions that effectively trigger identity-stabilizing memories, and at the same time encourage sensorimotor activities, have to be designed and implemented. To that end, we present a novel natural user interface (NUI) system combined with a reminiscence-provoking virtual 3D environment (VE). With it, PwD can delve into memories while interacting with the VE over dementia-fitted gestures. The results of the preliminary evaluations are promising, as they show that most PwD get immersed and cheerfully engage in gesture interactions after a short settling-in period.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3197391.3205440,
    title        = {A Virtual Environment Gesture Interaction System for People with Dementia},
    author       = {Bejan, Alexander and Wieland, Markus and Murko, Patrizia and Kunze, Christophe},
    year         = {2018},
    booktitle    = {Proceedings of the 2018 ACM Conference Companion Publication on Designing Interactive Systems},
    publisher    = {ACM},
    series       = {DIS '18 Companion},
    pages        = {225–230},
    doi          = {10.1145/3197391.3205440},
    url          = {https://doi.org/10.1145/3197391.3205440},
    abstract     = {As dementia will most likely become an impactful challenge for our future society, it is imperative to maintain the well-being of the diverse group of people with dementia (PwD). Thus, appropriate interventions that effectively trigger identity-stabilizing memories, and at the same time encourage sensorimotor activities, have to be designed and implemented. To that end, we present a novel natural user interface (NUI) system combined with a reminiscence-provoking virtual 3D environment (VE). With it, PwD can delve into memories while interacting with the VE over dementia-fitted gestures. The results of the preliminary evaluations are promising, as they show that most PwD get immersed and cheerfully engage in gesture interactions after a short settling-in period.},
    numpages     = {6},
    keywords     = {reminiscence therapy, joyful, dementia, natural user interfaces, virtual environment, fun moments, memory triggering, gesture interaction, virtual reality}
}
</textarea></div>
      
    </div>
  </div>
  
      </article>
    </div>
  </main>
</body>
</html>