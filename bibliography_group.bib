@ARTICLE{Bauer2025qvis,
  author            = {Ruben Bauer, Quynh Quang Ngo, Guido Reina, Steffen Frey, Michael Sedlmair},
  journal           = {TVCG}, 
  title             = {QVis: Query-based Visual Analysis of Multiscale Patterns in Spatiotemporal Ensembles}, 
  year              = {2025},
  month             = {11},
  volume            = {},
  number            = {},
  pages             = {1-15},
  venue             = {TVCG},
  abstract          = {Understanding how dynamic patterns vary across large spatiotemporal ensembles is essential in many scientific domains. In fluid dynamics, for instance, researchers analyze how splash patterns in droplet impact experiments change with physical parameters such as fluid type or impact velocity. These experiments produce large volumes of data where patterns differ in size, shape, and duration, making manual analysis tedious and error-prone. Recently, interactive visualization approaches have been developed to assist analysis using learned similarity models for pattern-based querying. However, they assume fixed size inputs and only support single-pattern queries, thus limiting their effectiveness for multiscale, multi-pattern analysis and exploration of ensembles. In this paper, we present a visual anal ysis approach for the interactive exploration of spatiotemporal ensembles through multiscale pattern querying. Our approach extends an existing similarity model to support variable-sized patterns, allowing users to define queries by selecting examples directly on visualized data. Coordinated views enable interactive querying, comparison, and analysis of pattern occurrences and relate pattern occurrences to ensemble parameters. A guidance mechanism supports the user in finding underexplored regions. We demonstrate the utility of our approach on synthetic and real-world datasets. Domain expert feedback confirms that the approach is intuitive, easy to use, and effective for revealing parameter-pattern relationships.},
  doi               = {https://doi.org/10.1109/TVCG.2025.3629575}
}


@ARTICLE{Rigling2025Selection,
    author          = {Sebastian Rigling, Steffen Koch, Dieter Schmalstieg, Bruce H. Thomas, Michael Sedlmair},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    venue           = {TVCG},
    title           = {Selection at a Distance through a Large Transparent Touch Screen}, 
    year            = {2025},
    month           = {10},
    volume          = {},
    number          = {},
    pages           = {1-11},
    abstract        = {Large transparent touch screens (LTTS) have recently become commercially available. These displays have the potential for engaging Augmented Reality (AR) applications, especially in public and shared spaces. However, the interaction with objects in the real environment behind the display remains challenging: Users must combine pointing and touch input if they want to select objects at varying distances. There is a lot of work on wearable or mobile AR displays, but little on how users interact with LTTS. Our goal is to contribute to a better understanding of natural user interaction for these AR displays. To this end, we developed a prototype and evaluated different pointing techniques for selecting 12 physical targets behind an LTTS, with distances ranging from 6 to 401 cm. We conducted a user study with 16 participants and measured user preferences, performance, and behavior. We analyzed the change in accuracy depending on the target position and the selection technique used. Our fndings include: (a) Users naturally align the touch point with their line of sight for targets farther than 36 cm behind the LTTS. (b) This technique provides the lowest angular deviation compared to other techniques. (c) Some user close one eye to improve their performance. Our results help to improve future AR scenarios using LTTS systems.},
    doi             = {https://doi.org/10.1109/TVCG.2025.3616756}
}

@ARTICLE{Wong2025Gone,
    author          = {Pak Chung Wong, June Abbas, Chaomei Chen, Christopher Collins, Danyel Fisher, Chi-Wing Fu, Samuel J. Huskey, Andreas Kerren, Alan M. MacEachren, Mike Potel, Anthony C. Robinson, Michael Sedlmair},
    journal         = {IEEE Computer Graphics and Applications},
    title           = {Gone Too Soon, Remembered Always: Chris Weaver and the Power of Visual Thinking},
    year            = {2025},
    month           = {9},
    volume          = {45},
    number          = {05},
    pages           = {8-11},
    abstract        = {We are deeply saddened by the unexpected passing of Dr. Chris Weaver on the morning of Sunday, 29 June 2025. Chris was a valued and longtime member of the IEEE Computer Graphics and Applications (IEEE CG&A) editorial board and most recently served as Associate Editor-in-Chief of Special Issues. An associate professor in the School of Computer Science at the University of Oklahoma, Chris earned a Ph.D. in Computer Science from the University of Wisconsin-Madison. From 2005 to 2008, he worked as a research associate at the GeoVISTA Center at Penn State, where he cofounded the North-East Visualization and Analytics Center (NEVAC) and led the development of several award-winning visual analysis tools. His research combined visualization, human-computer interaction, databases, and data mining to support the exploration of complex data. Chris was in the prime of life, and his sudden passing has been especially difficult for all who knew and worked with him. His colleagues from IEEE CG&A, the visualization and visual analytics community, NEVAC, and the University of Oklahoma mourn his loss and honor the lasting impact he made on the field and on those around him. Figure 1 shows a portrait of Chris, whose warmth, brilliance, and generosity touched so many. The following tributes reflect the deep respect and admiration he inspired throughout his career.},
    doi             = {https://doi.ieeecomputersociety.org/10.1109/MCG.2025.3598531},
    publisher       = {IEEE},
    address         = {New York, NY, USA},
    venue           = {CG&A},
}

@article{chu2026What,
    author          = {Mengdi Chu, Zefeng Qiu, Meng Ling, Shuning Jiang, Robert S. Laramee, Michael Sedlmair, Jian Chen},
    title           = {What Makes a Visualization Image Complex?},
    year            = {2026},
    month           = {1},
    publisher       = {IEEE},
    address         = {New York, NY, USA},
    doi             = {https://doi.org/10.31219/osf.io/ypez4_v2},
    url2            = {https://osf.io/5xe8a},
    abstract        = {We investigate the perceived visual complexity (VC) in data visualizations using objective image-based metrics. We collected VC scores through a large-scale crowdsourcing experiment involving 349 participants and 1,800 visualization images. We then examined how these scores align with 12 image-based metrics spanning pixel-based and statistic-information-theoretic (clutter), color, shape, and our two new object-based metrics (meaningful-color-count (MeC) and text-to-ink ratio (TiR)). Our results show that both low-level edges and high-level elements affect perceived VC in visualization images; the number of corners and distinct colors are robust metrics across visualizations. Second, feature congestion, a statistical information-theoretic metric capturing color and texture patterns, is the strongest predictor of perceived complexity in visualizations rich in the same continuous color/texture stimuli; edge density effectively explains VC in node-link diagrams. Additionally, we observe a bell-curve effect for texts: increasing TiR initially reduces complexity, reaching an optimal point, beyond which further text increases VC. Our quantification model is also interpretable—enabling metric-based explanations—grounded in the VisComplexity2K dataset, bridging computational metrics with human perceptual responses. The preregistration is available at osf.io/5xe8a. osf.io/bdet6 has the dataset and analysis code.},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    articleno       = {},
    numpages        = {28},
    venue           = {TVCG},
    suppl           = {https://osf.io/bdet6},
}

@inproceedings{LuKrauter2025DemoVisring,
    author          = {Runze Liu, Christian Krauter, Taiting Lu, Mara Schulte, Alexander Achberger, Tanja Blascheck, Michael Sedlmair, Mahant Gowda},
    title           = {Demonstration of VisRing: A Display-Extended Smartring for Nano Visualizations},
    year            = {2025},
    month           = {9},
    publisher       = {ACM},
    address         = {New York, NY, USA},
    doi             = {https://doi.org/10.1145/3746058.3758997},
    url             = {https://www.cse.psu.edu/~mkg31/projects/visring},
    url2            = {https://github.com/ChristianKrauter/VisRing},
    abstract        = {We demonstrate VisRing, the first smartring incorporating a bendable 160 x 32 4-bit grayscale organic light-emitting diode display. VisRing stands out by displaying nano visualizations while maintaining a compact design and minimal weight of 6.6 g, with an overall cost of around $35. We exploit opportunities for a system-on-a-chip architecture to tightly integrate an inertial measurement unit, a photoplethysmograph sensor, a temperature sensor, Bluetooth, a microcontroller, and a display unit that spans 270° to 360°, depending on finger size. Our contributions include the hardware design and implementation of VisRing, along with a software library that supports visualizing various data types. A qualitative study with 12 participants demonstrated the comfort, likability, and social acceptance of VisRing's hardware and software. The participants liked the visualizations and found the ring lightweight, but also pointed out possible improvements. We will open-source VisRing hardware and software for further development of interesting usage scenarios. The demo will include a showcase of the VisRing system hardware assembly and hands-on and live testing of various VisRing applications, which include message delivery, time display, and health monitoring.},
    booktitle       = {Adjunct Proc. Symp. User Interface Software and Technology},
    articleno       = {32},
    numpages        = {4},
    video           = {https://www.youtube.com/embed/FBm_dOg69tY?si=9Lt0ZbAyRH5tQOc3},
    venue           = {UIST Adjunct},
    footnoteindices = {0,1},
    footnotetext    = {contributed equally}
}

@inproceedings{LuKrauter2025Visring,
    author          = {Taiting Lu, Christian Krauter, Runze Liu, Mara Schulte, Alexander Achberger, Tanja Blascheck, Michael Sedlmair, Mahant Gowda},
    title           = {VisRing: A Display-Extended Smartring for Nano Visualizations},
    year            = {2025},
    month           = {9},
    publisher       = {ACM},
    address         = {New York, NY, USA},
    doi             = {https://doi.org/10.1145/3746059.3747806},
    url             = {https://www.cse.psu.edu/~mkg31/projects/visring},
    url2            = {https://github.com/ChristianKrauter/VisRing},
    abstract        = {We introduce VisRing, the first smartring incorporating a bendable 160 x 32 4-bit grayscale organic light-emitting diode display. VisRing stands out by displaying nano visualizations while maintaining a compact design and minimal weight of 6.6&nbsp;g, with an overall cost of around $35. We exploit opportunities for a system-on-a-chip architecture to tightly integrate an inertial measurement unit, a photoplethysmograph sensor, a temperature sensor, Bluetooth, a microcontroller, and a display unit that spans 270° to 360° , depending on finger size. Our contributions include the hardware design and implementation of VisRing, along with a software library that supports visualizing various data types. A qualitative study with 12 participants demonstrated the comfort, likability, and social acceptance of VisRing's hardware and software. The participants liked the visualizations and found the ring lightweight, but also pointed out possible improvements. All materials are shared under an open-source license to enable the community to extend and improve VisRing.},
    booktitle       = {Proc. Symp. User Interface Software and Technology},
    articleno       = {111},
    video           = {https://www.youtube.com/embed/FBm_dOg69tY?si=9Lt0ZbAyRH5tQOc3},
    numpages        = {18},
    venue           = {UIST},
    footnoteindices = {0,1},
    footnotetext    = {contributed equally}
}

@article{Angerbauer2025Inclusive,
    title           = {Inclusive avatars in the Metaverse: learning from the lived experiences of people with disabilities},
    journal         = {The Journal of Strategic Information Systems},
    volume          = {34},
    number          = {4},
    pages           = {101935},
    year            = {2025},
    month           = {12},
    issn            = {0963-8687},
    venue           = {Journal Strategic Information Systems},
    doi             = {https://doi.org/10.1016/j.jsis.2025.101935},
    url             = {https://www.sciencedirect.com/science/article/pii/S0963868725000502},
    author          = {Katrin Angerbauer, H. Phoenix Van Wagoner, Ksenia Keplinger, Tim Halach, Jonas Vogelsang, Natalie Hube, Andria Smith, Michael Sedlmair},
    abstract        = {Immersive platforms like the Metaverse have gained attention in information systems (IS) research, yet the diverse needs of people with disabilities (PWD) remain underexplored. This research examines the experiences of PWD using inclusive avatars that represent disabilities. Through an exploratory mixed-methods approach, combining qualitative interviews with an experience sampling study, we develop a framework informed by Affective Events Theory and voices of PWD to better understand how social interactions in the Metaverse impact PWD’s emotions and outcomes. Findings suggest that when PWD use inclusive avatars, inclusive and exclusionary social interactions shape their emotional responses, which in turn influence engagement, avatar connection and satisfaction, and perceptions of inclusion in the Metaverse. Although adopting inclusive avatars can be challenging, especially in the face of exclusionary interactions, the benefits can outweigh the costs. The role of disability identity is critical; PWD who identify strongly with their disability experience less negative emotional impact from exclusion. This research contributes to IS literature by conceptualizing the Metaverse as a relational, emotion-driven environment shaped by social interactions as well as a platform for authentic self-representation. Practical implications include supporting avatar-based disability representation, involving PWD in co-designing virtual reality technologies, and providing training to foster inclusive interactions in the Metaverse. These strategies can help organizations build more inclusive and engaging digital workplaces for an often underrepresented workforce segment.}
}
@misc{lu2025design,
    title           = {A Design Space for Visualization Transitions of 3D Spatial Data in Hybrid AR-Desktop Environments}, 
    author          = {Yucheng Lu, Tobias Rau, Benjamin Lee, Andreas Köhn, Michael Sedlmair, Christian Sandor, Tobias Isenberg},
    year            = {2025},
    month           = {08},
    doi             = {https://doi.org/10.48550/arXiv.2506.22250},
    venue           = {arxiv},
    abstract        = {We present a design space for animated transitions of the appearance of 3D spatial datasets in a hybrid Augmented Reality (AR)-desktop context. Such hybrid interfaces combine both traditional and immersive displays to facilitate the exploration of 2D and 3D data representations in the environment in which they are best displayed. One key aspect is to introduce transitional animations that change between the different dimensionalities to illustrate the connection between the different representations and to reduce the potential cognitive load on the user. The specific transitions to be used depend on the type of data, the needs of the application domain, and other factors. We summarize these as a transition design space to simplify the decision-making process and provide inspiration for future designs. First, we discuss 3D visualizations from a spatial perspective: a spatial encoding pipeline, where 3D data sampled from the physical world goes through various transformations, being mapped to visual representations, and then being integrated into a hybrid AR-desktop environment. The transition design then focuses on interpolating between two spatial encoding pipelines to provide a smooth experience. To illustrate the use of our design space, we apply it to three case studies that focus on applications in astronomy, radiology, and chemistry; we then discuss lessons learned from these applications.},
    acks            = {We thank Ambre Assor for her feedback on our manuscipt. This work is partially funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy—EXC 2075–390740016. We also acknowledge the support by the Stuttgart Center for Simulation Science (SimTech).}
}
@misc{sousacalepso2025exoar,
    title           = {Exoskeletons and Augmented Reality: Opening Pathways to Improved Coordination in Collaborative Tasks}, 
    author          = {Aimée Sousa Calepso, Jan Kolberg, Enrique Bances, Braulio Garcia, Quynh Quang Ngo, Jörg Siegert, Urs Schneider, Thomas Bauernhansl, Michael Sedlmair},
    abstract        = {Exoskeletons are designed to enhance users' strength in physically demanding tasks, such as lifting and carrying heavy objects. Despite increasing physical ability, exoskeletons can introduce mechanical constraints on joint articulation, restricting certain movements. These limitations can reduce the range of motion, alter natural movement patterns, and decrease agility. This can particularly impair collaborative tasks that require movement coordination. Since industrial applications of exoskeletons also involve teamwork, it is important to find ways to support users in maintaining coordination and mitigating the side effects that are introduced with exoskeletons. To explore solutions to this problem, we developed a system that integrates AR-based motion guidance to assist users wearing exoskeletons in collaborative object-handling tasks. Our approach leverages immersive visualizations to facilitate coordination, assisting users in maintaining alignment and executing movements more smoothly. We conducted an exploratory study involving 40 participants, divided into pairs, to examine the feasibility and challenges of this approach. Our findings uncover key considerations for motion guidance evaluation in object collaborative handling tasks, the impact of the participants' pairing strategies, and technical challenges.},
    year            = {2025},
    month           = {09},
    publisher       = {Springer},
    pages           = {489-509},
    doi             = {https://doi.org/10.1007/978-3-032-04999-5_29},
    address         = {Berlin},
    booktitle       = {Human-Computer Interaction},
    venue           = {INTERACT},
}

@inproceedings{heyen2025demoguitarpie,
    title           = {Demonstrating GuitarPie: Using the Fretboard of an Electric Guitar for Audio-Based Pie Menu Interaction},
    author          = {Frank Heyen, Marius Labudda, Michael Sedlmair, Andreas Fender},
    year            = {2025},
    month           = {9},
    booktitle       = {Adjunct Proc. Symp. User Interface Software and Technology (UIST))},
    publisher       = {ACM},
    address         = {New York, NY, USA},
    venue           = {UIST Adjunct},
    doi             = {https://doi.org/10.1145/3746058.3758989},
    url             = {https://doi.org/10.1145/3746059.3747799},
    articleno       = {22},
    numpages        = {3},
    abstract        = {Digital tablature interfaces are a de-facto standard for electric guitar hobbyists, featuring capabilities such as playing back tablatures with configurable individual instrument tracks (e.g., drums). Such interfaces are typically controlled via mouse and keyboard or via touch input. Hence, learners often need to switch back and fourth between playing the guitar and using the input device(s) for controlling the interface. In this demonstration, we showcase our audio-based pie menu technique called GuitarPie, which takes the guitar’s audio signals as input for interface control. GuitarPie utilizes the grid-like structure of a fretboard for an intuitive spatial representation of audio-controlled operations. We demonstrate GuitarPie as an integral part of our tablature interface TabCtrl.},
    video           = {https://www.youtube.com/embed/ItJGNO-IQDw?si=ZgIH272jNhAh2_z9}
}
@inproceedings{heyen2025guitarpie,
    title           = {GuitarPie: Using the Fretboard of an Electric Guitar for Audio-Based Pie Menu Interaction},
    author          = {Frank Heyen, Marius Labudda, Michael Sedlmair, Andreas Fender},
    year            = {2025},
    month           = {9},
    booktitle       = {Proc. Symp. User Interface Software and Technology (UIST)},
    publisher       = {ACM},
    venue           = {UIST Adjunct},
    doi             = {https://doi.org/10.1145/3746059.3747799},
    articleno       = {16},
    numpages        = {13},
    venue           = {UIST},
    abstract        = {Nowadays, electric guitars are often used together with digital interfaces. For instance, tablature applications can support guitar practice by rendering and playing back the tabs of individual instrument tracks of a song (guitar, drums, etc.). However, those interfaces are typically controlled via mouse and keyboard or via touch input. This means that controlling and configuring playback during practice can lead to high switching costs, as learners often need to switch between playing and interface control. In this paper, we explore the use of audio input from an unmodified electric guitar to enable interface control without letting go of the guitar. We present GuitarPie, an audio-based pie menu interaction method. GuitarPie utilizes the grid-like structure of a fretboard to spatially represent audio-controlled operations, avoiding the need to memorize note sequences. Furthermore, we implemented TabCtrl, a tablature interface that uses GuitarPie and other audio-based interaction methods for interface control.},
    video           = {https://www.youtube.com/embed/ItJGNO-IQDw?si=ZgIH272jNhAh2_z9}
}
@article{yang2025implementation,
    title           = {An implementation and evaluation of large-scale multi-user human–robot collaboration with head-mounted augmented reality},
    author          = {Xiliu Yang, Felix Amtsberg, Benjamin Kaiser, Lior Skoury, Tim Stark, Simon Treml, Nils Opgenorth, Aimée Sousa Calepso, Michael Sedlmair, Thomas Wortmann, Alexander Verl, Achim Menges},
    year            = {2025},
    month           = {09},
    journal         = {Advanced Engineering Informatics},
    volume          = {67},
    pages           = {103475},
    doi             = {https://doi.org/10.1016/j.aei.2025.103475},
    issn            = {1474-0346},
    venue           = {Advanced Engineering Informatics},
    abstract        = {Human–robot collaboration (HRC) offers promising potential for more flexible and sustainable production practices in architecture and construction. This requires HRC setups to scale up from light-payload collaborative robots to conform with the scale of building construction while considering the safety and teamwork culture for workers. This research proposes a system for large-scale multi-user HRC using head-mounted augmented reality (AR) devices. To achieve this, we contribute three methods that work in conjunction: (1) an AR system that enables multiple users to share tasks and work together with robots; (2) a dynamic human task allocation engine that reacts to the changing production teams and task types; and (3) a safety zone generation and allocation method to configure human collaboration in shared space with large-scale robots. The system is evaluated using a case study of prefabricated timber cassettes combining discrete event simulations, a user study and a fabrication process demonstrator with an industry partner.}
}
@inproceedings{park2025design,
    title           = {Design and Evaluation of a Generative AI-Driven VR Texturing Tool: A Design Science Approach},
    author          = {Hyerim Park, Joscha Eirich, Andre Luckow, Michael Sedlmair},
    year            = {2025},
    month           = {06},
    booktitle       = {ECIS 2025 Proceedings},
    publisher       = {Association for Information Systems},
    series          = {ECIS},
    url             = {https://aisel.aisnet.org/ecis2025/hci/hci/2},
    articleno       = {2},
    numpages        = {17},
    keywords        = {Virtual Reality, Generative AI, 3D Texture Design, Human-Computer Interaction, Design Science Research},
    acks            = {We thank all user study participants and domain experts, as well as the anonymous reviewers, for their time and valuable input. In this work, we used ChatGPT-4 Omni to refine phrasing and enhance readability while ensuring that all content remained self-authored.},
    pdf             = {https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1152&context=ecis2025},
    venue           = {ECIS},
    abstract        = {The integration of generative AI (GenAI) with virtual reality (VR) offers new opportunities for the automotive industry to enhance customer engagement by enabling customizable textures for 3D car models and sharing them within VR. However, creating textures for VR platforms remains challenging due to the expertise required for traditional texturing tools and the limited customization options VR platforms provide. This study introduces GenVRTex, a GenAI-driven VR texturing tool that helps novice users generate high-quality textures through text prompts and drawings. Developed through a Design Science Research approach in collaboration with an industrial partner, GenVRTex incorporates ten design guidelines addressing user interaction, prompt control, and texture creation. A qualitative evaluation with nine designers inexperienced in VR texturing confirmed the tool's usability and effectiveness while highlighting potential improvements, including multimodal inputs and iterative GenAI input-output management. This research provides insights into GenAI-VR integration from an HCI perspective and outlines future research directions.}
}
@inproceedings{krieglstein2025a,
    title           = {A Hybrid User Interface Combining AR, Desktop, and Mobile Interfaces for Enhanced Industrial Robot Programming},
    author          = {Jan Krieglstein, Jan Kolberg, Aimée Sousa Calepso, Werner Kraus, Michael Sedlmair},
    year            = {2025},
    month           = {06},
    booktitle       = {International Conference on Automation and Robotics},
    pages           = {4738-4745},
    publisher       = {IEEE},
    video           = {https://www.youtube.com/embed/s1zqoQ_3H38?si=UMQiRa05nQiFm9oY},
    acks            = {This work was supported by the Ministry of Economic Affairs of the state Baden-Württemberg in Germany under AI Innovation Center (Learning Systems and Cognitive Robotics) – Grant No. 017-180036; by the Federal Ministry for the Environment, Nature Conservation, Nuclear Safety and Consumer Protection (BMUV) within the project ”Desire4Electronics” - Grant No. 67KI31054A; and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany´s Excellence Strategy – EXC 2120/1 – 390831618},
    venue           = {ICRA},
    doi             = {https://doi.org/10.1109/ICRA55743.2025.11128291},
    abstract        = {Robot programming for complex assembly tasks is challenging and demands expert knowledge. With Augmented Reality (AR), immersive 3D visualization can be placed in the robot’s intrinsic coordinate system to support robot programming. However, AR interfaces introduce usability challenges. To address these, we introduce a hybrid user interface (HUI) that combines a 2D desktop, a smartphone, and an AR head-mounted display (HMD) application, enabling operators to choose the most suitable device for each sub-task. The evaluation with an expert user study shows that an HUI can enhance efficiency and user experience by selecting the appropriate device for each sub-task. Generally, the HMD is preferred for tasks involving 3D content, the desktop for creating the program structure and parametrization, and the smartphone for mobile parametrization. However, the device selection depends on individual user characteristics and their familiarity with the devices.}
}
@article{bauer2025voronoi,
    title           = {Voronoi Cell Interface-Based Parameter Sensitivity Analysis for Labeled Samples},
    author          = {Ruben Bauer, Marina Evers, Quynh Quang Ngo, Guido Reina, Steffen Frey, Michael Sedlmair},
    year            = {2025},
    month           = {05},
    journal         = {Computer Graphics Forum},
    pages           = {e70122},
    note            = {© 2025 The Author(s). Computer Graphics Forum published by Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd. This is an open access article under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</a>, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.},
    doi             = {https://doi.org/10.1111/cgf.70122},
    keywords        = {CCS Concepts, • Human-centered computing → Information visualization, Visual analytics},
    acks            = {Funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC 2075 - 390740016, Project 327154368 - SFB 1313 (D01), and Project 251654672 – TRR 161 (A01, A08). We acknowledge the support of the Stuttgart Center for Simulation Science (SimTech). Open Access funding enabled and organized by Projekt DEAL.},
    funding         = {Open Access funding enabled and organized by Projekt DEAL.},
    venue           = {CGF},
    abstract        = {Abstract Varying the input parameters of simulations or experiments often leads to different classes of results. Parameter sensitivity analysis in this context includes estimating the sensitivity to the individual parameters, that is, to understand which parameters contribute most to changes in output classifications and for which parameter ranges these occur. We propose a novel visual parameter sensitivity analysis approach based on Voronoi cell interfaces between the sample points in the parameter space to tackle the problem. The Voronoi diagram of the sample points in the parameter space is first calculated. We then extract Voronoi cell interfaces which we use to quantify the sensitivity to parameters, considering the class label information of each sample's corresponding output. Multiple visual encodings are then utilized to represent the cell interface transitions and class label distribution, including stacked graphs for local parameter sensitivity. We evaluate the approach's expressiveness and usefulness with case studies for synthetic and real-world datasets.}
}
@inproceedings{park2025exploring,
    title           = {Exploring Visual Prompts: Refining Images with Scribbles and Annotations in Generative AI Image Tools},
    author          = {Hyerim Park, Malin Eiband, Andre Luckow, Michael Sedlmair},
    year            = {2025},
    month           = {04},
    booktitle       = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI EA '25},
    doi             = {https://doi.org/10.1145/3706599.3719802},
    isbn            = {9798400713958},
    url             = {https://arxiv.org/abs/2503.03398},
    abstract        = {Generative AI (GenAI) tools are increasingly integrated into design workflows. While text prompts remain the primary input method for GenAI image tools, designers often struggle to craft effective ones. Moreover, research has primarily focused on input methods for ideation, with limited attention to refinement tasks. This study explores designers' preferences for three input methods - text prompts, annotations, and scribbles - through a preliminary digital paper-based study with seven professional designers. Designers preferred annotations for spatial adjustments and referencing in-image elements, while scribbles were favored for specifying attributes such as shape, size, and position, often combined with other methods. Text prompts excelled at providing detailed descriptions or when designers sought greater GenAI creativity. However, designers expressed concerns about AI misinterpreting annotations and scribbles and the effort needed to create effective text prompts. These insights inform GenAI interface design to better support refinement tasks, align with workflows, and enhance communication with AI systems.},
    articleno       = {257},
    numpages        = {10},
    keywords        = {generative AI, text prompts, annotation-based input, scribble-based input, design refinement},
    venue           = {CHI}
}
@inproceedings{rigling2025reverse,
    title           = {Reverse Vampire UI: Reflecting on AR Interaction with Smart Mirrors},
    author          = {Sebastian Rigling, Ševal Avdić, Muhammed Enes Özver, Michael Sedlmair},
    year            = {2025},
    month           = {04},
    booktitle       = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI EA '25},
    doi             = {https://doi.org/10.1145/3706599.3719930},
    isbn            = {9798400713958},
    abstract        = {Mirror surfaces can be used as information displays in smart homes and even for augmented reality (AR). The big advantage is the seamless integration of the visual output into the user’s natural environment. However, user input poses a challenge. On the one hand, touch input would make the mirror dirty. On the other hand, mid-air gestures have proven to be less accurate, slower and more error-prone. We propose the use of an AR user interface (UI): Interactive UI elements are visible “on the other side of the mirror” and can be pressed by the user’s reflection. We built a functional prototype and investigated whether this is a viable option for interacting with mirrors. In a pilot study, we compared the interaction with UI elements placed on three different planes relative to the mirror surface: Behind the mirror (reflection), on the mirror (touch) and in front of the mirror (hologram).},
    articleno       = {463},
    numpages        = {7},
    keywords        = {augmented reality, mirror, interaction},
    venue           = {CHI}
}
@inproceedings{yang2025exploring,
    title           = {Exploring the Use of Augmented Reality for Multi-human-robot Collaboration with Industry Users in Timber Construction},
    author          = {Xiliu Yang, Nelusa Pathmanathan, Sarah Zabel, Felix Amtsberg, Siegmar Otto, Kuno Kurzhals, Michael Sedlmair, Achim Menges},
    year            = {2025},
    month           = {04},
    booktitle       = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI EA '25},
    doi             = {https://doi.org/10.1145/3706599.3720104},
    isbn            = {9798400713958},
    abstract        = {As robots are introduced into construction environments, situations may arise where construction workers without programming expertise need to interact with robotic operations to ensure smooth and successful task execution. We designed a head-mounted augmented reality (AR) system that allowed control of the robot’s tasks and motions during human-robot collaboration (HRC) in timber assembly tasks. To explore workers’ feedback and attitudes towards HRC with this system, we conducted a user study with 10 carpenters. The workers collaborated in pairs with a heavy-payload industrial robot to construct a 2 x 3 m timber panel. The study contributes an evaluation of multi-human-robot collaboration along with qualitative feedback from the workers. Exploratory data analysis revealed the influence of asymmetrical user roles in multi-user collaborative construction, providing research directions for future work.},
    articleno       = {272},
    numpages        = {8},
    pdf             = {https://dl.acm.org/doi/pdf/10.1145/3706599.3720104},
    venue           = {CHI}
}
@inproceedings{yang2025who,
    title           = {Who is in Control? Understanding User Agency in AR-assisted Construction Assembly},
    author          = {Xiliu Yang, Prasanth Sasikumar, Felix Amtsberg, Achim Menges, Michael Sedlmair, Suranga Nanayakkara},
    year            = {2025},
    month           = {04},
    booktitle       = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI '25},
    doi             = {https://doi.org/10.1145/3706598.3713765},
    isbn            = {9798400713941},
    abstract        = {Adaptive AR assistance can automatically trigger content to support users based on their context. Such intelligent automation offers many benefits but also alters users’ degree of control, which is seldom explored in existing research. In this paper, we compare high- and low-agency control in AR-assisted construction assembly to understand the role of user agency. We designed cognitive and physical assembly scenarios and conducted a lab study (N=24), showing that low-agency control reduced mental workloads and perceived autonomy in several tasks. A follow-up domain expert study with trained carpenters (N=8) contextualised these results in an ecologically valid setting. Through semi-structured interviews, we examined the carpenters’ perspectives on AR support in their daily work and the trade-offs of automating interactions. Based on these findings, we summarise key design considerations to inform future adaptive AR designs in the context of timber construction.},
    articleno       = {1230},
    numpages        = {15},
    keywords        = {Augmented Reality, Worker Assistance, User Agency, Construction Industry},
    pdf             = {https://dl.acm.org/doi/pdf/10.1145/3706598.3713765},
    venue           = {CHI}
}
@inproceedings{rau2025traversing,
    title           = {Traversing Dual Realities: Investigating Techniques for Transitioning 3D Objects between Desktop and Augmented Reality Environments},
    author          = {Tobias Rau, Tobias Isenberg, Andreas Köhn, Michael Sedlmair, Benjamin Lee},
    year            = {2025},
    month           = {04},
    booktitle       = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI '25},
    doi             = {https://doi.org/10.1145/3706598.3713949},
    note            = {Received a best paper award},
    badge           = {bestpaper},
    isbn            = {9798400713941},
    url             = {https://hal.science/hal-05050852},
    abstract        = {Desktop environments can integrate augmented reality (AR) head-worn devices to support 3D representations, visualizations, and interactions in a novel yet familiar setting. As users navigate across the dual realities - desktop and AR - a way to move 3D objects between them is needed. We devise three baseline transition techniques based on common approaches in the literature and evaluate their usability and practicality in an initial user study (N=18). After refining both our transition techniques and the surrounding technical setup, we validate the applicability of the overall concept for real-world activities in an expert user study (N=6). In it, computational chemists followed their usual desktop workflows to build, manipulate, and analyze 3D molecular structures, but now aided with the addition of AR and our transition techniques. Based on our findings from both user studies, we provide lessons learned and takeaways for the design of 3D object transition techniques in desktop + AR environments.},
    articleno       = {1236},
    numpages        = {16},
    keywords        = {Augmented reality, Cross-reality, Hybrid user interfaces, Usability study, Expert study, Computational Chemistry, Gestural input},
    video           = {https://www.youtube.com/embed/gknKEMn2Rv4?si=mVAY_XpgiNoG7PtZ},
    pdf             = {https://arxiv.org/abs/2504.00371},
    venue           = {CHI}
}
@article{zhou2025personal,
    title           = {Personal Protection Equipment Training as a Virtual Reality Game in Immersive Environments: Development Study and Pilot Randomized Controlled Trial},
    author          = {Liang Zhou, Haoyang Liu, Mengjie Fan, Jiahao Liu, Xingyao Yu, Xintian Zhao, Shaoxing Zhang},
    year            = {2025},
    month           = {03},
    journal         = {JMIR Serious Games},
    publisher       = {JMIR Publications Inc., Toronto, Canada},
    volume          = {13},
    number          = {1},
    pages           = {e69021},
    doi             = {https://doi.org/10.2196/69021},
    venue           = {JMIR Serious Games},
    abstract        = {Background: Proper donning and doffing of personal protection equipment (PPE) and hand hygiene in the correct spatial context of a health facility is important for the prevention and control of nosocomial infections. On-site training is difficult due to the potential infectious risks and shortages of PPE, whereas video-based training lacks immersion which is vital for the familiarization of the environment. Virtual reality (VR) training can support the repeated practice of PPE donning and doffing in an immersive environment that simulates a realistic configuration of a health facility. Objective: This study aims to develop and evaluate a VR simulation focusing on the correct event order of PPE donning and doffing, that is, the item and hand hygiene order in the donning and doffing process but not the detailed steps of how to don and doff an item, in an immersive environment that replicates the spatial zoning of a hospital. The VR method should be generic and support customizable sequencing of PPE donning and doffing. Methods: An immersive VR PPE training tool was developed by computer scientists and medical experts. The effectiveness of the immersive VR method versus video-based learning was tested in a pilot study as a randomized controlled trial (N=32: VR group, n=16; video-based training, n=16) using questionnaires on spatial-aware event order memorization questions, usability, and task workload. Trajectories of participants in the immersive environment were also recorded for behavior analysis and potential improvements of the real environment of the health facility. Results: Comparable sequence memorization scores (VR mean 79.38, SD 12.90 vs video mean 74.38, SD 17.88; P=.37) as well as National Aeronautics and Space Administration Task Load Index scores (VR mean 42.9, SD 13.01 vs video mean 51.50, SD 20.44; P=.16) were observed. The VR group had an above-average usability in the System Usability Scale (mean 74.78>70.0) and was significantly better than the video group (VR mean 74.78, SD 13.58 vs video mean 57.73, SD 21.13; P=.009). The analysis and visualization of trajectories revealed a positive correlation between the length of trajectories and the completion time, but neither correlated to the accuracy of the memorization task. Further user feedback indicated a preference for the VR method over the video-based method. Limitations of and suggestions for improvements in the study were also identified. Conclusions: A new immersive VR PPE training method was developed and evaluated against the video-based training. Results of the pilot study indicate that the VR method provides training quality comparable to video-based training and is more usable. In addition, the immersive experience of realistic settings and the flexibility of training configurations make the VR method a promising alternative to video instructions.}
}
@inproceedings{aygün2025mixing,
    title           = {Mixing and Matching: Instruction Conveyance for Collaborative Tasks Using Asymmetric Augmented Reality Setups},
    author          = {Dilara Aygün, Aimée Sousa Calepso, Xiliu Yang, Achim Menges, Michael Sedlmair},
    year            = {2025},
    month           = {03},
    booktitle       = {2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
    pages           = {1072--1078},
    doi             = {https://doi.org/10.1109/VRW66409.2025.00215},
    keywords        = {Three-dimensional displays;Head-mounted displays;Conferences;Collaboration;Data visualization;Resists;Usability;Augmented reality;Assembly;Sorting;Augmented Reality;Collaboration;Data Visualization},
    venue           = {VRW},
    abstract        = {Augmented Reality (AR) applications can provide support to users with task instructions in-situ. Among different AR display types used for these applications, head-mounted displays (HMDs) and handheld displays (HHDs) are popular solutions. Previous research has examined asymmetrical setups, i.e., two or more people using different types of devices at the same time. However, asymmetrical setups for physical tasks that require collaboration have been little investigated. Our work implements dyadic assembly and sorting tasks supported by simultaneously using an HHD and an HMD. We conducted a user study (N=20) to evaluate this setup. Participants rated both displays’ usability similarly but showed a preference for HMD during both sorting and assembly tasks. While most participants agreed that they collaborated with their partners and the task was easier done in a team, less than half of HMD users in the sorting task agreed with the statement.}
}
@inproceedings{farley2025liftvr,
    title           = {LiftVR: A VR-Based Training System for Back-Friendly Lifting},
    author          = {Andreas Farley, Xingyao Yu, André Tomalka, Tobias Siebert, Michael Sedlmair},
    year            = {2025},
    month           = {03},
    booktitle       = {2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
    publisher       = {IEEE Computer Society},
    address         = {Los Alamitos, CA, USA},
    pages           = {671--678},
    doi             = {https://doi.org/10.1109/VRW66409.2025.00138},
    abstract        = {This paper introduces LiftVR, a VR-based training system designed to support back-friendly deadlift practice. The system integrates two feedforward guidance methods: 'skeleton,' which provides detailed posture replication, and 'zone,' which offers simplified, symmetrical visualizations to reduce cognitive load. Additionally, post-training feedback visualizations—such as motion replay, joint path analysis, and performance scoring—help users identify and correct movement errors. A user study revealed that the 'zone' method reduced cognitive effort and enabled participants to understand movements more quickly, albeit with slightly lower postural accuracy compared to the 'skeleton' method. Furthermore, post-training feedback was observed to disrupt muscle memory formation during intensive sessions. Nonetheless, participants’ performance across all experimental conditions, regardless of the feedforward method or feedback mode, showed significant improvement compared to their baseline. These findings underscore LiftVR’s potential as an effective and safe training tool for back-friendly lifting practices.},
    keywords        = {Training;Visualization;Three-dimensional displays;Virtual reality;Muscles;User interfaces;Skeleton;Error correction;Feedforward systems;Usability},
    venue           = {VRW}
}
@article{rau2025maico,
    title           = {MAICO: A Visualization Design Study on AI-Assisted Music Composition},
    author          = {Simeon Rau, Frank Heyen, Benedikt Brachtel, Michael Sedlmair},
    year            = {2025},
    month           = {02},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    pages           = {1--16},
    doi             = {https://doi.org/10.1109/TVCG.2025.3539779},
    url             = {https://www.replicabilitystamp.org/#https-github-com-visvar-maicov2},
    note            = {Received the Graphics Replicability Stamp},
    badge           = {reproducibility},
    keywords        = {Music;Artificial intelligence;Analytical models;Data visualization;Data models;Visual analytics;Aerospace electronics;Training;Generative AI;Computational modeling;Music composition;human-AI;comparative visualization;glyphs;similarity;design study},
    suppl           = {https://github.com/visvar/MAICoV2},
    venue           = {TVCG},
    abstract        = {We contribute a design study on using visual analytics for AI-assisted music composition. The main result is the interface MAICO (Music AI Co-creativity), which allows composers and other music creators to interactively generate, explore, select, edit, and compare samples from generative music models. MAICO is based on the idea of visual parameter space analysis and supports the simultaneous analysis of hundreds of short samples of symbolic music from multiple models, displaying them in different metric- and similarity-based layouts. We developed and evaluated MAICO together with a professional composer who actively used it for five months to create, among other things, a composition for the Biennale Arte 2024 in Venice, which was recorded by the Munich Symphonic Orchestra. We discuss our design choices and lessons learned from this endeavor to support Human-AI co-creativity with visual analytics.}
}
@article{rau2024understanding,
    title           = {Understanding Collaborative Learning of Molecular Structures in AR with Eye Tracking},
    author          = {Tobias Rau, Maurice Koch, Nelusa Pathmanathan, Tobias Isenberg, Daniel Weiskopf, Michael Sedlmair, Andreas Köhn, Kuno Kurzhals},
    year            = {2024},
    month           = {11},
    journal         = {IEEE Computer Graphics and Applications},
    doi             = {https://doi.org/10.1109/MCG.2024.3503903},
    venue           = {CG&A},
    video           = {https://www.youtube.com/embed/jXly5nAwCd8?si=zbZVblF673Ls9kzl},
    abstract        = {We present an approach for on-site instruction of multiple students accompanied by gaze-based monitoring to observe patterns of visual attention during task solving. We focus on collaborative processes in augmented reality (AR) that play an essential role in on-site and remote teaching alike. From a teaching perspective, it is important in such scenarios to communicate content and tasks effectively, observe whether students understand the task, and help appropriately. In our setting, students work with head-mounted displays with eye-tracking support to collaborate in a co-located space. The supervisor can observe the scene and the students and interact with them in a hybrid setup using both AR and a desktop PC. Attention monitoring and guidance are facilitated via a bidirectional mapping between 2D structural formulas and 3D molecules. We showcase our approach with an interactive teaching scenario in which chemistry students learn aspects of stereochemistry by interacting with virtual 3D models of molecular structures. An interview with supervisors and students showed that our approach has much potential in classroom applications for (1) engaging students in collaborative task solving and (2) assisting teachers in monitoring and supporting the learning processes of their students.}
}
@book{fekete2024progressive,
    title           = {Progressive Data Analysis},
    author          = {Jean-Daniel Fekete, Danyel Fisher, Michael Sedlmair},
    year            = {2024},
    month           = {11},
    publisher       = {Eurographics},
    pages           = {231},
    doi             = {https://doi.org/10.2312/pda.20242707},
    url             = {https://inria.hal.science/hal-04776568},
    keywords        = {visualization ; scalability ; progressive data analysis},
    pdf             = {https://inria.hal.science/hal-04776568/document},
    hal_id          = {hal-04776568},
    hal_version     = {v1},
    venue           = {Eurographics},
    abstract        = {We live in an era in which data is abundant and growing rapidly. Big data databases sprawl past memory and computation limits and across distributed systems. To sustain this growth, engineers are designing new hardware and software systems with new storage management and capabilities for predictive computation. Yet, as datasets grow and computations become more complex, response times suffer. These infrastructures, while good for data at scale, do not support exploratory data analysis (EDA) effectively. EDA allows analysts to make sense of data with little or no known model and is essential in many application domains, from network security and fraud detection to epidemiology and preventive medicine. Data exploration is conducted with an iterative loop in which analysts interact with data through computations that return results, usually displayed with visualizations, which the analyst can then interact with again. EDA requires highly responsive system response times: at 500 ms, users typically change their querying behavior; after five or ten seconds, they abandon tasks or lose attention. To address this problem, a new computational paradigm has emerged in the last decade, which goes under several names. In the database community, it is called online aggregation, while among visualization researchers, it has been called progressive, incremental, or iterative visualization. In this book, we will refer to it as Progressive Data Analysis. This paradigm consists of splitting long computations into a series of approximate results that improve over time. In this process, partial or approximate results are rapidly returned to the user and can be interacted with in a fluent and iterative fashion. With the increasing growth in data, such progressive data analysis approaches will become one of the leading paradigms for data exploration systems, but it will also require major changes to algorithms, data structures, and visualization tools. By solving the latency issue, progressive data analysis opens up new perspectives but also presents new challenges. Problems involving complex analyses can now be addressed interactively with little or no preparation time, but how long should the analyst wait before obtaining results that are good enough to make decisions? Can progressive systems provide effective quality measures to avoid either waiting too long or deciding too early? While the progressive process is being computed, the iterative visualization of partial results should remain stable enough to be monitored. Can we stabilize these partial results without hurting their quality or speed? And, more fundamentally, can we transform all data analysis operations so that they become progressive? These questions, among others, are at the core of this book. This book is an introduction to the new paradigm of progressive data analysis and visualization. It explores the major scientific and technical benefits of performing complex data analysis progressively on big data. It also examines the challenges that must be addressed for the paradigm to become fully usable. These important issues involve research fields that are traditionally viewed as separate areas in computer science: databases, scientific computing, machine learning, visualization, statistics, and human-computer interaction; they will need to work together to achieve end to- end solutions to solve these challenges and enable the emergence of practical, progressive systems. This book closes with a research agenda to help researchers converge on key questions.}
}
@article{yu2024persival,
    title           = {PerSiVal: On-Body AR Visualization of Biomechanical Arm Simulations},
    author          = {Xingyao Yu, David Rosin, Johannes Kässinger, Benjamin Lee, Frank Dürr, Christian Becker, Oliver Röhrle, Michael Sedlmair},
    year            = {2024},
    month           = {11},
    journal         = {IEEE Computer Graphics and Applications},
    volume          = {44},
    number          = {6},
    pages           = {24--38},
    doi             = {https://doi.org/10.1109/MCG.2024.3494598},
    keywords        = {Biological system modeling;Muscles;Biomechanics;Data visualization;Data models;Geometry;Computational modeling;Prototypes;Three-dimensional displays;Real-time systems;Augmented reality;Biomechanics;Arms},
    venue           = {CG&A},
    abstract        = {In this work, we explore different combinations of techniques for an interactive, on-body visualization in augmented reality (AR) of an upper arm muscle simulation model. In terms of data, we focus on a continuum-mechanical simulation model involving five different muscles of the human upper arm, with physiologically realistic geometry. In terms of use cases, we focus on the immersive illustration, education, and dissemination of such simulation models. We describe the process of developing six on-body visualization prototypes over a period of five years. For each prototype, we employed different types of motion capture, AR display technologies, and visual encoding approaches, and gathered feedback throughout outreach activities. We reflect on the development of the individual prototypes and summarize lessons learned of our exploration process into the design space of situated on-body visualization.}
}
@inproceedings{angerbauer2024is,
    title           = {Is it Part of Me? Exploring Experiences of Inclusive Avatar Use For Visible and Invisible Disabilities in Social VR},
    author          = {Katrin Angerbauer, Phoenix Van Wagoner, Tim Halach, Jonas Vogelsang, Natalie Hube, Andria Smith, Ksenia Keplinger, Michael Sedlmair},
    year            = {2024},
    month           = {10},
    booktitle       = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
    location        = {St. John's, NL, Canada},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {ASSETS '24},
    doi             = {https://doi.org/10.1145/3663548.3675601},
    isbn            = {9798400706776},
    abstract        = {Social Virtual Reality (VR) platforms have surged in popularity in recent years, including among people with disabilities (PWD). Previous research has documented accessibility challenges, harassment, and negative experiences for PWD using disability signifiers in VR, primarily focusing on those with visible disabilities who encounter negative experiences. Yet, little is known about the experiences of people with invisible disabilities in social VR environments, and whether positive experiences are also common. To address these gaps, we designed inclusive avatars (avatars with disability signifiers) and investigated the lived experiences of 26 individuals with both visible and invisible disabilities immersing themselves in social interactions in VRChat for a week. We utilized a mixed methods experience sampling design and multilevel regression to explore the relationships between social interactions of PWD in VR and various psychological outcomes. Our results indicate that PWD, both visible and invisible, experienced positive and negative social interactions in VR. These interactions, in turn, significantly influenced users’ overall experience with inclusive avatars, affecting aspects such as emotional responses, engagement levels, satisfaction with the avatar’s design, and perceptions of inclusion in VR. Qualitative interviews of 18 participants allowed for a more nuanced exploration of the experiences of PWD by giving voice to users who are rarely studied in depth. Findings provided unique insights into both the positive and negative experiences of PWD, as well as identified key design factors influencing user experience in social VR.},
    articleno       = {54},
    numpages        = {15},
    keywords        = {accessibility, diary study, inclusive avatars, lived experiences, social VR},
    venue           = {ASSETS}
}
@inproceedings{grioui2024combating,
    title           = {Combating Stage Fright in VR using Gradual Lighting},
    author          = {Fairouz Grioui, Marc Roland Ghütter, Sanan Akther, Hithesh Chettenahalli Honnegowda, Michael Sedlmair},
    year            = {2024},
    month           = {10},
    booktitle       = {2024 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
    pages           = {353--354},
    doi             = {https://doi.org/10.1109/ISMAR-Adjunct64951.2024.00087},
    keywords        = {Medical treatment;Lighting;Stress;Augmented reality;Virtual Reality;Stage fright;Presentation},
    venue           = {ISMAR-adjunct},
    abstract        = {The principle of exposure therapy is to gradually increase the exposure of an individual to the feared stimulus. In our study, we explore this approach to help people with stage fright to gradually notice the presence of a virtual audience in a classroom. We propose a Virtual Reality (VR) application where the users can rehearse their presentations in front of a virtual audience. Then, we test the influence of two different light conditions: 1) constant and 2) gradually increasing light, on reducing presenters’ stress levels. Finally, we report results on our participants’ preferences with the two conditions.}
}
@inproceedings{park2024we,
    title           = {We Are Visual Thinkers, Not Verbal Thinkers!': A Thematic Analysis of How Professional Designers Use Generative AI Image Generation Tools},
    author          = {Hyerim Park, Joscha Eirich, Andre Luckow, Michael Sedlmair},
    year            = {2024},
    month           = {10},
    booktitle       = {Proceedings of the 13th Nordic Conference on Human-Computer Interaction},
    location        = {Uppsala, Sweden},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {NordiCHI '24},
    doi             = {https://doi.org/10.1145/3679318.3685370},
    isbn            = {9798400709661},
    abstract        = {Generative artificial intelligence (GenAI) has become increasingly popular, influencing various creative domains. However, while broader societal perspectives have been analyzed, specific examinations of how practitioners utilize GenAI tools to enhance their current workflows remain limited. To address this gap, we conducted a qualitative study involving 16 professional designers from the automotive industry. We aimed to identify their challenges with existing GenAI image generation tools in daily design practices. Thematic analysis revealed four key themes: (1) the need for visual input-centric multi-modal interfaces that extend beyond textual prompts, (2) the lack of support for the iterative nature of design processes in GenAI tools, (3) difficulties in controlling prompts to achieve desired outputs, and (4) the significance of incorporating human experiences and emotions into design. Based on our findings, we propose and discuss potential design considerations for enhancing future GenAI image generation tool interfaces.},
    articleno       = {35},
    numpages        = {14},
    keywords        = {creativity support tools, generative AI, human-AI interaction, qualitative research},
    venue           = {NordiCHI}
}
@inproceedings{bartels2024active,
    title           = {Active Haptic Feedback for a Virtual Wrist-Anchored User Interface},
    author          = {Jan Ulrich Bartels, Natalia Sanchez-Tamayo, Michael Sedlmair, Katherine J. Kuchenbecker},
    year            = {2024},
    month           = {10},
    booktitle       = {Adjunct Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
    location        = {Pittsburgh, PA, USA},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {UIST Adjunct '24},
    doi             = {https://doi.org/10.1145/3672539.3686765},
    isbn            = {9798400707186},
    abstract        = {The presented system combines a virtual wrist-anchored user interface (UI) with a new low-profile, wrist-worn device that provides salient and expressive haptic feedback such as contact, pressure and broad-bandwidth vibration. This active feedback is used to add tactile cues to interactions with virtual mid-air UI elements that track the user’s wrist; we demonstrate a simple menu-interaction task to showcase the utility of haptics for interactions with virtual buttons and sliders. Moving forward, we intend to use this platform to develop haptic guidelines for body-anchored interfaces and test multiple haptic devices across the body to create engaging interactions.},
    articleno       = {53},
    numpages        = {3},
    keywords        = {body anchored interfaces, haptic interfaces, human computer interaction, virtual reality},
    venue           = {UIST Adjunct}
}
@article{hube2024a,
    title           = {A Study on the Influence of Situations on Personal Avatar Characteristics},
    author          = {Natalie Hube, Melissa Reinelt, Kresimir Vidackovic, Michael Sedlmair},
    year            = {2024},
    month           = {09},
    journal         = {Visual Computing for Industry, Biomedicine, and Art},
    volume          = {7},
    doi             = {https://doi.org/10.1186/s42492-024-00174-7},
    issue           = {1},
    pdf             = {https://vciba.springeropen.com/counter/pdf/10.1186/s42492-024-00174-7.pdf},
    venue           = {VCIBA},
    abstract        = {Avatars play a key role in how persons interact within virtual environments, acting as the digital selves. There are many types of avatars, each serving the purpose of representing users or others in these immersive spaces. However, the optimal approach for these avatars remains unclear. Although consumer applications often use cartoon-like avatars, this trend is not as common in work settings. To gain a better understanding of the kinds of avatars people prefer, three studies were conducted involving both screen-based and virtual reality setups, looking into how social settings might affect the way people choose their avatars. Personalized avatars were created for 91 participants, including 71 employees in the automotive field and 20 participants not affiliated with the company. The research shows that work-type situations influence the chosen avatar. At the same time, a correlation between the type of display medium used to display the avatar or the person’s personality and their avatar choice was not found. Based on the findings, recommendations are made for future avatar representations in work environments and implications and research questions derived that can guide future research.}
}
@article{achberger2024an,
    title           = {An Exploratory Expert-Study for Multi-Type Haptic Feedback for Automotive Virtual Reality Tasks},
    author          = {Alexander Achberger, Patrick Gebhardt, Michael Sedlmair},
    year            = {2024},
    month           = {09},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    pages           = {1--11},
    doi             = {https://doi.org/10.1109/TVCG.2024.3456203},
    keywords        = {Haptic interfaces;Automotive engineering;Actuators;Thumb;Automobiles;Reservoirs;Force;Haptics;Virtual Reality;Human Computer Interaction},
    venue           = {TVCG},
    abstract        = {Previous research has shown that integrating haptic feedback can improve immersion and realism in automotive VR applications. However, current haptic feedback approaches primarily focus on a single feedback type. This means users must switch between devices to experience haptic stimuli for different feedback types, such as grabbing, collision, or weight simulation. This restriction limits the ability to simulate haptics realistically for complex tasks such as maintenance. To address this issue, we evaluated existing feedback devices based on our requirements analysis to determine which devices are most suitable for simulating these three feedback types. Since no suitable haptic feedback system can simulate all three feedback types simultaneously, we evaluated which devices can be combined. Based on that, we devised a new multi-type haptic feedback system combining three haptic feedback devices. We evaluated the system with different feedback-type combinations through a qualitative expert study involving twelve automotive VR experts. The results showed that combining weight and collision feedback yielded the best and most realistic experience. The study also highlighted technical limitations in current grabbing devices. Our findings provide insights into the effectiveness of haptic device combinations and practical boundaries for automotive virtual reality tasks.}
}
@inproceedings{kergaßner2024hivefive360,
    title           = {HiveFive360: Extending the VR Gaze Guidance Technique HiveFive to Highlight Out-Of-FOV Targets},
    author          = {Sophie Kergaßner, Nina Doerr, Markus Wieland, Martin Fuchs, Michael Sedlmair},
    year            = {2024},
    month           = {09},
    booktitle       = {Proceedings of Mensch Und Computer 2024},
    location        = {Karlsruhe, Germany},
    publisher       = {ACM},
    address         = {New York, NY, USA},
    series          = {MuC '24},
    pages           = {11–20},
    doi             = {https://doi.org/10.1145/3670653.3670662},
    isbn            = {9798400709982},
    abstract        = {Modern display technologies, particularly those supporting 360° content, are increasingly used for immersive experiences in a variety of domains. However, information outside of the user’s field of view (FOV) may be easily overlooked. To address this, guiding cues can be provided to effectively direct attention. Subtle and diegetic cues are particularly effective in keeping the coherence and immersion of the presented content. HiveFive is one of the few diegetic highlighting techniques. It effectively highlights objects by attracting the user’s attention with swarm-like motion. However, HiveFive is restricted to in-FOV target highlighting. This work presents the novel technique HiveFive360, an extension of HiveFive that enables it to guide users to out-of-FOV targets. HiveFive360 is evaluated in a user study against FlyingARrow and Subtle Gaze Direction VR regarding completion time, sense of presence and task load. HiveFive360 was found to effectively guide users in various environments without excessive distraction or task load.},
    numpages        = {10},
    keywords        = {Virtual Reality, Visual Gaze Guidance},
    venue           = {MuC}
}
@inproceedings{jenadeleh2024an,
    title           = {An Image Quality Dataset with Triplet Comparisons for Multi-dimensional Scaling},
    author          = {Mohsen Jenadeleh, Frederik L. Dennig, Rene Cutura, Quynh Quang Ngo, Daniel A. Keim, Michael Sedlmair, Dietmar Saupe},
    year            = {2024},
    month           = {06},
    booktitle       = {2024 16th International Conference on Quality of Multimedia Experience (QoMEX)},
    pages           = {278--281},
    doi             = {https://doi.org/10.1109/QoMEX61742.2024.10598258},
    keywords        = {Image quality;Layout;Reconstruction algorithms;Distortion;Topology;Distortion measurement;Logistics;multidimensional image quality assessment;triplet comparison;image quality dataset},
    venue           = {QoMEX},
    abstract        = {In the early days of perceptual image quality research more than 30 years ago, the multidimensionality of distortions in perceptual space was considered important. However, research focused on scalar quality as measured by mean opinion scores. With our work, we intend to revive interest in this relevant area by presenting a first pilot dataset of annotated triplet comparisons for image quality assessment. It contains one source stimulus together with distorted versions derived from 7 distortion types at 12 levels each. Our crowdsourced and curated dataset contains roughly 50,000 responses to 7,000 triplet comparisons. We show that the multidimensional embedding of the dataset poses a challenge for many established triplet embedding algorithms. Finally, we propose a new reconstruction algorithm, dubbed logistic triplet embedding (LTE) with Tikhonov regularization. It shows promising performance. This study helps researchers to create larger datasets and better embedding techniques for multidimensional image quality. The dataset includes images and ratings and can be accessed at https://github.com/jenadeleh/multidimensionalIQA-dataset/tree/main.}
}
@inproceedings{kloetzl2024nmfbased,
    title           = {NMF-Based Analysis of Mobile Eye-Tracking Data},
    author          = {Daniel Klötzl, Tim Krake, Frank Heyen, Michael Becher, Maurice Koch, Daniel Weiskopf, Kuno Kurzhals},
    year            = {2024},
    month           = {06},
    booktitle       = {Proceedings of the 2024 Symposium on Eye Tracking Research and Applications},
    publisher       = {ACM},
    series          = {ETRA '24},
    doi             = {https://doi.org/10.1145/3649902.3653518},
    isbn            = {9798400706073},
    abstract        = {The depiction of scanpaths from mobile eye-tracking recordings by thumbnails from the stimulus allows the application of visual computing to detect areas of interest in an unsupervised way. We suggest using nonnegative matrix factorization (NMF) to identify such areas in stimuli. For a user-defined integer k, NMF produces an explainable decomposition into k components, each consisting of a spatial representation associated with a temporal indicator. In the context of multiple eye-tracking recordings, this leads to k spatial representations, where the temporal indicator highlights the appearance within recordings. The choice of k provides an opportunity to control the refinement of the decomposition, i.e., the number of areas to detect. We combine our NMF-based approach with visualization techniques to enable an exploratory analysis of multiple recordings. Finally, we demonstrate the usefulness of our approach with mobile eye-tracking data of an art gallery.},
    articleno       = {76},
    numpages        = {9},
    keywords        = {Clustering, Eye Tracking, Matrix Factorization, NMF, Visualization},
    pdf             = {https://arxiv.org/pdf/2404.03417v1},
    venue           = {ETRA}
}
@article{rau2024charpack,
    title           = {chARpack: The Chemistry Augmented Reality Package},
    author          = {Tobias Rau, Michael Sedlmair, Andreas Köhn},
    year            = {2024},
    month           = {05},
    journal         = {Journal of Chemical Information and Modeling},
    volume          = {0},
    number          = {0},
    pages           = {null},
    doi             = {https://doi.org/10.1021/acs.jcim.4c00462},
    pdf             = {https://pubs.acs.org/doi/epdf/10.1021/acs.jcim.4c00462},
    venue           = {JCIM},
    video           = {https://www.youtube.com/embed/4t7OTaahNZY?si=DKHe_h5KbnhqRW3x},
    abstract        = {Off-loading visualization and interaction into virtual reality (VR) using head-mounted displays (HMDs) has gained considerable popularity in simulation sciences, particularly in chemical modeling. Because of its unique way of soft immersion, augmented reality (AR) HMD technology has even more potential to be integrated into the everyday workflow of computational chemists. In this work, we present our environment to explore the prospects of AR in chemistry and general molecular sciences: The chemistry in Augmented Reality package (chARpack). Besides providing an extensible framework, our software focuses on a seamless transition between a 3D stereoscopic view with true 3D interactions and the traditional desktop PC setup to provide users with the best setup for all tasks in their workflow. Using feedback from domain experts, we discuss our design requirements for this kind of hybrid working environment (AR + PC), regarding input, features, degree of immersion, and collaboration.}
}
@inproceedings{quijano-chavez2024brushing,
    title           = {Brushing and Linking for Situated Analytics},
    author          = {Carlos Quijano-Chavez, Nina Doerr, Benjamin Lee, Dieter Schmalstieg, Michael Sedlmair},
    year            = {2024},
    month           = {05},
    booktitle       = {2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
    pages           = {597--603},
    doi             = {https://doi.org/10.1109/VRW62533.2024.00116},
    venue           = {VRW},
    abstract        = {Situated analytics is visual analysis that is embedded in the physical world. Conventionally, the data related to referents (i.e., physical objects) is manipulated indirectly, through dedicated interaction devices or using separate abstract representations. The natural way of interacting in 3D space using direct manipulation with one's hands is hardly employed when abstract data is concerned. In this paper, we explore the idea of directly brushing and linking referents in the real world to analyze abstract data related to the referents. We discuss what brushing & linking means when applied to the real world, including multiple ways of selecting referents (brushing) in augmented reality, as well as different ways of linking with an abstract data view, presented on a dedicated display (a tablet). A proof of concept was implemented to test and demonstrate the capabilities of brushing & linking of referents. We conclude with a set of open research challenges that exist in this new and emerging area.}
}
@article{doerr2024visual,
    title           = {Visual Highlighting for Situated Brushing and Linking},
    author          = {Nina Doerr, Benjamin Lee, Katarina Baricova, Dieter Schmalstieg, Michael Sedlmair},
    year            = {2024},
    month           = {05},
    journal         = {Computer Graphics Forum},
    volume          = {43},
    number          = {3},
    pages           = {e15105},
    doi             = {https://doi.org/10.1111/cgf.15105},
    keywords        = {CCS Concepts, • Human-centered computing → Empirical studies in visualization, Empirical studies in HCI, Information visualization},
    eprint          = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.15105},
    pdf             = {https://arxiv.org/pdf/2403.15321},
    venue           = {EuroVis},
    abstract        = {Brushing and linking is widely used for visual analytics in desktop environments. However, using this approach to link many data items between situated (e.g., a virtual screen with data) and embedded views (e.g., highlighted objects in the physical environment) is largely unexplored. To this end, we study the effectiveness of visual highlighting techniques in helping users identify and link physical referents to brushed data marks in a situated scatterplot. In an exploratory virtual reality user study (N=20), we evaluated four highlighting techniques under different physical layouts and tasks. We discuss the effectiveness of these techniques, as well as implications for the design of brushing and linking operations in situated analytics.}
}
@article{beck2024choreovis,
    title           = {ChoreoVis: Planning and Assessing Formations in Dance Choreographies},
    author          = {Samuel Beck, Nina Doerr, Kuno Kurzhals, Alexander Riedlinger, Fabian Schmierer, Michael Sedlmair, Steffen Koch},
    year            = {2024},
    month           = {05},
    journal         = {Computer Graphics Forum},
    volume          = {43},
    number          = {3},
    pages           = {e15104},
    doi             = {https://doi.org/10.1111/cgf.15104},
    keywords        = {CCS Concepts, • Human-centered computing → Visual analytics},
    eprint          = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.15104},
    suppl           = {https://diglib.eg.org/items/ebb60750-d719-45cc-ae90-733ad77fdef5},
    pdf             = {https://arxiv.org/pdf/2404.04100},
    venue           = {EuroVis},
    abstract        = {Sports visualization has developed into an active research field over the last decades. Many approaches focus on analyzing movement data recorded from unstructured situations, such as soccer. For the analysis of choreographed activities like formation dancing, however, the goal differs, as dancers follow specific formations in coordinated movement trajectories. To date, little work exists on how visual analytics methods can support such choreographed performances. To fill this gap, we introduce a new visual approach for planning and assessing dance choreographies. In terms of planning choreographies, we contribute a web application with interactive authoring tools and views for the dancers' positions and orientations, movement trajectories, poses, dance floor utilization, and movement distances. For assessing dancers' real-world movement trajectories, extracted by manual bounding box annotations, we developed a timeline showing aggregated trajectory deviations and a dance floor view for detailed trajectory comparison. Our approach was developed and evaluated in collaboration with dance instructors, showing that introducing visual analytics into this domain promises improvements in training efficiency for the future.}
}
@inproceedings{yu2024design,
    title           = {Design Space of Visual Feedforward And Corrective Feedback in XR-Based Motion Guidance Systems},
    author          = {Xingyao Yu, Benjamin Lee, Michael Sedlmair},
    year            = {2024},
    month           = {05},
    booktitle       = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
    publisher       = {ACM},
    series          = {CHI '24},
    doi             = {https://doi.org/10.1145/3613904.3642143},
    isbn            = {9798400703300},
    abstract        = {Extended reality (XR) technologies are highly suited in assisting individuals in learning motor skills and movements – referred to as motion guidance. In motion guidance, the 'feedforward' provides instructional cues of the motions that are to be performed, whereas the 'feedback' provides cues which help correct mistakes and minimize errors. Designing synergistic feedforward and feedback is vital to providing an effective learning experience, but this interplay between the two has not yet been adequately explored. Based on a survey of the literature, we propose design space for both motion feedforward and corrective feedback in XR, and describe the interaction effects between them. We identify common design approaches of XR-based motion guidance found in our literature corpus, and discuss them through the lens of our design dimensions. We then discuss additional contextual factors and considerations that influence this design, together with future research opportunities for motion guidance in XR.},
    articleno       = {723},
    numpages        = {15},
    keywords        = {Design Space, Extended Reality, Motion Guidance, Visualization},
    pdf             = {https://dl.acm.org/doi/pdf/10.1145/3613904.3642143},
    venue           = {CHI}
}
@inproceedings{krauter2024sitting,
    title           = {Sitting Posture Recognition and Feedback: A Literature Review},
    author          = {Christian Krauter, Katrin Angerbauer, Aimée Sousa Calepso, Alexander Achberger, Sven Mayer, Michael Sedlmair},
    year            = {2024},
    month           = {05},
    booktitle       = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
    publisher       = {ACM},
    series          = {CHI '24},
    doi             = {https://doi.org/10.1145/3613904.3642657},
    isbn            = {9798400703300},
    abstract        = {Extensive sitting is unhealthy; thus, countermeasures are needed to react to the ongoing trend toward more prolonged sitting. A variety of studies and guidelines have long addressed the question of how we can improve our sitting habits. Nevertheless, sitting time is still increasing. Here, smart devices can provide a general overview of sitting habits for more nuanced feedback on the user's sitting posture. Based on a literature review (N=223), including publications from engineering, computer science, medical sciences, electronics, and more, our work guides developers of posture systems. There is a large variety of approaches, with pressure-sensing hardware and visual feedback being the most prominent. We found factors like environment, cost, privacy concerns, portability, and accuracy important for deciding hardware and feedback types. Further, one should consider the user's capabilities, preferences, and tasks. Regarding user studies for sitting posture feedback, there is a need for better comparability and for investigating long-term effects.},
    articleno       = {943},
    numpages        = {20},
    keywords        = {Literature review, chair, posture, sitting},
    pdf             = {https://dl.acm.org/doi/pdf/10.1145/3613904.3642657},
    venue           = {CHI},
    video           = {https://www.youtube.com/embed/nzDWsRvg-k8?si=YB2d9j9-yRC0pMPv},
    video2          = {https://www.youtube.com/embed/wZaAT2zj8H8?si=aOyqXcemlOrQQYP4}
}
@inproceedings{xiao2024systematic,
    title           = {A Systematic Review of Ability-diverse Collaboration through Ability-based Lens in HCI},
    author          = {Lan Xiao, Maryam Bandukda, Katrin Angerbauer, Weiyue Lin, Tigmanshu Bhatnagar, Michael Sedlmair, Catherine Holloway},
    year            = {2024},
    month           = {05},
    booktitle       = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
    publisher       = {ACM},
    series          = {CHI '24},
    doi             = {https://doi.org/10.1145/3613904.3641930},
    isbn            = {9798400703300},
    abstract        = {In a world where diversity is increasingly recognised and celebrated, it is important for HCI to embrace the evolving methods and theories for technologies to reflect the diversity of its users and be ability-centric. Interdependence Theory, an example of this evolution, highlights the interpersonal relationships between humans and technologies and how technologies should be designed to meet shared goals and outcomes for people, regardless of their abilities. This necessitates a contemporary understanding of 'ability-diverse collaboration,' which motivated this review. In this review, we offer an analysis of 117 papers sourced from the ACM Digital Library spanning the last two decades. We contribute (1) a unified taxonomy and the Ability-Diverse Collaboration Framework, (2) a reflective discussion and mapping of the current design space, and (3) future research opportunities and challenges. Finally, we have released our data and analysis tool to encourage the HCI research community to contribute to this ongoing effort.},
    articleno       = {961},
    numpages        = {21},
    keywords        = {Interdependence, ability-based method, accessibility, collaboration},
    pdf             = {https://dl.acm.org/doi/pdf/10.1145/3613904.3641930},
    venue           = {CHI}
}
@inproceedings{pathmanathan2024eyes,
    title           = {Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks},
    author          = {Nelusa Pathmanathan, Tobias Rau, Xiliu Yang, Aimée Sousa Calepso, Felix Amtsberg, Achim Menges, Michael Sedlmair, Kuno Kurzhals},
    year            = {2024},
    month           = {04},
    booktitle       = {2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
    publisher       = {IEEE Computer Society},
    address         = {Los Alamitos, CA, USA},
    pages           = {785--795},
    doi             = {https://doi.org/10.1109/VR58804.2024.00098},
    abstract        = {The use of augmented reality technology to support humans with situated visualization in complex tasks such as navigation or assembly has gained increasing importance in research and industrial applications. One important line of research regards supporting and understanding collaborative tasks. Analyzing collaboration patterns is usually done by conducting observations and interviews. To expand these methods, we argue that eye tracking can be used to extract further insights and quantify behavior. To this end, we contribute a study that uses eye tracking to investigate participant strategies for solving collaborative sorting and assembly tasks. We compare participants’ visual attention during situated instructions in AR and traditional paper-based instructions as a baseline. By investigating the performance and gaze behavior of the participants, different strategies for solving the provided tasks are revealed. Our results show that with situated visualization, participants focus more on task-relevant areas and require less discussion between collaboration partners to solve the task at hand.},
    keywords        = {visualization;three-dimensional displays;collaboration;gaze tracking;user interfaces;user experience;task analysis},
    venue           = {VR}
}
@inproceedings{yang2024putting,
    title           = {Putting Our Minds Together: Iterative Exploration for Collaborative Mind Mapping},
    author          = {Ying Yang, Tim Dwyer, Zachari Swiecki, Benjamin Lee, Michael Wybrow, Maxime Cordeil, Teresa Wulandari, Bruce H. Thomas, Mark Billinghurst},
    year            = {2024},
    month           = {04},
    booktitle       = {Proceedings of the Augmented Humans International Conference 2024},
    publisher       = {ACM},
    series          = {AHs '24},
    pages           = {255–258},
    doi             = {https://doi.org/10.1145/3652920.3653043},
    isbn            = {9798400709807},
    url             = {https://arxiv.org/abs/2403.13517},
    abstract        = {We delineate the development of a mind-mapping system designed concurrently for both VR and desktop platforms. Employing an iterative methodology with groups of users, we systematically examined and improved various facets of our system, including interactions, communication mechanisms and gamification elements, to streamline the mind-mapping process while augmenting situational awareness and promoting active engagement among collaborators. We also report our observational findings on these facets from this iterative design process.},
    numpages        = {4},
    keywords        = {Collaborative Sensemaking, Gamification, Hand Gestures, Virtual Reality},
    venue           = {AHs}
}
@article{yang2024challenges,
    title           = {Challenges and potential for human–robot collaboration in timber prefabrication},
    author          = {Xiliu Yang, Felix Amtsberg, Michael Sedlmair, Achim Menges},
    year            = {2024},
    month           = {02},
    journal         = {Automation in Construction},
    volume          = {160},
    pages           = {105333},
    doi             = {https://doi.org/10.1016/j.autcon.2024.105333},
    issn            = {0926-5805},
    url             = {https://www.sciencedirect.com/science/article/pii/S0926580524000694},
    keywords        = {Human–robot collaboration, Robotic fabrication, Timber prefabrication, Human-centred design, Co-design},
    abstract        = {Recent advancements in robotics and human–machine interfaces enable new collaborative procedures that combine the strengths of machines and humans. Compared to existing automation technologies in the timber prefabrication industry, human–robot collaboration (HRC) offers new possibilities for increased flexibility and productivity. This paper aims to map out the challenges and opportunities for HRC within the context of timber prefabrication by constructing a conceptual framework. The framework is based on three pillars: (1) existing HRC theories and frameworks, (2) a literature review of HRC research in robotic timber fabrication, and (3) perspectives of human labour in the timber construction industry. The relevant topics among these three areas are triangulated to construct a conceptual framework that bridges the system-, design-, and human-centred considerations. The framework serves as an organising device to support future explorations and research on human–robot collaboration in robotic timber construction.},
    venue           = {Automation in Construction}
}
@article{abbas2021arxiv,
    title           = {ClustML: A Measure of Cluster Pattern Complexity in Scatterplots Learnt from Human-labeled Groupings},
    author          = {Mostafa Abbas, Ehsan Ullah, Abdelkader Baggag, Halima Bensmail, Michael Sedlmair, Michael Aupetit},
    year            = {2024},
    month           = {01},
    journal         = {Information Visualization},
    publisher       = {SAGE Publications},
    volume          = {0},
    number          = {0},
    pages           = {14738716231220536},
    doi             = {https://doi.org/10.1177/14738716231220536},
    abstract        = {Visual quality measures (VQMs) are designed to support analysts by automatically detecting and quantifying patterns in visualizations. We propose a new data-driven technique called ClustRank that allows to rank scatterplots according to visible grouping patterns. Our model first encodes scatterplots in the parametric space of a Gaussian Mixture Model, and then uses a classifier trained on human judgment data to estimate the perceptual complexity of grouping patterns. The numbers of initial mixture components and final combined groups determine the rank of the scatterplot. ClustRank improves on existing VQM techniques by mimicking human judgments on two-Gaussian cluster patterns and gives more accuracy when ranking general cluster patterns in scatterplots. We demonstrate its benefit by analyzing kinship data for genome-wide association studies, a domain in which experts rely on the visual analysis of large sets of scatterplots. We make the three benchmark datasets and the ClustRank VQM available for practical use and further improvements.},
    pdf             = {https://arxiv.org/pdf/2106.00599.pdf},
    venue           = {Information Visualization}
}
@article{bauer2023visual,
    title           = {Visual Ensemble Analysis of Fluid Flow in Porous Media across Simulation Codes and Experiment},
    author          = {Ruben Bauer, Quynh Quang Ngo, Guido Reina, Steffen Frey, Bernd Flemisch, Helwig Hauser, Thomas Ertl, Michael Sedlmair},
    year            = {2023},
    month           = {12},
    journal         = {Transport in Porous Media},
    publisher       = {Springer},
    pages           = {1--29},
    doi             = {https://doi.org/10.1007/s11242-023-02019-y},
    pdf             = {https://link.springer.com/content/pdf/10.1007/s11242-023-02019-y.pdf},
    venue           = {Transport in Porous Media},
    abstract        = {We study the question of how visual analysis can support the comparison of spatio-temporal ensemble data of liquid and gas flow in porous media. To this end, we focus on a case study, in which nine different research groups concurrently simulated the process of injecting CO2 into the subsurface. We explore different data aggregation and interactive visualization approaches to compare and analyze these nine simulations. In terms of data aggregation, one key component is the choice of similarity metrics that define the relationship between different simulations. We test different metrics and find that using the machine-learning model “S4” (tailored to the present study) as metric provides the best visualization results. Based on that, we propose different visualization methods. For overviewing the data, we use dimensionality reduction methods that allow us to plot and compare the different simulations in a scatterplot. To show details about the spatio-temporal data of each individual simulation, we employ a space-time cube volume rendering. All views support linking and brushing interaction to allow users to select and highlight subsets of the data simultaneously across multiple views. We use the resulting interactive, multi-view visual analysis tool to explore the nine simulations and also to compare them to data from experimental setups. Our main findings include new insights into ranking of simulation results with respect to experimental data, and the development of gravity fingers in simulations.}
}
@misc{heyen2023tabcomp,
    title           = {Visual Guitar Tab Comparison},
    author          = {Frank Heyen, Alejandro Gabino Diaz Mendoza, Quynh Quang Ngo, Michael Sedlmair},
    year            = {2023},
    month           = {11},
    doi             = {https://doi.org/10.48550/arXiv.2311.14726},
    url             = {https://ismir2023program.ismir.net/lbd_357.html},
    note            = {International Conference on Music Information Retrieval (ISMIR) Late-breaking Demo},
    eprint          = {2311.14726},
    suppl           = {https://github.com/visvar/visual-guitar-tab-comparison},
    pdf             = {https://arxiv.org/pdf/2311.14726.pdf},
    venue           = {ISMIR},
    abstract        = {We designed a visual interface for comparing different guitar tablature (tab) versions of the same piece. By automatically aligning the bars of these versions and visually encoding different metrics, our interface helps determine similarity, difficulty, and correctness. During our design, we collected and integrated feedback from musicians and finally conducted a qualitative evaluation with five guitarists. Results confirm that our interface effectively supports comparison and helps musicians choose a version appropriate for their personal skills and tastes.}
}
@inproceedings{heyen2023visual,
    title           = {Visual Overviews for Sheet Music Structure},
    author          = {Frank Heyen, Quynh Quang Ngo, Michael Sedlmair},
    year            = {2023},
    month           = {11},
    booktitle       = {Proceedings of the 24th International Society for Music Information Retrieval Conference},
    publisher       = {ISMIR},
    pages           = {692--699},
    doi             = {https://doi.org/10.5281/zenodo.10265383},
    url             = {https://ismir2023program.ismir.net/poster_216.html},
    venue           = {ISMIR},
    video           = {https://www.youtube.com/embed/zvok5W4p8oo?si=-hMQg9Yi7t3bqON9},
    suppl           = {https://github.com/visvar/sheetmusic-overviews},
    abstract        = {We propose different methods for alternative representation and visual augmentation of sheet music that help users gain an overview of general structure, repeating patterns, and the similarity of segments. To this end, we explored mapping the overall similarity between sections or bars to colors. For these mappings, we use dimensionality reduction or clustering to assign similar segments to similar colors and vice versa. To provide a better overview, we further designed simplified music notation representations, including hierarchical and compressed encodings. These overviews allow users to display whole pieces more compactly on a single screen without clutter and to find and navigate to distant segments more quickly. Our preliminary evaluation with guitarists and tablature shows that our design supports users in tasks such as analyzing structure, finding repetitions, and determining the similarity of specific segments to others}
}
@article{lee2023design,
    title           = {Design Patterns for Situated Visualization in Augmented Reality},
    author          = {Benjamin Lee, Michael Sedlmair, Dieter Schmalstieg},
    year            = {2023},
    month           = {10},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    pages           = {1--12},
    doi             = {https://doi.org/10.1109/TVCG.2023.3327398},
    suppl           = {https://www.researchgate.net/profile/Benjamin-Lee-34/publication/372551296_Lee2023DesignPatternsSitVis_Supplementalpdf/data/64be359495bbbe0c6e5ac5db/Lee2023DesignPatternsSitVis-Supplemental.pdf},
    venue           = {VIS},
    abstract        = {Situated visualization has become an increasingly popular research area in the visualization community, fueled by advancements in augmented reality (AR) technology and immersive analytics. Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations not present in traditional visualization, which researchers are now beginning to explore. However, the AR research community has an extensive history of designing graphics that are displayed in highly physical contexts. In this work, we leverage the richness of AR research and apply it to situated visualization. We derive design patterns which summarize common approaches of visualizing data in situ. The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as our own expertise. We discuss design dimensions that help to describe both our patterns and previous work in the literature. This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints imposed by the real world. We conclude by discussing future research directions that will help establish a complete understanding of the design of situated visualization, including the role of interactivity, tasks, and workflows.}
}
@misc{kouts2023lsdvis,
    title           = {LSDvis: Hallucinatory Data Visualisations in Real World Environments},
    author          = {Ari Kouts, Lonni Besançon, Michael Sedlmair, Benjamin Lee},
    year            = {2023},
    month           = {10},
    doi             = {https://doi.org/10.48550/arXiv.2312.11144},
    url             = {https://altvis.github.io/},
    eprint          = {2312.11144},
    archiveprefix   = {arXiv},
    pdf             = {https://arxiv.org/pdf/2312.11144},
    venue           = {alt.VIS},
    abstract        = {We propose the concept of 'LSDvis': the (highly exaggerated) visual blending of situated visualisations and the real-world environment to produce data representations that resemble hallucinations. Such hallucinatory visualisations incorporate elements of the physical environment, twisting and morphing their appearance such that they become part of the visualisation itself. We demonstrate LSDvis in a 'proof of proof of concept', where we use Stable Diffusion to modify images of real environments with abstract data visualisations as input. We conclude by discussing considerations of LSDvis. We hope that our work promotes visualisation designs which deprioritise saliency in favour of quirkiness and ambience.}
}
@inproceedings{hube2023work,
    title           = {Work vs. Leisure – Differences in Avatar Characteristics Depending on Social Situations},
    author          = {Natalie Hube, Melissa Reinelt, Kresimir Vidackovic, Michael Sedlmair},
    year            = {2023},
    month           = {10},
    booktitle       = {Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},
    publisher       = {ACM},
    series          = {VINCI '23},
    doi             = {https://doi.org/10.1145/3615522.3615537},
    isbn            = {9798400707513},
    abstract        = {User avatars are a critical component in collaborative Virtual Environments. A multitude of tools exist, employing various avatar types for self-representation and the representation of others. However, the optimal appearance for these avatars remains unclear. Consumer applications predominantly utilize stylized avatars, which are less prevalent in enterprise sectors. To investigate users’ avatar preferences, we conducted three user studies in both non-immersive and immersive environments, exploring whether social contexts influence virtual self-avatar selection. We generated individual avatars based on photographs of 91 participants, comprising 71 employees in the automotive industry and 20 individuals external to the company. Our findings indicate that work situations, irrespective of the occupational domain, significantly impact self-avatar choices. Conversely, we observed no correlation between the display medium or personality dimensions and avatar selection. Drawing upon our results, we provide recommendations for future avatar representation in work environments.},
    articleno       = {15},
    numpages        = {9},
    keywords        = {user study, virtual reality, avatars},
    venue           = {VINCI}
}
@article{skreinig2023guitarhero,
    title           = {guitARhero: Interactive Augmented Reality Guitar Tutorials},
    author          = {Lucchas R Skreinig, Denis Kalkofen, Ana Stanescu, Peter Mohr, Frank Heyen, Shohei Mori, Michael Sedlmair, Dieter Schmalstieg, Alexander Plopski},
    year            = {2023},
    month           = {10},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    pages           = {1--10},
    doi             = {https://doi.org/10.1109/TVCG.2023.3320266},
    pdf             = {https://ieeexplore.ieee.org/iel7/2945/4359476/10268399.pdf},
    venue           = {ISMAR},
    abstract        = {This paper presents guitARhero, an Augmented Reality application for interactively teaching guitar playing to beginners through responsive visualizations overlaid on the guitar neck. We support two types of visual guidance, a highlighting of the frets that need to be pressed and a 3D hand overlay, as well as two display scenarios, one using a desktop magic mirror and one using a video see-through head-mounted display. We conducted a user study with 20 participants to evaluate how well users could follow instructions presented with different guidance and display combinations and compare these to a baseline where users had to follow video instructions. Our study highlights the trade-off between the provided information and visual clarity affecting the user’s ability to interpret and follow instructions for fine-grained tasks. We show that the perceived usefulness of instruction integration into an HMD view highly depends on the hardware capabilities and instruction details.}
}
@misc{sayara2023designing,
    title           = {Designing Situated Dashboards: Challenges and Opportunities},
    author          = {Anika Sayara, Benjamin Lee, Carlos Quijano-Chavez, Michael Sedlmair},
    year            = {2023},
    month           = {10},
    doi             = {https://doi.org/10.1109/ISMAR-Adjunct60411.2023.00028},
    url             = {https://arxiv.org/abs/2309.02945},
    eprint          = {2309.02945},
    archiveprefix   = {arXiv},
    venue           = {ISMAR},
    abstract        = {Situated Visualization is an emerging field that unites several areas - visualization, augmented reality, human-computer interaction, and internet-of-things, to support human data activities within the ubiquitous world. Likewise, dashboards are broadly used to simplify complex data through multiple views. However, dashboards are only adapted for desktop settings, and requires visual strategies to support situatedness. We propose the concept of AR-based situated dashboards and present design considerations and challenges developed over interviews with experts. These challenges aim to propose directions and opportunities for facilitating the effective designing and authoring of situated dashboards.}
}
@inproceedings{yang2023usability,
    title           = {Usability Evaluation of an Augmented Reality System for Collaborative Fabrication between Multiple Humans and Industrial Robots},
    author          = {Xiliu Yang, Aimée Sousa Calepso, Felix Amtsberg, Achim Menges, Michael Sedlmair},
    year            = {2023},
    month           = {10},
    booktitle       = {Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
    location        = {Sydney, NSW, Australia},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {SUI '23},
    badge           = {honorablemention},
    note            = {Received an honorable mention award},
    doi             = {https://doi.org/10.1145/3607822.3614528},
    isbn            = {9798400702815},
    abstract        = {Semi-automated timber fabrication tasks demand the expertise and dexterity of human workers in addition to the use of automated robotic systems. In this paper, we introduce a human-robot collaborative system based on Augmented Reality (AR). To assess our approach, we conducted an exploratory user study on a head-mounted display (HMD) interface for task sharing between humans and an industrial robotic platform (N=16). Instead of screen-based interfaces, HMDs allowed users to receive information in-situ, regardless of their location in the workspace and the need to use their hands to handle tools or carry out tasks. We analyzed the resulting open-ended, qualitative user feedback as well as the quantitative user experience data. From the results, we derived challenges to tackle in future implementations and questions that need to be investigated to improve AR-based HRI in fabrication scenarios. The results also suggest that some aspects of human-robot interaction, like communication and trust, are more prominent when implementing a non-dyadic scenario and dealing with larger robots. The study is intended as a prequel to future work into AR-based collaboration between multiple humans and industrial robots.},
    articleno       = {18},
    numpages        = {10},
    keywords        = {user experience, robotic fabrication, human-robot interaction, augmented reality},
    venue           = {SUI}
}
@phdthesis{achberger2023moving,
    title           = {Moving Haptics Research into Practice: Four Case Studies from Automotive Engineering},
    author          = {Alexander Achberger},
    year            = {2023},
    month           = {10},
    doi             = {http://dx.doi.org/10.18419/opus-13903},
    school          = {University of Example},
    pdf             = {https://elib.uni-stuttgart.de/bitstream/11682/13922/3/Diss_Achberger_Final.pdf},
    venue           = {University of Stuttgart},
    abstract        = {Virtual Reality (VR) has gained popularity and found applications in various fields, including the automotive industry. Over the years, VR has been used in assembly, ergonomic studies, and other automotive use cases to aid development. Engineers already benefit from using VR at different stages of development, but the current setups provide only visual, auditory, and limited vibrotactile feedback on controllers. These feedback types cannot accurately simulate forces for realistic collisions or weight simulation of virtual car components. Despite ongoing research in haptic technology, there are still limitations in creating a perfect haptic feedback system that can accurately simulate all tactile and kinesthetic stimuli without hindering movement or comfort. The automotive industry needs suitable haptic feedback devices, but most research focuses on developing new devices rather than integrating them practically. We conducted four case studies to address this challenge to bridge the gap between haptic research and practical applications in automotive VR tasks. During these studies, we collaborated closely with automotive VR engineers to understand their needs and obtain feedback on using haptic devices. Our approach involved developing new haptic feedback devices based on technique- and problem-driven approaches. We created the PropellerHand, an ungrounded hand-mounted haptic device that allows forces on the hand without hindering hand use. STRIVE and STROE were developed based on problem-driven approaches, providing string-based haptic feedback devices to simulate collisions and weight. Finally, we created a multimodal haptic feedback system by combining STRIVE, STROE, and the haptic feedback glove SenseGlove, enabling users to simultaneously experience grabbing, weight, and collision feedback. Over three years, we extensively researched and implemented haptic feedback devices in practical settings. We interviewed more than 25 VR experts from the automotive industry, observed over 45 VR use cases, and collected feedback from over 200 individuals who tested our feedback devices. Based on this information, we formulated recommendations for moving haptic research into practice.}
}
@inproceedings{calepso2023exploring,
    title           = {Exploring Augmented Reality for Situated Analytics with Many Movable Physical Referents},
    author          = {Aimée Sousa Calepso, Philipp Fleck, Dieter Schmalstieg, Michael Sedlmair},
    year            = {2023},
    month           = {10},
    booktitle       = {Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology},
    location        = {Christchurch, New Zealand},
    publisher       = {Association for Computing Machinery},
    series          = {VRST '23},
    doi             = {https://doi.org/10.1145/3611659.3615700},
    isbn            = {9798400703287},
    abstract        = {Situated analytics (SitA) uses visualization in the context of physical referents, typically by using augmented reality (AR). We want to pave the way toward studying SitA in more suitable and realistic settings. Toward this goal, we contribute a testbed to evaluate SitA based on a scenario in which participants play the role of a museum curator and need to organize an exhibition of music artifacts. We conducted two experiments: First, we evaluated an AR headset interface and the testbed itself in an exploratory manner. Second, we compared the AR headset to a tablet interface. We summarize the lessons learned as guidance for designing and evaluating SitA.},
    articleno       = {6},
    numpages        = {12},
    keywords        = {Immersive analytics, Augmented Reality, Situated analytics},
    pdf             = {https://dl.acm.org/doi/pdf/10.1145/3611659.3615700},
    venue           = {VRST}
}
@inproceedings{haischt2023whats,
    title           = {What’s (Not) Tracking? Factors of Influence in Industrial Augmented Reality Tracking: A Use Case Study in an Automotive Environment},
    author          = {Jonas Haischt, Michael Sedlmair},
    year            = {2023},
    month           = {09},
    booktitle       = {Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
    location        = {Ingolstadt, Germany},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {AutomotiveUI '23},
    pages           = {42–51},
    doi             = {https://doi.org/10.1145/3580585.3607156},
    isbn            = {9798400701054},
    abstract        = {Augmented Reality (AR) is a key technology for digitization in enterprises. However, often there is a lack of stable tracking solutions when used inside manufacturing environments. Many different tracking technologies are available, yet, it can be difficult to choose the most appropriate tracking solution for different use cases with their varying conditions. In order to shed light on common tracking requirements and conditions for automotive AR use cases we conducted a use case study spanning 61 use cases within the complete product life-cycle of a large automotive manufacturer. By analyzing the gathered data we were able to note the frequency of different tracking requirements and conditions within automotive AR use cases. Based on these use cases we could also derive common factors of influence for AR tracking in the automotive industry, which show the various challenges automotive AR tracking is currently facing.},
    numpages        = {10},
    keywords        = {Automotive Industry, Augmented Reality Tracking, Use Case Study},
    venue           = {AutomotiveUI}
}
@article{amtsberg2023multi-akteur-fabrikation,
    title           = {Multi-Akteur-Fabrikation im Bauwesen},
    author          = {Felix Amtsberg, Xiliu Yang, Lior Skoury, Gili Ron, Benjamin Kaiser, Aimée Sousa Calepso, Michael Sedlmair, Alexander Verl, Thomas Wortmann, Achim Menges},
    year            = {2023},
    month           = {09},
    journal         = {Bautechnik},
    doi             = {https://doi.org/10.1002/bate.202300070},
    keywords        = {Mensch-Roboter-Kollaboration, Mensch-Roboter-Kooperation, Augmented Reality, Vorfabrikation, maschinelles Lernen, Co-Design, human-robot collaboration, human-robot collaboration, augmented reality, prefabrication, machine learning, co-design},
    eprint          = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/bate.202300070},
    abstract        = {Die aktuelle Generation von Planern ist mit der Aufgabe konfrontiert, neue, nachhaltigere Bausysteme zu entwickeln, um der Problematik der Ressourcenknappheit, der Verstädterung, dem Klima-, aber auch dem demografischen Wandel entgegenzuwirken. Hierbei ist die Frage nach deren effizienter Herstellung unter Nutzung nachwachsender Rohstoffe in den Fokus der Forschungsbemühungen gerückt. In den letzten Jahren hat der Automatisierungsgrad in der Vorfertigung unter der Agenda der produktbasierten Bausysteme zugenommen, die ein hohes Potenzial aufweisen, aber in Umgebungen, wo eine höhere Flexibilität erforderlich ist, nur bedingt zielführend sind. Die Realisierung ressourcensparender Entwürfe führt unweigerlich zu kleinen Losgrößen und einmalig zu produzierenden Bauelementen, die für eine effiziente Fabrikation anpassungsfähige Vorfertigungsanlagen für eine Vielzahl von Lösungen erfordern. Am Exzellenzcluster Integratives computerbasiertes Planen und Bauen für die Architektur (IntCDC) werden Methoden erforscht, welche mittels Mensch-Maschine-Kollaboration eine Steigerung von Flexibilität und Adaptivität der Vorfabrikation und Konstruktion von Bauwerken anstreben. Dazu werden neben der Durchführung von Untersuchungen zur Augmented-Reality-Technologie-unterstützten Integration und Kommunikation von Mensch und Maschine auch auf maschinellem Lernen basierende Trainingsmethoden und Vorhersagemodelle entwickelt. The current generation of planners is confronted with the task of developing new, more sustainable building systems to counteract the problems of resource scarcity, urbanization, climate change and demographic change. In this context, the question of their efficient production using renewable raw materials has become the focus of research efforts. In recent years, the degree of automation in prefabrication has increased under the agenda of product-based building systems, which have a high potential but are of limited use, where greater flexibility is required. The realization of resource-efficient designs inevitably leads to small lot sizes and one-off building elements that require adaptable prefabrication facilities for a variety of solutions for efficient fabrication. At the Cluster of Excellence Integrative Computational Design and Construction for Architecture (IntCDC) methods are being researched which aim to increase the flexibility and adaptivity of prefabrication and construction of buildings by means of human-machine collaboration. To this end, research is also being conducted on the integration and communication of humans and machines supported by augmented reality technology, as well as on training methods and prediction models based on machine learning.},
    pdf             = {https://papers.cumincad.org/data/works/att/ecaade2023_422.pdf},
    venue           = {Bautechnik}
}
@inproceedings{bossecker2023a,
    title           = {A Virtual Reality Simulator for Timber Fabrication Tasks Using Industrial Robotic Arms},
    author          = {Eric Bossecker, Aimée Sousa Calepso, Benjamin Kaiser, Alexander Verl, Michael Sedlmair},
    year            = {2023},
    month           = {09},
    booktitle       = {Proceedings of Mensch Und Computer (MuC)},
    publisher       = {ACM},
    series          = {MuC '23},
    pages           = {568–570},
    doi             = {https://doi.org/10.1145/3603555.3609316},
    isbn            = {9798400707711},
    abstract        = {Virtual Reality (VR) simulators are well known applications for immersive spaces. When it comes to Human-Robot Collaboration(HRC), training and safety are important aspects that can be supported by simulators. In this demo, we propose a workflow between ROS and a VR application that allows the use of real robot programming plans to control the robot’s digital twin. We also provide the results of a short evaluation that was done with 6 experts, where they provided insights for future improvement and use cases. Our demo is the first step towards a full integrated system where path planning and fabrication steps can be tested with users without any of the risk and cost that are involved when using real industrial robots.},
    numpages        = {3},
    pdf             = {https://dl.acm.org/doi/pdf/10.1145/3603555.3609316},
    venue           = {MuC}
}
@inproceedings{beck2023visual,
    title           = {Visual Planning and Analysis of Latin Formation Dance Patterns},
    author          = {Samuel Beck, Nina Doerr, Fabian Schmierer, Michael Sedlmair, Steffen Koch},
    year            = {2023},
    month           = {06},
    booktitle       = {EuroVis 2023 - Posters},
    publisher       = {The Eurographics Association},
    doi             = {https://doi.org/10.2312/evp.20231076},
    isbn            = {978-3-03868-220-2},
    editor          = {Gillmann, Christina and Krone, Michael and Lenti, Simone},
    suppl           = {https://doi.org/10.2312/evp.20231076},
    venue           = {EuroVis},
    abstract        = {Latin formation dancing is a team sport in which up to eight couples perform a coordinated choreography. A central part are the patterns formed by the dancers on the dance floor and the transitions between them. Planning and practicing patterns are some of the most challenging aspects of Latin formation dancing. Interactive visualization approaches can support instructors as well as dancers in tackling these challenges. We present a web-based visualization prototype that assists with the planning, training, and analysis of patterns. Its design was iteratively developed with the involvement of experienced formation instructors. The interface offers views of the dancers' positions and orientations, pattern transitions, poses, and analytical information like dance floor utilization and movement distances. In a first expert study with formation instructors, the prototype was well received.}
}
@article{oeney2023visual,
    title           = {Visual Gaze Labeling for Augmented Reality Studies},
    author          = {Seyda Öney, Nelusa Pathmanathan, Michael Becher, Michael Sedlmair, Daniel Weiskopf, Kuno Kurzhals},
    year            = {2023},
    month           = {06},
    journal         = {Computer Graphics Forum},
    publisher       = {The Eurographics Association and John Wiley \& Sons Ltd.},
    doi             = {https://doi.org/10.1111/cgf.14837},
    issn            = {1467-8659},
    suppl           = {https://doi.org/10.1111/cgf.14837},
    venue           = {EuroVis},
    abstract        = {Augmented Reality (AR) provides new ways for situated visualization and human-computer interaction in physical environments. Current evaluation procedures for AR applications rely primarily on questionnaires and interviews, providing qualitative means to assess usability and task solution strategies. Eye tracking extends these existing evaluation methodologies by providing indicators for visual attention to virtual and real elements in the environment. However, the analysis of viewing behavior, especially the comparison of multiple participants, is difficult to achieve in AR. Specifically, the definition of areas of interest (AOIs), which is often a prerequisite for such analysis, is cumbersome and tedious with existing approaches. To address this issue, we present a new visualization approach to define AOIs, label fixations, and investigate the resulting annotated scanpaths. Our approach utilizes automatic annotation of gaze on virtual objects and an image-based approach that also considers spatial context for the manual annotation of objects in the real world. Our results show, that with our approach, eye tracking data from AR scenes can be annotated and analyzed flexibly with respect to data aspects and annotation strategies.}
}
@article{pathmanathan2023been,
    title           = {Been There, Seen That: Visualization of Movement and 3D Eye Tracking Data from Real-World Environments},
    author          = {Nelusa Pathmanathan, Seyda Öney, Michael Becher, Michael Sedlmair, Daniel Weiskopf, Kuno Kurzhals},
    year            = {2023},
    month           = {06},
    journal         = {Computer Graphics Forum},
    publisher       = {The Eurographics Association and John Wiley & Sons Ltd.},
    doi             = {https://doi.org/10.1111/cgf.14838},
    issn            = {1467-8659},
    suppl           = {https://doi.org/10.1111/cgf.14838},
    venue           = {EuroVis},
    abstract        = {The distribution of visual attention can be evaluated using eye tracking, providing valuable insights into usability issues and interaction patterns. However, when used in real, augmented, and collaborative environments, new challenges arise that go beyond desktop scenarios and purely virtual environments. Toward addressing these challenges, we present a visualization technique that provides complementary views on the movement and eye tracking data recorded from multiple people in realworld environments. Our method is based on a space-time cube visualization and a linked 3D replay of recorded data. We showcase our approach with an experiment that examines how people investigate an artwork collection. The visualization provides insights into how people moved and inspected individual pictures in their spatial context over time. In contrast to existing methods, this analysis is possible for multiple participants without extensive annotation of areas of interest. Our technique was evaluated with a think-aloud experiment to investigate analysis strategies and an interview with domain experts to examine the applicability in other research fields.}
}
@inproceedings{chen2023reading,
    title           = {Reading Strategies for Graph Visualizations that Wrap Around in Torus Topology},
    author          = {Kun-Ting Chen, Quynh Quang Ngo, Kuno Kurzhals, Kim Marriott, Tim Dwyer, Michael Sedlmair, Daniel Weiskopf},
    year            = {2023},
    month           = {05},
    booktitle       = {Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},
    location        = {Tubingen, Germany},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {ETRA '23},
    doi             = {https://doi.org/10.1145/3588015.3589841},
    isbn            = {9798400701504},
    abstract        = {We investigate reading strategies for node-link diagrams that wrap around the boundaries in a flattened torus topology by examining eye tracking data recorded in a previous controlled study. Prior work showed that torus drawing affords greater flexibility in clutter reduction than traditional node-link representations, but impedes link-and-path exploration tasks, while repeating tiles around boundaries aids comprehension. However, it remains unclear what strategies users apply in different wrapping settings. This is important for design implications for future work on more effective wrapped visualizations for network applications, and cyclic data that could benefit from wrapping. We perform visual-exploratory data analysis of gaze data, and conduct statistical tests derived from the patterns identified. Results show distinguishable gaze behaviors, with more visual glances and transitions between areas of interest in the non-replicated layout. Full-context has more successful visual searches than partial-context, but the gaze allocation indicates that the layout could be more space-efficient.},
    articleno       = {67},
    numpages        = {7},
    keywords        = {area of interest, graph visualization, Eye tracking, scanpath analysis},
    pdf             = {https://arxiv.org/pdf/2303.17066.pdf},
    venue           = {ETRA}
}
@inproceedings{gebhardt2023auxiliary,
    title           = {Auxiliary Means to Improve Motion Guidance Memorability in Extended Reality},
    author          = {Patrick Gebhardt, Maximilian Weiß, Pascal Huszár, Xingyao Yu, Alexander Achberger, Xiaobing Zhang, Michael Sedlmair},
    year            = {2023},
    month           = {05},
    booktitle       = {2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
    pages           = {689--690},
    doi             = {https://doi.org/10.1109/VRW58643.2023.00187},
    venue           = {VRW},
    abstract        = {VR-based motion guidance systems can provide 3D movement instructions and real-time feedback for practicing movement without a live instructor. However, the precise visualization of movement paths or postures may be insufficient to learn a new motor skill, as they might make users too dependent and lead to poor performance when there is no guidance. In this paper, we propose to use enhanced error visualization, asymptotic path, increasing transparency, and haptic constraint to improve the memorability of motion guidance. Our study results indicated that adding an enhanced error feedback visualization helped the users with short-term retention.}
}
@inproceedings{wieland2023vr,
    title           = {VR, Gaze, and Visual Impairment: An Exploratory Study of the Perception of Eye Contact across different Sensory Modalities for People with Visual Impairments in Virtual Reality},
    author          = {Markus Wieland, Michael Sedlmair, Tonja-Katrin Machulla},
    year            = {2023},
    month           = {04},
    booktitle       = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
    location        = {Hamburg, Germany},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI EA '23},
    doi             = {https://doi.org/10.1145/3544549.3585726},
    isbn            = {9781450394222},
    abstract        = {As social virtual reality (VR) becomes more popular, avatars are being designed with realistic behaviors incorporating non-verbal cues like eye contact. However, perceiving eye contact during a onversation can be challenging for people with visual impairments. VR presents an opportunity to display eye contact cues in alternative ways, making them perceivable for people with visual impairments. We performed an exploratory study to gain initial insights on designing eye contact cues for people with visual impairments, including a focus group for a deeper understanding of the topic. We implemented eye contact cues via visual, auditory, and tactile sensory modalities in VR and tested these approaches with eleven participants with visual impairments and collected qualitative feedback. The results show that visual cues indicating the gaze direction were preferred, but auditory and tactile cues were also prevalent as they do not superimpose additional visual information.},
    articleno       = {313},
    numpages        = {6},
    keywords        = {eye contact, assistive technology, visual impairment, social virtual reality},
    video           = {https://www.youtube.com/embed/D9Z9058RyxQ?si=IMkB2hFgwtQElIMX},
    venue           = {CHI}
}
@inproceedings{rigling2023inyourface,
    title           = {In Your Face!': Visualizing Fitness Tracker Data in Augmented Reality},
    author          = {Sebastian Rigling, Xingyao Yu, Michael Sedlmair},
    year            = {2023},
    month           = {04},
    booktitle       = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
    location        = {Hamburg, Germany},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI EA '23},
    doi             = {https://doi.org/10.1145/3544549.3585912},
    isbn            = {9781450394222},
    abstract        = {The benefits of augmented reality (AR) have been demonstrated in both medicine and fitness, while its application in areas where these two fields overlap has been barely explored. We argue that AR opens up new opportunities to interact with, understand and share personal health data. To this end, we developed an app prototype that uses a Snapchat-like face filter to visualize personal health data from a fitness tracker in AR. We tested this prototype in two pilot studies and found that AR does have potential in this type of application. We suggest that AR cannot replace the current interfaces of smartwatches and mobile apps, but it can pick up where current technology falls short in creating intrinsic motivation and personal health awareness. We also provide ideas for future work in this direction.},
    articleno       = {176},
    numpages        = {7},
    keywords        = {fitness tracker, health, visualization, augmented reality},
    video           = {https://www.youtube.com/embed/2l5ImWh8uJY?si=YcE0ksPbiHQIOxXQ},
    suppl           = {https://dl.acm.org/doi/10.1145/3544549.3585912},
    pdf             = {https://dl.acm.org/doi/pdf/10.1145/3544549.3585912},
    venue           = {CHI}
}
@inproceedings{doerr2023bees,
    title           = {Bees, Birds and Butterflies: Investigating the Influence of Distractors on Visual Attention Guidance Techniques},
    author          = {Nina Doerr, Katrin Angerbauer, Melissa Reinelt, Michael Sedlmair},
    year            = {2023},
    month           = {04},
    booktitle       = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
    location        = {Hamburg, Germany},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI EA '23},
    doi             = {https://doi.org/10.1145/3544549.3585816},
    isbn            = {9781450394222},
    abstract        = {Visual attention guidance methods direct the viewer’s gaze in immersive environments by visually highlighting elements of interest. The highlighting can be done, for instance, by adding a colored circle around elements, adding animated swarms (HiveFive), or removing objects from one eye in a stereoscopic display (Deadeye). We contribute a controlled user experiment (N=30) comparing these three techniques under the influence of visual distractors, such as bees flying by. Our results show that Circle and HiveFive performed best in terms of task performance and qualitative feedback, and were largely robust against different levels of distractions. Furthermore, we discovered a high mental demand for Deadeye.},
    articleno       = {51},
    numpages        = {7},
    keywords        = {perception, visual attention, attention guidance, virtual reality},
    video           = {https://www.youtube.com/watch?v=17ALkNxm1mI},
    venue           = {CHI}
}
@article{wortmeier2023configuring,
    title           = {Configuring Augmented Reality Users: Analysing YouTube Commercials to Understand Industry Expectations},
    author          = {Ann-Kathrin Wortmeier, Aimée Sousa Calepso, Cordula Kropp, Michael Sedlmair, Daniel Weiskopf},
    year            = {2023},
    month           = {01},
    journal         = {Behaviour \& Information Technology},
    publisher       = {Taylor & Francis},
    pages           = {1--16},
    doi             = {https://doi.org/10.1080/0144929X.2022.2163693},
    abstract        = {Commercial videos are often used to familiarise potential buyers and users with new technologies and their possibilities. In addition, presenting visions of future applications is a way to configure users and define social worlds of technology use. We analyse 30 YouTube videos featuring augmented reality (AR) devices in industrial manufacturing and construction, to explore how these commercial videos situate AR technology and future users by showcasing techno-euphoric promises and imagined use cases. With a video analysis based on Grounded Theory and Situational Analysis, we untangle the promises of AR for manufacturing and construction work; second, we present two prevailing configurations of AR users: ‘experts in situ’ and ‘smart dummies’; and third, we discuss how YouTube videos put forward developmental expectations. In addition, we identify discrepancies between expectations and foreseeable requirements in construction work. Finally, our research could contribute to a more holistic understanding of workplaces and socially robust AR applications.},
    suppl           = {https://www.tandfonline.com/doi/figure/10.1080/0144929X.2022.2163693?scroll=top&needAccess=true&role=tab},
    venue           = {Behaviour & Information Technology}
}
@inproceedings{hube2022study,
    title           = {Study on the Influence of Upper Limb Representations and Haptic Feedback in Virtual Reality},
    author          = {Natalie Hube, Alexander Achberger, Philipp Liepert, Jonas Vogelsang, Kresimir Vidackovic, Michael Sedlmair},
    year            = {2022},
    month           = {12},
    booktitle       = {2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
    pages           = {802--807},
    doi             = {https://doi.org/10.1109/ISMAR-Adjunct57072.2022.00172},
    pdf             = {https://ieeexplore.ieee.org/iel7/9973799/9974160/09974563.pdf},
    venue           = {ISMAR-adjunct},
    abstract        = {Visual representations are used in various digitization processes to display objects in immersive environments. In industrial environments, the self-representation can be very helpful when reviewing 3D models to conform realistic proportions, particularly in combi-nation with haptic feedback that goes beyond vibrating controllers. For instance, haptic feedback in combination with a virtual representation supports working on use cases where collision is an important part to maintain data quality. To understand the dependency of both, we conducted a pilot study with 15 users from the automotive sector to examine the influence of upper limb representations on haptic feedback in Virtual Reality. Each participant was assigned with one of three upper limb representations which was used in different scenarios using haptic feedback devices. Overall, we found that the realistic arm representation was rated highest in terms of perceived realism and achieved the best task performance.}
}
@inproceedings{rau2022visualization,
    title           = {Visualization for AI-Assisted Composing},
    author          = {Simeon Rau, Frank Heyen, Stefan Wagner, Michael Sedlmair},
    year            = {2022},
    month           = {12},
    booktitle       = {Proceedings of the 23rd International Society for Music Information Retrieval Conference (ISMIR)},
    publisher       = {ISMIR},
    address         = {Bengaluru, India},
    pages           = {151--159},
    doi             = {https://doi.org/10.5281/zenodo.7316618},
    url             = {https://ismir2022program.ismir.net/poster_217.html},
    note            = {Was nominated for a best paper award},
    badge           = {nomination},
    venue           = {ISMIR},
    suppl           = {https://github.com/visvar/vis-ai-comp},
    acks            = {This work was funded by the Cyber Valley Research Fund and the Artificial Intelligence Software Academy (AISA).},
    abstract        = {We propose a visual approach for interactive, AI-assisted composition that serves as a compromise between fully automatic and fully manual composition. Instead of generating a whole piece, the AI takes on the role of an assistant that generates short melodies for the composer to choose from and adapt. In an iterative process, the composer queries the AI for continuations or alternative fill-ins, chooses a suggestion, and adds it to the piece. As listening to many suggestions would take time, we explore different ways to visualize them, to allow the composer to focus on the most interesting-looking melodies. We also present the results of a qualitative evaluation with five composers.}
}
@misc{heyen2022postermidicontroller,
    title           = {A Web-Based MIDI Controller for Music Live Coding},
    author          = {Frank Heyen, Dilara Aygün, Michael Sedlmair},
    year            = {2022},
    month           = {12},
    url             = {https://ismir2022program.ismir.net/lbd_379.html},
    note            = {International Conference on Music Information Retrieval (ISMIR) Late-breaking Demo},
    suppl           = {https://github.com/visvar/sonic-pi-controller},
    acks            = {This work was funded by the Cyber Valley Research Fund.},
    venue           = {ISMIR},
    abstract        = {We contribute an interactive visual frontend to live coding environments, which allows live coders and performers to influence the behavior of their code more quickly and efficiently. Users can trigger actions and change parameters via instruments, buttons, and sliders, instead of only inside the code. For instance, toggling a loop or controlling a fading effect through mouse or touch interaction on a screen is faster than editing code. While this kind of control has already been possible with hardware MIDI devices, we provide a more accessible, easy-to-use, and customizable alternative that only requires a web browser. With examples, we show how users perform live-coded music faster and more easily with our design compared to using pure code.}
}
@misc{heyen2022augmented,
    title           = {Augmented Reality Visualization for Musical Instrument Learning},
    author          = {Frank Heyen, Michael Sedlmair},
    year            = {2022},
    month           = {12},
    url             = {https://ismir2022program.ismir.net/lbd_376.html},
    note            = {International Conference on Music Information Retrieval (ISMIR) Late-breaking Demo},
    acks            = {This work was funded by the Cyber Valley Research Fund.},
    venue           = {ISMIR},
    abstract        = {We contribute two design studies for augmented reality visualizations that support learning musical instruments. First, we designed simple, glanceable encodings for drum kits, which we display through a projector. As second instrument, we chose guitar and designed visualizations to be displayed either on a screen as an augmented mirror or an an optical see-through AR headset. These modalities allow us to also show information around the instrument and in 3D. We evaluated our prototypes through case studies and our results demonstrate the general effectivity and revealed design-related and technical limitations.}
}
@inproceedings{gebhardt2022molecusense,
    title           = {MolecuSense: Using Force-Feedback Gloves for Creating and Interacting with Ball-and-Stick Molecules in VR},
    author          = {Patrick Gebhardt, Xingyao Yu, Andreas Köhn, Michael Sedlmair},
    year            = {2022},
    month           = {10},
    booktitle       = {Proceedings of the 15th International Symposium on Visual Information Communication and Interaction (VINCI)},
    publisher       = {ACM},
    series          = {VINCI '22},
    doi             = {https://doi.org/10.1145/3554944.3554956},
    isbn            = {9781450398060},
    abstract        = {We contribute MolecuSense, a virtual version of a physical molecule construction kit, based on visualization in Virtual Reality (VR) and interaction with force-feedback gloves. Targeting at chemistry education, our goal is to make virtual molecule structures more tangible. Results of an initial user study indicate that the VR molecular construction kit was positively received. Compared to a physical construction kit, the VR molecular construction kit is on the same level in terms of natural interaction. Besides, it fosters the typical digital advantages though, such as saving, exporting, and sharing of molecules. Feedback from the study participants has also revealed potential future avenues for tangible molecule visualizations.},
    articleno       = {15},
    numpages        = {5},
    keywords        = {human-computer interaction, Virtual reality, digital gloves},
    pdf             = {https://arxiv.org/pdf/2203.09577.pdf},
    venue           = {VINCI}
}
@inproceedings{angerbauer2022toward,
    title           = {Toward Inclusion and Accessibility in Visualization Research: Speculations on Challenges, Solution Strategies, and Calls for Action (Position Paper)},
    author          = {Katrin Angerbauer, Michael Sedlmair},
    year            = {2022},
    month           = {10},
    booktitle       = {2022 IEEE Evaluation and Beyond - Methodological Approaches for Visualization (BELIV)},
    pages           = {20--27},
    doi             = {https://doi.org/10.1109/BELIV57783.2022.00007},
    url             = {https://arxiv.org/abs/2209.05224},
    venue           = {BELIV},
    abstract        = {Inclusion and accessibility in visualization research have gained increasing attention in recent years. However, many challenges still remain to be solved on the road toward a more inclusive, shared-experience-driven visualization design and evaluation process. In this position paper, we discuss challenges and speculate about potential solutions, based on related work, our own research, as well as personal experiences. The goal of this paper is to start discussions on the role of accessibility and inclusion in visualization design and evaluation.}
}
@misc{chen2022not,
    title           = {Not As Easy As You Think - Experiences and Lessons Learnt from Creating a Visualization Image Typology},
    author          = {Jian Chen, Petra Isenberg, Robert S Laramee, Tobias Isenberg, Michael Sedlmair, Torsten Mōller, Han-Wei Shen},
    year            = {2022},
    month           = {10},
    publisher       = {arXiv},
    doi             = {https://doi.org/10.48550/arXiv.2209.07533},
    url             = {https://hal.inria.fr/hal-03812031/},
    copyright       = {Creative Commons Attribution 4.0 International},
    keywords        = {Graphics (cs.GR), Multimedia (cs.MM), FOS: Computer and information sciences, FOS: Computer and information sciences},
    pdf             = {https://arxiv.org/pdf/2209.07533},
    venue           = {arXiv},
    abstract        = {We present and discuss the results of a two-year qualitative analysis of images published in IEEE Visualization (VIS) papers. Specifically, we derive a typology of 13 visualization image types, coded to distinguish visualizations and several image characteristics. The categorization process required much more time and was more difficult than we initially thought. The resulting typology and image analysis may serve a number of purposes: to study the evolution of the community and its research output over time, to facilitate the categorization of visualization images for the purpose of teaching, to identify visual designs for evaluation purposes, or to enable progress towards standardization in visualization. In addition to the typology and image characterization, we provide a dataset of 6,833 tagged images and an online tool that can be used to explore and analyze the large set of tagged images. We thus facilitate a discussion of the diverse visualizations used and how they are published and communicated in our community.}
}
@misc{richer2022scalability,
    title           = {Scalability in Visualization},
    author          = {Gaëlle Richer, Alexis Pister, Moataz Abdelaal, Jean-Daniel Fekete, Michael Sedlmair, Daniel Weiskopf},
    year            = {2022},
    month           = {10},
    publisher       = {arXiv},
    doi             = {https://doi.org/10.1109/TVCG.2022.3231230},
    url             = {https://arxiv.org/abs/2210.06562},
    copyright       = {Creative Commons Attribution 4.0 International},
    keywords        = {Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences, H.5.2},
    pdf             = {https://arxiv.org/pdf/2210.06562},
    venue           = {TVCG},
    abstract        = {We introduce a conceptual model for scalability designed for visualization research. With this model, we systematically analyze over 120 visualization publications from 1990-2020 to characterize the different notions of scalability in these works. While many papers have addressed scalability issues, our survey identifies a lack of consistency in the use of the term in the visualization research community. We address this issue by introducing a consistent terminology meant to help visualization researchers better characterize the scalability aspects in their research. It also helps in providing multiple methods for supporting the claim that a work is 'scalable'. Our model is centered around an effort function with inputs and outputs. The inputs are the problem size and resources, whereas the outputs are the actual efforts, for instance, in terms of computational run time or visual clutter. We select representative examples to illustrate different approaches and facets of what scalability can mean in visualization literature. Finally, targeting the diverse crowd of visualization researchers without a scalability tradition, we provide a set of recommendations for how scalability can be presented in a clear and consistent way to improve fair comparison between visualization techniques and systems and foster reproducibility.}
}
@article{abdelaal2022comparative,
    title           = {Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network Representations},
    author          = {Moataz Abdelaal, Nathan D. Schiele, Katrin Angerbauer, Kuno Kurzhals, Michael Sedlmair, Daniel Weiskopf},
    year            = {2022},
    month           = {10},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    volume          = {29},
    number          = {1},
    pages           = {896--906},
    doi             = {https://doi.org/10.1109/TVCG.2022.3209427},
    venue           = {TVCG},
    abstract        = {This work investigates and compares the performance of node-link diagrams, adjacency matrices, and bipartite layouts for visualizing networks. In a crowd-sourced user study ( n=150 ), we measure the task accuracy and completion time of the three representations for different network classes and properties. In contrast to the literature, which covers mostly topology-based tasks (e.g., path finding) in small datasets, we mainly focus on overview tasks for large and directed networks. We consider three overview tasks on networks with 500 nodes: (T1) network class identification, (T2) cluster detection, and (T3) network density estimation, and two detailed tasks: (T4) node in-degree vs. out-degree and (T5) representation mapping, on networks with 50 and 20 nodes, respectively. Our results show that bipartite layouts are beneficial for revealing the overall network structure, while adjacency matrices are most reliable across the different tasks.}
}
@article{morariu2022predicting,
    title           = {Predicting User Preferences of Dimensionality Reduction Embedding Quality},
    author          = {Cristina Morariu, Adrien Bibal, Rene Cutura, Benoit Frenay, Michael Sedlmair},
    year            = {2022},
    month           = {09},
    journal         = {IEEE Transactions on Visualization and Computer Graphics (TVCG)},
    publisher       = {IEEE},
    pages           = {1--11},
    doi             = {https://doi.org/10.1109/TVCG.2022.3209449},
    suppl           = {https://doi.org/10.1109/TVCG.2022.3209449/mm1},
    acks            = {Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (Project A08).},
    venue           = {TVCG},
    url             = {https://arxiv.org/abs/2105.09275},
    abstract        = {A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional hyper-parametrization (e.g., t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D embeddings is usually qualitatively decided, by setting embeddings side-by-side and letting human judgment decide which embedding is the best. In this work, we propose a quantitative way of evaluating embeddings, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select “good” and “misleading” views between scatterplots of low-dimensional embeddings of image datasets, simulating the way people usually select embeddings. We use the study data as labels for a set of quality metrics for a supervised machine learning model whose purpose is to discover and quantify what exactly people are looking for when deciding between embeddings. With the model as a proxy for human judgments, we use it to rank embeddings on new datasets, explain why they are relevant, and quantify the degree of subjectivity when people select preferred embeddings.}
}
@article{klein2022immersive,
    title           = {Immersive Analytics: An Overview},
    author          = {Karsten Klein, Michael Sedlmair, Falk Schreiber},
    year            = {2022},
    month           = {09},
    journal         = {it - Information Technology},
    volume          = {64},
    number          = {4-5},
    pages           = {155--168},
    doi             = {https://doi.org/10.1515/itit-2022-0037},
    pdf             = {https://www.degruyter.com/document/doi/10.1515/itit-2022-0037/pdf},
    venue           = {it - Information Technology},
    abstract        = {Immersive Analytics is concerned with the systematic examination of the benefits and challenges of using immersive environments for data analysis, and the development of corresponding designs that improve the quality and efficiency of the analysis process. While immersive technologies are now broadly available, practical solutions haven’t received broad acceptance in real-world applications outside of several core areas, and proper guidelines on the design of such solutions are still under development. Both fundamental research and applications bring together topics and questions from several fields, and open a wide range of directions regarding underlying theory, evidence from user studies, and practical solutions tailored towards the requirements of application areas. We give an overview on the concepts, topics, research questions, and challenges.}
}
@article{ngo2022machine,
    title           = {Machine learning meets visualization – Experiences and lessons learned},
    author          = {Quynh Quang Ngo, Frederik L. Dennig, Daniel A. Keim, Michael Sedlmair},
    year            = {2022},
    month           = {09},
    journal         = {it - Information Technology},
    doi             = {https://doi.org/10.1515/itit-2022-0034},
    acks            = {This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) within the projects A03 and A08 of TRR 161 (Project-ID 251654672)},
    venue           = {it - Information Technology},
    abstract        = {In this article, we discuss how Visualization (VIS) with Machine Learning (ML) could mutually benefit from each other. We do so through the lens of our own experience working at this intersection for the last decade. Particularly we focus on describing how VIS supports explaining ML models and aids ML-based Dimensionality Reduction techniques in solving tasks such as parameter space analysis. In the other direction, we discuss approaches showing how ML helps improve VIS, such as applying ML-based automation to improve visualization design. Based on the examples and our own perspective, we describe a number of open research challenges that we frequently encountered in our endeavors to combine ML and VIS.}
}
@inproceedings{dosdall2022toward,
    title           = {Toward In-Situ Authoring of Situated Visualization with Chorded Keyboards},
    author          = {Sarah Dosdall, Katrin Angerbauer, Leonel Merino, Michael Sedlmair, Daniel Weiskopf},
    year            = {2022},
    month           = {08},
    booktitle       = {Proceedings of the 15th International Symposium on Visual Information Communication and Interaction},
    publisher       = {ACM},
    series          = {VINCI '22},
    doi             = {https://doi.org/10.1145/3554944.3554970},
    isbn            = {9781450398060},
    abstract        = {Authoring situated visualizations in-situ is challenging due to the need of writing code in a mobile and highly dynamic fashion. To provide better support for that, we define requirements for text input methods that target situated visualization authoring. We identify wearable chorded keyboards as a potentially suitable method that fulfills some of these requirements. To further investigate this approach, we tailored a chorded keyboard device to visualization authoring, developed a learning application, and conducted a pilot user study. Our results confirm that learning a high number of chords is the main barrier for adoption, as in other application areas. Based on that, we discuss ideas on how chorded keyboards with a strongly reduced alphabet, hand gestures, and voice recognition might be used as a viable, multi-modal support for authoring situated visualizations in-situ.},
    articleno       = {14},
    numpages        = {5},
    keywords        = {mixed reality, chorded keyboard, pilot study, Situated visualization},
    pdf             = {https://dl.acm.org/doi/pdf/10.1145/3554944.3554970},
    venue           = {VINCI}
}
@article{cutura2022hagrid,
    title           = {Hagrid: using Hilbert and Gosper curves to gridify scatterplots},
    author          = {Rene Cutura, Cristina Morariu, Zhanglin Cheng, Yunhai Wang, Daniel Weiskopf, Michael Sedlmair},
    year            = {2022},
    month           = {07},
    journal         = {Journal of Visualization},
    publisher       = {Springer},
    pages           = {1--17},
    doi             = {https://doi.org/10.1007/s12650-022-00854-7},
    acks            = {This work was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 251654672 - TRR 161.},
    funding         = {Open Access funding enabled and organized by Projekt DEAL},
    pdf             = {https://link.springer.com/content/pdf/10.1007/s12650-022-00854-7.pdf},
    venue           = {JoV},
    abstract        = {A common enhancement of scatterplots represents points as small multiples, glyphs, or thumbnail images. As this encoding often results in overlaps, a general strategy is to alter the position of the data points, for instance, to a grid-like structure. Previous approaches rely on solving expensive optimization problems or on dividing the space that alter the global structure of the scatterplot. To find a good balance between efficiency and neighborhood and layout preservation, we propose HAGRID, a technique that uses space-filling curves (SFCs) to “gridify” a scatterplot without employing expensive collision detection and handling mechanisms. Using SFCs ensures that the points are plotted close to their original position, retaining approximately the same global structure. The resulting scatterplot is mapped onto a rectangular or hexagonal grid, using Hilbert and Gosper curves. We discuss and evaluate the theoretic runtime of our approach and quantitatively compare our approach to three state-of-the-art gridifying approaches, DGRID, Small multiples with gaps SMWG, and CorrelatedMultiples CMDS, in an evaluation comprising 339 scatterplots. Here, we compute several quality measures for neighborhood preservation together with an analysis of the actual runtimes. The main results show that, compared to the best other technique, HAGRID is faster by a factor of four, while achieving similar or even better quality of the gridified layout. Due to its computational efficiency, our approach also allows novel applications of gridifying approaches in interactive settings, such as removing local overlap upon hovering over a scatterplot.}
}
@article{achberger2022touching,
    title           = {Touching Data with PropellerHand},
    author          = {Alexander Achberger, Frank Heyen, Kresimir Vidackovic, Michael Sedlmair},
    year            = {2022},
    month           = {06},
    journal         = {Journal of Visualization},
    doi             = {https://doi.org/10.1007/s12650-022-00859-2},
    acks            = {Partially supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy – EXC 2120/1 – 390831618},
    funding         = {Open Access funding enabled and organized by Projekt DEAL},
    pdf             = {https://link.springer.com/content/pdf/10.1007/s12650-022-00859-2.pdf},
    venue           = {Journal of Visualization},
    abstract        = {Immersive analytics often takes place in virtual environments which promise the users immersion. To fulfill this promise, sensory feedback, such as haptics, is an important component, which is however not well supported yet. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our prototype through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality. Additionally, we conducted a second user study in the field of immersive visualization to investigate the potential benefits of PropellerHand there.}
}
@inproceedings{calepso2022cardlearner,
    title           = {cARdLearner: Using Expressive Virtual Agents when Learning Vocabulary in Augmented Reality},
    author          = {Aimée Sousa Calepso, Natalie Hube, Noah Berenguel Senn, Vincent Brandt, Michael Sedlmair},
    year            = {2022},
    month           = {04},
    booktitle       = {CHI Conference on Human Factors in Computing Systems Extended Abstracts},
    location        = {New Orleans, LA, USA},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI EA '22},
    doi             = {https://doi.org/10.1145/3491101.3519631},
    isbn            = {9781450391566},
    abstract        = {Augmented reality (AR) has a diverse range of applications, including language teaching. When studying a foreign language, one of the biggest challenges learners face is memorizing new vocabulary. While augmented holograms are a promising means of supporting this memorization process, few studies have explored their potential in the language learning context. We demonstrate the possibility of using flashcard along with an expressive holographic agent on vocabulary learning. Users scan a flashcard and play an animation that is connected with an emotion related to the word they are seeing. Our goal is to propose an alternative to the traditional use of flashcards, and also introduce another way of using AR in the association process.},
    articleno       = {245},
    numpages        = {6},
    keywords        = {contextual learning, augmented reality, language learning},
    venue           = {CHI}
}
@misc{heyen2022cellovis,
    title           = {Immersive Visual Analysis of Cello Bow Movements},
    author          = {Frank Heyen, Yannik Kohler, Sebastian Triebener, Sebastian Rigling, Michael Sedlmair},
    year            = {2022},
    month           = {04},
    publisher       = {arXiv},
    doi             = {https://doi.org/10.48550/arxiv.2203.13316},
    url             = {https://arxiv.org/abs/2203.13316},
    copyright       = {Creative Commons Attribution Share Alike 4.0 International},
    keywords        = {Human-Computer Interaction (cs.HC), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences},
    acks            = {Funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy - EXC 2075 - 390740016, and by Cyber Valley (InstruData project).},
    venue           = {CHI IMI Workshop},
    abstract        = {We propose a 3D immersive visualization environment for analyzing the right hand movements of a cello player. To achieve this, we track the position and orientation of the cello bow and record audio. As movements mostly occur in a shallow volume and the motion is therefore mostly two-dimensional, we use the third dimension to encode time. Our concept further explores various mappings from motion and audio data to spatial and other visual attributes. We work in close cooperation with a cellist and plan to evaluate our prototype through a user study with a group of cellists in the near future.}
}
@misc{heyen2022datadriven,
    title           = {Data-Driven Visual Reflection on Music Instrument Practice},
    author          = {Frank Heyen, Quynh Quang Ngo, Kuno Kurzhals, Michael Sedlmair},
    year            = {2022},
    month           = {04},
    publisher       = {arXiv},
    doi             = {https://doi.org/10.48550/arxiv.2203.13320},
    url             = {https://arxiv.org/abs/2203.13320},
    copyright       = {Creative Commons Attribution Share Alike 4.0 International},
    keywords        = {Human-Computer Interaction (cs.HC), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences},
    acks            = {Funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy - EXC 2075 - 390740016, and by Cyber Valley (InstruData project).},
    venue           = {CHI IMI Workshop},
    abstract        = {We propose a data-driven approach to music instrument practice that allows studying patterns and long-term trends through visualization. Inspired by life logging and fitness tracking, we imagine musicians to record their practice sessions over the span of months or years. The resulting data in the form of MIDI or audio recordings can then be analyzed sporadically to track progress and guide decisions. Toward this vision, we started exploring various visualization designs together with a group of nine guitarists, who provided us with data and feedback over the course of three months.}
}
@inproceedings{hube2022using,
    title           = {Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study},
    author          = {Natalie Hube,  Kresimir Vidackovic, Michael Sedlmair},
    year            = {2022},
    month           = {04},
    booktitle       = {CHI Conference on Human Factors in Computing Systems Extended Abstracts},
    location        = {New Orleans, LA, USA},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI EA '22},
    doi             = {https://doi.org/10.1145/3491101.3519822},
    isbn            = {9781450391566},
    abstract        = {Virtual avatars are widely used for collaborating in virtual environments. Yet, often these avatars lack expressiveness to determine a state of mind. Prior work has demonstrated efective usage of determining emotions and animated lip movement through analyzing mere audio tracks of spoken words. To provide this information on a virtual avatar, we created a natural audio data set consisting of 17 audio fles from which we then extracted the underlying emotion and lip movement. To conduct a pilot study, we developed a prototypical system that displays the extracted visual parameters and then maps them on a virtual avatar while playing the corresponding audio fle. We tested the system with 5 participants in two conditions: (i) while seeing the virtual avatar only an audio fle was played. (ii) In addition to the audio fle, the extracted facial visual parameters were displayed on the virtual avatar. Our results suggest the validity of using additional visual parameters in the avatars face as it helps to determine emotions. We conclude with a brief discussion on the outcomes and their implications on future work.},
    articleno       = {260},
    numpages        = {7},
    keywords        = {emotion, avatars, lip synchronization, virtual reality},
    venue           = {CHI}
}
@inproceedings{angerbauer2022accessibility,
    title           = {Accessibility for Color Vision Deficiencies: Challenges and Findings of a Large Scale Study on Paper Figures},
    author          = {Katrin Angerbauer, Nils Rodrigues, Rene Cutura, Seyda Öney, Nelusa Pathmanathan, Cristina Morariu, Daniel Weiskopf, Michael Sedlmair},
    year            = {2022},
    month           = {04},
    booktitle       = {CHI Conference on Human Factors in Computing Systems},
    publisher       = {ACM},
    series          = {CHI '22},
    doi             = {https://doi.org/10.1145/3491102.3502133},
    isbn            = {9781450391573},
    abstract        = {We present an exploratory study on the accessibility of images in publications when viewed with color vision deficiencies (CVDs). The study is based on 1,710 images sampled from a visualization dataset (VIS30K) over five years. We simulated four CVDs on each image. First, four researchers (one with a CVD) identified existing issues and helpful aspects in a subset of the images. Based on the resulting labels, 200 crowdworkers provided  30,000 ratings on present CVD issues in the simulated images. We analyzed this data for correlations, clusters, trends, and free text comments to gain a first overview of paper figure accessibility. Overall, about 60 % of the images were rated accessible. Furthermore, our study indicates that accessibility issues are subjective and hard to detect. On a meta-level, we reflect on our study experience to point out challenges and opportunities of large-scale accessibility studies for future research directions.},
    articleno       = {134},
    numpages        = {23},
    keywords        = {color vision deficiency, crowdsourcing, visualization, accessibility},
    video           = {https://www.youtube.com/embed/fhhg0k2LLkk?si=a9r5Lz_19-jUopYy},
    acks            = {Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (projects A08 and B01). We thank all our study participants and in particular Sajid Baloch for his valuable input.},
    venue           = {CHI}
}
@inproceedings{achberger2022stroe,
    title           = {STROE: An Ungrounded String-Based Weight Simulation Device},
    author          = {Alexander Achberger, Pirathipan Arulrajah, Michael Sedlmair, Kresimir Vidackovic},
    year            = {2022},
    month           = {04},
    booktitle       = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
    pages           = {112--120},
    doi             = {https://doi.org/10.1109/VR51125.2022.00029},
    issn            = {2642-5254},
    abstract        = {We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user’s hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. It is also quieter than other devices, and is comparatively cheap. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users’ perceived realism and immersion of VR scenes.},
    video           = {https://www.youtube.com/embed/9edaBf7VqNY?si=VWxdCksykS-mrmmO},
    venue           = {VR}
}
@inproceedings{tkachev2022metaphorical,
    title           = {Metaphorical Visualization: Mapping Data to Familiar Concepts},
    author          = {Gleb Tkachev, Rene Cutura, Michael Sedlmair, Steffen Frey, Thomas Ertl},
    year            = {2022},
    month           = {04},
    booktitle       = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
    publisher       = {ACM},
    series          = {CHI EA},
    doi             = {https://doi.org/10.1145/3491101.3516393},
    isbn            = {9781450391566},
    articleno       = {10},
    numpages        = {10},
    keywords        = {Metaphor, Image embedding, Word embedding},
    video           = {https://gleb-t.com/media/videos/MetaphorVisFastForward_LowBitrate.mp4},
    suppl           = {https://gleb-t.com/publication/metaphor-vis/metaphorvis_2022_supplemental.pdf},
    acks            = {Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy EXC 2075 – 390740016, and TRR 161 – 251654672.},
    pdf             = {https://gleb-t.com/publication/metaphor-vis/metaphor-vis.pdf},
    venue           = {alt.CHI},
    abstract        = {We present a new approach to visualizing data that is well-suited for personal and casual applications. The idea is to map the data to another dataset that is already familiar to the user, and then rely on their existing knowledge to illustrate relationships in the data. We construct the map by preserving pairwise distances or by maintaining relative values of specific data attributes. This metaphorical mapping is very flexible and allows us to adapt the visualization to its application and target audience. We present several examples where we map data to different domains and representations. This includes mapping data to cat images, encoding research interests with neural style transfer and representing movies as stars in the night sky. Overall, we find that although metaphors are not as accurate as the traditional techniques, they can help design engaging and personalized visualizations}
}
@inproceedings{skreinig2022ar,
    title           = {AR Hero: Generating Interactive Augmented Reality Guitar Tutorials},
    author          = {Lucchas Ribeiro Skreinig, Ana Stanescu, Shohei Mori, Frank Heyen, Peter Mohr, Michael Sedlmair, Dieter Schmalstieg, Denis Kalkofen},
    year            = {2022},
    month           = {03},
    booktitle       = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
    pages           = {395--401},
    doi             = {https://doi.org/10.1109/VRW55335.2022.00086},
    venue           = {VRW},
    abstract        = {We introduce a system capable of generating interactive Augmented Reality guitar tutorials by parsing common digital guitar tablature and by capturing the performance of an expert using a multi-camera array. Instructions are presented to the user in an Augmented Reality application using either an abstract visualization, a 3D virtual hand, or a 3D video. To support individual users at different skill levels the system provides full control of the playback of a tutorial, including its speed and looping behavior, while delivering live feedback on the user’s performance.}
}
@article{fleck2022ragrug,
    title           = {RagRug: A Toolkit for Situated Analytics},
    author          = {Philipp Fleck, Aimée Sousa Calepso, Sebastian Hubenschmid, Michael Sedlmair, Dieter Schmalstieg},
    year            = {2022},
    month           = {03},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    publisher       = {IEEE},
    doi             = {https://doi.org/10.1109/TVCG.2022.3157058},
    video           = {https://www.youtube.com/embed/mFxSdvQhSVU?si=agxhf3DfcKgmwn4z},
    venue           = {TVCG},
    abstract        = {We present RagRug, an open-source toolkit for situated analytics. The abilities of RagRug go beyond previous immersive analytics toolkits by focusing on specific requirements emerging when using augmented reality (AR) rather than virtual reality. RagRug combines state of the art visual encoding capabilities with a comprehensive physical-virtual model, which lets application developers systematically describe the physical objects in the real world and their role in AR. We connect AR visualization with data streams from the Internet of Things using distributed dataflow. To this aim, we use reactive programming patterns so that visualizations become context-aware, i.e., they adapt to events coming in from the environment. The resulting authoring system is low-code; it emphasises describing the physical and the virtual world and the dataflow between the elements contained therein. We describe the technical design and implementation of RagRug, and report on five example applications illustrating the toolkit's abilities.}
}
@article{eirich2022rfx,
    title           = {RfX: A Design Study for the Interactive Exploration of a Random Forest to Enhance Testing Procedures for Electrical Engines},
    author          = {Joscha Eirich, M. Münch, Dominik Jäckle, Michael Sedlmair, Jakob Bonart, Tobias Schreck},
    year            = {2022},
    month           = {03},
    journal         = {Computer Graphics Forum (CGF)},
    doi             = {https://doi.org/10.1111/cgf.14452},
    keywords        = {human–computer interfaces, interaction, visual analytics, visualization},
    abstract        = {Random Forests (RFs) are a machine learning (ML) technique widely used across industries. The interpretation of a given RF usually relies on the analysis of statistical values and is often only possible for data analytics experts. To make RFs accessible to experts with no data analytics background, we present RfX, a Visual Analytics (VA) system for the analysis of a RF's decision-making process. RfX allows to interactively analyse the properties of a forest and to explore and compare multiple trees in a RF. Thus, its users can identify relationships within a RF's feature subspace and detect hidden patterns in the model's underlying data. We contribute a design study in collaboration with an automotive company. A formative evaluation of RFX was carried out with two domain experts and a summative evaluation in the form of a field study with five domain experts. In this context, new hidden patterns such as increased eccentricities in an engine's rotor by observing secondary excitations of its bearings were detected using analyses made with RfX. Rules derived from analyses with the system led to a change in the company's testing procedures for electrical engines, which resulted in 80% reduced testing time for over 30% of all components.},
    pdf             = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14452},
    venue           = {CGF}
}
@article{abdelaal2022visualization,
    title           = {Visualization for Architecture, Engineering, and Construction: Shaping the Future of Our Built World},
    author          = {Moataz Abdelaal, Felix Amtsberg, Michael Becher, Rebeca Duque Estrada, Fabian Kannenberg, Aimée Sousa Calepso, Hans Jakob Wagner, Guido Reina, Michael Sedlmair, Achim Menges, Daniel Weiskopf},
    year            = {2022},
    month           = {02},
    journal         = {IEEE Computer Graphics and Applications},
    volume          = {42},
    number          = {2},
    pages           = {10--20},
    doi             = {https://doi.org/10.1109/MCG.2022.3149837},
    issn            = {1558-1756},
    abstract        = {Our built world is one of the most important factors for a livable future, accounting for massive impact on resource and energy use, as well as climate change, but also the social and economic aspects that come with population growth. The architecture, engineering, and construction industry is facing the challenge that it needs to substantially increase its productivity, let alone the quality of buildings of the future. In this article, we discuss these challenges in more detail, focusing on how digitization can facilitate this transformation of the industry, and link them to opportunities for visualization and augmented reality research. We illustrate solution strategies for advanced building systems based on wood and fiber.},
    funding         = {10.13039/501100001659-Deutsche Forschungsgemeinschaft (Grant Number: 279064222—SFB 1244)},
    venue           = {CG&A}
}
@inproceedings{rau2021visual,
    title           = {Visual Support for Human-AI Co-Composition},
    author          = {Simeon Rau, Frank Heyen, Michael Sedlmair},
    year            = {2021},
    month           = {11},
    booktitle       = {Extended Abstracts for the Late-Breaking Demo Session of the 22nd Int. Society for Music Information Retrieval Conf. (ISMIR)},
    url             = {https://archives.ismir.net/ismir2021/latebreaking/000014.pdf},
    acks            = {This work was funded by the Cyber Valley Research Fund – Project InstruData.},
    pdf             = {https://archives.ismir.net/ismir2021/latebreaking/000014.pdf},
    venue           = {ISMIR},
    abstract        = {We propose a visual approach for AI-assisted music composition, where the user interactively generates, selects, and adapts short melodies. Based on an entered start melody, we automatically generate multiple continuation samples. Repeating this step and in turn generating continuations for these samples results in a tree or graph of melodies. We visualize this structure with two visualizations, where nodes display the piano roll of the corresponding sample. By interacting with these visualizations, the user can quickly listen to, choose, and adapt melodies, to iteratively create a composition. A third visualization provides an overview over larger numbers of samples, allowing for insights into the AI's predictions and the sample space.}
}
@inproceedings{hube2021ismar,
    title           = {VR Collaboration in Large Companies: An Interview Study on the Role of Avatars},
    author          = {Natalie Hube, Katrin Angerbauer, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair},
    year            = {2021},
    month           = {11},
    booktitle       = {2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
    pages           = {139--144},
    doi             = {https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00037},
    keywords        = {Avatars;Design methodology;Collaboration;Virtual environments;Companies;Tools;Interviews;Human-centered computing;Collaborative and social computing;Human computer interaction (HCI);HCI design and evaluation methods},
    venue           = {ISMAR},
    abstract        = {Collaboration is essential in companies and often physical presence is required, thus, more and more Virtual Reality (VR) systems are used to work together remotely. To support social interaction, human representations in form of avatars are used in collaborative virtual environment (CVE) tools. However, up to now, the avatar representations often are limited in their design and functionality, which may hinder effective collaboration. In our interview study, we explored the status quo of VR collaboration in a large automotive company setting with a special focus on the role of avatars. We collected inter-view data from 21 participants, from which we identified challenges of current avatar representations used in our setting. Based on these findings, we discuss design suggestions for avatars in a company setting, which aim to improve social interaction. As opposed to state-of-the-art research, we found that users within the context of a large automotive company have an altered need with respect to avatar representations.}
}
@inproceedings{cutura2021visap,
    title           = {DaRt: Generative Art using Dimensionality Reduction Algorithms},
    author          = {Rene Cutura, Katrin Angerbauer, Frank Heyen, Natalie Hube, Michael Sedlmair},
    year            = {2021},
    month           = {10},
    booktitle       = {2021 IEEE VIS Arts Program (VISAP)},
    pages           = {59--72},
    doi             = {https://doi.org/10.1109/VISAP52981.2021.00013},
    organization    = {IEEE},
    video           = {https://www.youtube.com/embed/pOcksJOiAPw?si=VQkT9qFkz-M8CEO5},
    acks            = {Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 251654672 - TRR 161},
    pdf             = {https://renecutura.eu/pdfs/DaRt.pdf},
    venue           = {VIS},
    abstract        = {Dimensionality Reduction (DR) is a popular technique that is often used in Machine Learning and Visualization communities to analyze high-dimensional data. The approach is empirically proven to be powerful for uncovering previously unseen structures in the data. While observing the results of the intermediate optimization steps of DR algorithms, we coincidently discovered the artistic beauty of the DR process. With enthusiasm for the beauty, we decided to look at DR from a generative art lens rather than their technical application aspects and use DR techniques to create artwork. Particularly, we use the optimization process to generate images, by drawing each intermediate step of the optimization process with some opacity over the previous intermediate result. As another alternative input, we used a neural-network model for face-landmark detection, to apply DR to portraits, while maintaining some facial properties, resulting in abstracted facial avatars. In this work, we provide such a collection of such artwork.}
}
@inproceedings{grossmann2021vis,
    title           = {Does the Layout Really Matter? A Study on Visual Model Accuracy Estimation},
    author          = {Nicolas Grossmann, Jürgen Bernard, Michael Sedlmair, Manuela Waldner},
    year            = {2021},
    month           = {10},
    booktitle       = {2021 IEEE Visualization Conference (VIS)},
    pages           = {61--65},
    doi             = {https://doi.org/10.1109/VIS49827.2021.9623326},
    url             = {https://arxiv.org/abs/2110.07188},
    keywords        = {Visualization;Layout;Estimation;Inspection;Predictive models;Prediction algorithms;Complexity theory;Human-centered computing;Visualization;Empirical studies in visualization Human-centered computing;Visualization design and evaluation methods},
    pdf             = {https://arxiv.org/abs/2110.07188},
    venue           = {VIS},
    abstract        = {In visual interactive labeling, users iteratively assign labels to data items until the machine model reaches an acceptable accuracy. A crucial step of this process is to inspect the model’s accuracy and decide whether it is necessary to label additional elements. In scenarios with no or very little labeled data, visual inspection of the predictions is required. Similarity-preserving scatterplots created through a dimensionality reduction algorithm are a common visualization that is used in these cases. Previous studies investigated the effects of layout and image complexity on tasks like labeling. However, model evaluation has not been studied systematically. We present the results of an experiment studying the influence of image complexity and visual grouping of images on model accuracy estimation. We found that users outperform traditional automated approaches when estimating a model’s accuracy. Furthermore, while the complexity of images impacts the overall performance, the layout of the items in the plot has little to no effect on estimations.}
}
@inproceedings{achberger2021uist,
    title           = {Strive: String-Based Force Feedback for Automotive Engineering},
    author          = {Alexander Achberger, Fabian Aust, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair},
    year            = {2021},
    month           = {10},
    booktitle       = {Symp. User Interface Software and Technology},
    location        = {Virtual Event, USA},
    publisher       = {ACM},
    series          = {UIST},
    pages           = {841–853},
    doi             = {https://doi.org/10.1145/3472749.3474790},
    isbn            = {9781450386357},
    abstract        = {The large potential of force feedback devices for interacting in Virtual Reality (VR) has been illustrated in a plethora of research prototypes. Yet, these devices are still rarely used in practice and it remains an open challenge how to move this research into practice. To that end, we contribute a participatory design study on the use of haptic feedback devices in the automotive industry. Based on a 10-month observing process with 13 engineers, we developed STRIVE, a string-based haptic feedback device. In addition to the design of STRIVE, this process led to a set of requirements for introducing haptic devices into industrial settings, which center around a need for flexibility regarding forces, comfort, and mobility. We evaluated STRIVE with 16 engineers in five different day-to-day automotive VR use cases. The main results show an increased level of trust and perceived safety as well as further challenges towards moving haptics research into practice.},
    numpages        = {13},
    keywords        = {participatory design, haptic device, force feedback, automotive},
    venue           = {UIST}
}
@article{eirich2021vis,
    title           = {IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines},
    author          = {Joscha Eirich, Jakob Bonart, Dominik Jäckle, Michael Sedlmair, Ute Schmid, Kai Fischbach, Tobias Schreck, Jürgen Bernard},
    year            = {2021},
    month           = {09},
    journal         = {IEEE Trans. Visualization and Computer Graphics (TVCG, Proc. VIS 2021)},
    doi             = {https://doi.org/10.1109/TVCG.2021.3114797},
    note            = {Received a best paper award},
    badge           = {bestpaper},
    venue           = {TVCG},
    abstract        = {In this design study, we present IRVINE, a Visual Analytics (VA) system, which facilitates the analysis of acoustic data to detect and understand previously unknown errors in the manufacturing of electrical engines. In serial manufacturing processes, signatures from acoustic data provide valuable information on how the relationship between multiple produced engines serves to detect and understand previously unknown errors. To analyze such signatures, IRVINE leverages interactive clustering and data labeling techniques, allowing users to analyze clusters of engines with similar signatures, drill down to groups of engines, and select an engine of interest. Furthermore, IRVINE allows to assign labels to engines and clusters and annotate the cause of an error in the acoustic raw measurement of an engine. Since labels and annotations represent valuable knowledge, they are conserved in a knowledge database to be available for other stakeholders. We contribute a design study, where we developed IRVINE in four main iterations with engineers from a company in the automotive sector. To validate IRVINE, we conducted a field study with six domain experts. Our results suggest a high usability and usefulness of IRVINE as part of the improvement of a real-world manufacturing process. Specifically, with IRVINE domain experts were able to label and annotate produced electrical engines more than 30% faster.}
}
@inproceedings{achberger2021vinci,
    title           = {PropellerHand: Hand-Mounted, Propeller-Based Force Feedback Device},
    author          = {Alexander Achberger, Frank Heyen, Kresimir Vidackovic, Michael Sedlmair},
    year            = {2021},
    month           = {09},
    booktitle       = {International Symposium on Visual Information Communication and Interaction (VINCI)},
    publisher       = {ACM},
    pages           = {4:1--4:8},
    doi             = {https://doi.org/10.1145/3481549.3481563},
    venue           = {VINCI},
    abstract        = {Immersive analytics is a fast growing field that is often applied in virtual reality (VR). VR environments often lack immersion due to missing sensory feedback when interacting with data. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our final version through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality.}
}
@inproceedings{cutura2021vinci,
    title           = {Hagrid — Gridify Scatterplots with Hilbert and Gosper Curves},
    author          = {Rene Cutura, Cristina Morariu, Zhanglin Cheng, Yunhai Wang, Daniel Weiskopf, Michael Sedlmair},
    year            = {2021},
    month           = {09},
    booktitle       = {The 14th International Symposium on Visual Information Communication and Interaction},
    location        = {Potsdam, Germany},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {VINCI 2021},
    doi             = {https://doi.org/10.1145/3481549.3481569},
    isbn            = {9781450386470},
    articleno       = {1},
    numpages        = {8},
    keywords        = {Grid layout, Neighborhood-preserving., Space-filling curve},
    video           = {https://www.youtube.com/embed/E_XP31_JzGY?si=ZjWoiiDrRg4CKE2L},
    suppl           = {https://renecutura.eu/pdfs/hagrid_supplemental.pdf},
    acks            = {This work was supported by the BMK FFG ICT of the Future program via the ViSciPub project (no. 867378), and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161.},
    pdf             = {https://renecutura.eu/pdfs/hagrid.pdf},
    venue           = {VINCI},
    abstract        = {A common enhancement of scatterplots represents points as small multiples, glyphs, or thumbnail images. As this encoding often results in overlaps, a general strategy is to alter the position of the data points, for instance, to a grid-like structure. Previous approaches rely on solving expensive optimization problems or on dividing the space that alter the global structure of the scatterplot. To find a good balance between efficiency and neighborhood and layout preservation, we propose Hagrid, a technique that uses space-filling curves (SFCs) to “gridify” a scatterplot without employing expensive collision detection and handling mechanisms. Using SFCs ensures that the points are plotted close to their original position, retaining approximately the same global structure. The resulting scatterplot is mapped onto a rectangular or hexagonal grid, using Hilbert and Gosper curves. We discuss and evaluate the theoretic runtime of our approach and quantitatively compare our approach to three state-of-the-art gridifying approaches, DGrid, Small multiples with gaps SMWG, and CorrelatedMultiples CMDS, in an evaluation comprising 339 scatterplots. Here, we compute several quality measures for neighborhood preservation together with an analysis of the actual runtimes. The main results show that, compared to the best other technique, Hagrid is faster by a factor of four, while achieving similar or even better quality of the gridified layout. Due to its computational efficiency, our approach also allows novel applications of gridifying approaches in interactive settings, such as removing local overlap upon hovering over a scatterplot.}
}
@inproceedings{krauter2021muc,
    title           = {Don't Catch It: An Interactive Virtual-Reality Environment to Learn About COVID-19 Measures Using Gamification Elements},
    shorttitle      = {Don't Catch It},
    author          = {Christian Krauter, Jonas Vogelsang, Aimée Sousa Calepso, Katrin Angerbauer, Michael Sedlmair},
    year            = {2021},
    month           = {09},
    booktitle       = {Proc. Mensch Und Computer},
    publisher       = {ACM},
    series          = {MuC},
    pages           = {593--596},
    doi             = {https://doi.org/10.1145/3473856.3474031},
    isbn            = {978-1-4503-8645-6},
    abstract        = {The world is still under the influence of the COVID-19 pandemic. Even though vaccines are deployed as rapidly as possible, it is still necessary to use other measures to reduce the spread of the virus. Measures such as social distancing or wearing a mask receive a lot of criticism. Therefore, we want to demonstrate a serious game to help the players understand these measures better and show them why they are still necessary. The player of the game has to avoid other agents to keep their risk of a COVID-19 infection low. The game uses Virtual Reality through a Head-Mounted-Display to deliver an immersive and enjoyable experience. Gamification elements are used to engage the user with the game while they explore various environments. We also implemented visualizations that help the user with social distancing.},
    venue           = {MuC},
    video           = {https://www.youtube.com/embed/kjqu9W2DIko?si=rteZtuE5FEBnOYfP},
    footnoteindices     = {0,1},
    footnotetext       = {contributed equally}
}
@article{rijken2021illegible,
    title           = {Illegible Semantics: Exploring the Design Space of Metal Logos},
    author          = {Gerrit J. Rijken, Rene Cutura, Frank Heyen, Michael Sedlmair, Michael Correll, Jason Dykes, Noeska Smit},
    year            = {2021},
    month           = {09},
    journal         = {CoRR},
    volume          = {abs/2109.01688},
    doi             = {https://doi.org/10.48550/arXiv.2109.01688},
    url             = {https://arxiv.org/abs/2109.01688},
    eprinttype      = {arXiv},
    eprint          = {2109.01688},
    video           = {https://www.youtube.com/embed/BZOdIhU-mrA?si=DcJ4XS7314XDS12z},
    suppl           = {http://illegiblesemantics.com},
    pdf             = {https://arxiv.org/ftp/arxiv/papers/2109/2109.01688.pdf},
    venue           = {alt.VIS},
    abstract        = {The logos of metal bands can be by turns gaudy, uncouth, or nearly illegible. Yet, these logos work: they communicate sophisticated notions of genre and emotional affect. In this paper we use the design considerations of metal logos to explore the space of “illegible semantics”: the ways that text can communicate information at the cost of readability, which is not always the most important objective. In this work, drawing on formative visualization theory, professional design expertise, and empirical assessments of a corpus ofmetal band logos, we describe a design space of metal logos and present a tool through which logo characteristics can be explored through visualization. We investigate ways in which logo designers imbue their text with meaning and consider opportunities and implications for visualization more widely.}
}
@article{bernard2021tiis,
    title           = {A Taxonomy of Property Measures to Unify Active Learning and Human-centered Approaches to Data Labeling},
    author          = {Jürgen Bernard,  Marco Hutter, Michael Sedlmair, Matthias Zeppelzauer, Tamara Munzner},
    year            = {2021},
    month           = {08},
    journal         = {ACM Transactions on Interactive Intelligent Systems (TiiS)},
    volume          = {11},
    number          = {3-4},
    pages           = {1--42},
    doi             = {https://doi.org/10.1145/3439333},
    venue           = {TiiS},
    abstract        = {Strategies for selecting the next data instance to label, in service of generating labeled data for machine learning, have been considered separately in the machine learning literature on active learning and in the visual analytics literature on human-centered approaches. We propose a unified design space for instance selection strategies to support detailed and fine-grained analysis covering both of these perspectives. We identify a concise set of 15 properties, namely measureable characteristics of datasets or of machine learning models applied to them, that cover most of the strategies in these literatures. To quantify these properties, we introduce Property Measures (PM) as fine-grained building blocks that can be used to formalize instance selection strategies. In addition, we present a taxonomy of PMs to support the description, evaluation, and generation of PMs across four dimensions: machine learning (ML) Model Output, Instance Relations, Measure Functionality, and Measure Valence. We also create computational infrastructure to support qualitative visual data analysis: a visual analytics explainer for PMs built around an implementation of PMs using cascades of eight atomic functions. It supports eight analysis tasks, covering the analysis of datasets and ML models using visual comparison within and between PMs and groups of PMs, and over time during the interactive labeling process. We iteratively refined the PM taxonomy, the explainer, and the task abstraction in parallel with each other during a two-year formative process, and show evidence of their utility through a summative evaluation with the same infrastructure. This research builds a formal baseline for the better understanding of the commonalities and differences of instance selection strategies, which can serve as the stepping stone for the synthesis of novel strategies in future work.}
}
@article{kraus2021cga,
    title           = {The Value of Immersive Visualization},
    author          = {Matthias Kraus, Karsten Klein, Johannes Fuchs, Daniel A Keim, Falk Schreiber, Michael Sedlmair},
    year            = {2021},
    month           = {06},
    journal         = {IEEE Computer Graphics and Applications (CG\&A)},
    volume          = {41},
    number          = {4},
    pages           = {125--132},
    doi             = {https://doi.org/10.1109/MCG.2021.3075258},
    venue           = {CG&A},
    abstract        = {In recent years, research on immersive environments has experienced a new wave of interest, and immersive analytics has been established as a new research field. Every year, a vast amount of different techniques, applications, and user studies are published that focus on employing immersive environments for visualizing and analyzing data. Nevertheless, immersive analytics is still a relatively unexplored field that needs more basic research in many aspects and is still viewed with skepticism. Rightly so, because in our opinion, many researchers do not fully exploit the possibilities offered by immersive environments and, on the contrary, sometimes even overestimate the power of immersive visualizations. Although a growing body of papers has demonstrated individual advantages of immersive analytics for specific tasks and problems, the general benefit of using immersive environments for effective analytic tasks remains controversial. In this article, we reflect on when and how immersion may be appropriate for the analysis and present four guiding scenarios. We report on our experiences, discuss the landscape of assessment strategies, and point out the directions where we believe immersive visualizations have the greatest potential.}
}
@inproceedings{ling2021icdar,
    title           = {Document Domain Randomization for Deep Learning Document Layout Extraction},
    author          = {Meng Ling, Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert S Laramee, Han-Wei Shen, Jian Wu, C Lee Giles},
    year            = {2021},
    month           = {05},
    booktitle       = {Document Analysis and Recognition (ICDAR)},
    publisher       = {Springer International Publishing},
    pages           = {497--513},
    doi             = {https://doi.org/10.1007/978-3-030-86549-8_32},
    isbn            = {978-3-030-86549-8},
    url             = {https://arxiv.org/abs/2105.14931},
    abstract        = {We present document domain randomization (DDR), the first successful transfer of convolutional neural networks (CNNs) trained only on graphically rendered pseudo-paper pages to real-world document segmentation. DDR renders pseudo-document pages by modeling randomized textual and non-textual contents of interest, with user-defined layout and font styles to support joint learning of fine-grained classes. We demonstrate competitive results using our DDR approach to extract nine document classes from the benchmark CS-150 and papers published in two domains, namely annual meetings of Association for Computational Linguistics (ACL) and IEEE Visualization (VIS). We compare DDR to conditions of style mismatch, fewer or more noisy samples that are more easily obtained in the real world. We show that high-fidelity semantic information is not necessary to label semantic classes but style mismatch between train and test can lower model accuracy. Using smaller training samples had a slightly detrimental effect. Finally, network models still achieved high test accuracy when correct labels are diluted towards confusing labels; this behavior hold across several classes.},
    pdf             = {https://arxiv.org/pdf/2105.14931},
    venue           = {ICDAR}
}
@misc{ling2021visimages,
    title           = {Three Benchmark Datasets for Scholarly Article Layout Analysis},
    author          = {Meng Ling, Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert Laramee, Han-Wei Shen, Jian Wu, Clyde Lee Giles},
    year            = {2021},
    month           = {05},
    publisher       = {IEEE Dataport},
    doi             = {https://doi.org/10.21227/326q-bf39},
    url             = {https://ieee-dataport.org/open-access/three-benchmark-datasets-scholarly-article-layout-analysis},
    suppl           = {https://ieee-dataport.org/open-access/three-benchmark-datasets-scholarly-article-layout-analysis},
    venue           = {IEEE Dataport},
    abstract        = {This dataset contains three benchmark datasets as part of the scholarly output of an ICDAR 2021 paper: Meng Ling, Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert S. Laramee, Han-Wei Shen, Jian Wu, and C. Lee Giles, Document Domain Randomization for Deep Learning Document Layout Extraction, 16th International Conference on Document Analysis and Recognition (ICDAR) 2021. September 5-10, Lausanne, Switzerland. This dataset contains nine class lables: abstract, algorithm, author, body text, caption, equation, figure, table, and title. * Dataset 1: CS-150x, an extension of the classical benchmark dataset CS-150 from three classes (figure, table, and caption) to nine classes, 1176 pages, Clark, C., Divvala, S.: Looking beyond text: Extracting figures, tables and captions from com- puter science papers. In: Workshops at the 29th AAAI Conference on Artificial Intelligence (2015), https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10092. * Dataset 2: ACL300, 300 randomly sampled articles (or 2508 pages) from the 55,759 papers scraped from the ACL anthology website; https://www.aclweb.org/anthology/. * Dataset 3: VIS300, about 10% (or 2619 pages) of the document pages in randomly partitioned articles from 26,350 VIS paper pages published in  Chen, J., Ling, M., Li, R., Isenberg, P., Isenberg, T., Sedlmair, M., Möller, T., Laramee, R.S., Shen, H.W., Wünsche, K., Wang, Q.: VIS30K: A collection of figures and tables from IEEE visualization conference publications. IEEE Trans. Vis. Comput. Graph. 27 (2021), to appear doi: 10.1109/TVCG.2021.3054916. This dataset is also available online at https://web.cse.ohio-state.edu/~chen.8028/ICDAR2021Benchmark/.}
}
@article{bernard2021proseco,
    title           = {ProSeCo: Visual Analysis of Class Separation Measures and Dataset Characteristics},
    author          = {Jürgen Bernard, Marco Hutter, Matthias Zeppelzauer, Michael Sedlmair, Tamara Munzner},
    year            = {2021},
    month           = {05},
    journal         = {Computers \& Graphics},
    publisher       = {Elsevier},
    volume          = {96},
    pages           = {48--60},
    doi             = {https://doi.org/10.1016/j.cag.2021.03.004},
    venue           = {C&G},
    abstract        = {Class separation is an important concept in machine learning and visual analytics. We address the visual analysis of class separation measures for both high-dimensional data and its corresponding projections into 2D through dimensionality reduction (DR) methods. Although a plethora of separation measures have been proposed, it is difficult to compare class separation between multiple datasets with different characteristics, multiple separation measures, and multiple DR methods. We present ProSeCo, an interactive visualization approach to support comparison between up to 20 class separation measures and up to 4 DR methods, with respect to any of 7 dataset characteristics: dataset size, dataset dimensions, class counts, class size variability, class size skewness, outlieriness, and real-world vs. synthetically generated data. ProSeCo supports (1) comparing across measures, (2) comparing high-dimensional to dimensionally-reduced 2D data across measures, (3) comparing between different DR methods across measures, (4) partitioning with respect to a dataset characteristic, (5) comparing partitions for a selected characteristic across measures, and (6) inspecting individual datasets in detail. We demonstrate the utility of ProSeCo in two usage scenarios, using datasets [1] posted at https://osf.io/epcf9/.}
}
@article{chen2021tvcg,
    title           = {VIS30K: A Collection of Figures and Tables from IEEE Visualization Conference Publications},
    author          = {Jian Chen, Meng Ling, Rui Li, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Torsten Möller, Robert S Laramee, Han-Wei Shen, Katharina Wünsche, Qiru Wang},
    year            = {2021},
    month           = {01},
    journal         = {IEEE Transactions on Visualization and Computer Graphics (TVCG)},
    volume          = {27},
    number          = {9},
    pages           = {3826--3833},
    doi             = {https://doi.org/10.1109/TVCG.2021.3054916},
    venue           = {TVCG},
    abstract        = {We present the VIS30K dataset, a collection of 29,689 images that represents 30 years of figures and tables from each track of the IEEE Visualization conference series (Vis, SciVis, InfoVis, VAST). VIS30K's comprehensive coverage of the scientific literature in visualization not only reflects the progress of the field but also enables researchers to study the evolution of the state-of-the-art and to find relevant work based on graphical content. We describe the dataset and our semi-automatic collection process, which couples convolutional neural networks (CNN) with curation. Extracting figures and tables semi-automatically allows us to verify that no images are overlooked or extracted erroneously. To improve quality further, we engaged in a peer-search process for high-quality figures from early IEEE Visualization papers. With the resulting data, we also contribute VISImageNavigator (VIN, visimagenavigator.github.io ), a web-based tool that facilitates searching and exploring VIS30K by author names, paper keywords, title and abstract, and years.}
}
@article{waldner2021iv,
    title           = {Linking Unstructured Evidence to Structured Observations},
    author          = {Manuela Waldner, Thomas Geymayer, Dieter Schmalstieg, Michael Sedlmair},
    year            = {2021},
    month           = {01},
    journal         = {Information Visualization},
    publisher       = {SAGE Publications},
    volume          = {20},
    number          = {1},
    pages           = {47--65},
    doi             = {https://doi.org/10.1177/1473871620986249},
    venue           = {IV},
    abstract        = {Many professionals, like journalists, writers, or consultants, need to acquire information from various sources, make sense of this unstructured evidence, structure their observations, and finally create and deliver their product, such as a report or a presentation. In formative interviews, we found that tools allowing structuring of observations are often disconnected from the corresponding evidence. Therefore, we designed a sensemaking environment with a flexible observation graph that visually ties together evidence in unstructured documents with the user’s structured knowledge. This is achieved through bi-directional deep links between highlighted document portions and nodes in the observation graph. In a controlled study, we compared users’ sensemaking strategies using either the observation graph or a simple text editor on a large display. Results show that the observation graph represents a holistic, compact representation of users’ observations, which can be linked to unstructured evidence on demand. In contrast, users taking textual notes required much more display space to spatially organize source documents containing unstructured evidence. This implies that spatial organization is a powerful strategy to structure observations even if the available space is limited.}
}
@inproceedings{kraus2020ismar,
    title           = {A Comparative Study of Orientation Support Tools in Virtual Reality Environments with Virtual Teleportation},
    author          = {Matthias Kraus, Hanna Schaefer, Philipp Meschenmoser, Daniel Schweitzer, Daniel Keim, Michael Sedlmair, Johannes Fuchs},
    year            = {2020},
    month           = {12},
    booktitle       = {2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
    pages           = {227--238},
    doi             = {https://doi.org/10.1109/ISMAR50242.2020.00046},
    keywords        = {Visualization;Virtual environments;Tools;Teleportation;Time measurement;Task analysis;Space heating;Human-centered computing—;Human computer interaction (HCI)—;Interaction paradigms—;Virtual reality},
    venue           = {ISMAR},
    abstract        = {Movement-compensating interactions like teleportation are commonly deployed techniques in virtual reality environments. Although practical, they tend to cause disorientation while navigating. Previous studies show the effectiveness of orientation-supporting tools, such as trails, in reducing such disorientation and reveal different strengths and weaknesses of individual tools. However, to date, there is a lack of a systematic comparison of those tools when teleportation is used as a movement-compensating technique, in particular under consideration of different tasks. In this paper, we compare the effects of three orientation-supporting tools, namely minimap, trail, and heatmap. We conducted a quantitative user study with 48 participants to investigate the accuracy and efficiency when executing four exploration and search tasks. As dependent variables, task performance, completion time, space coverage, amount of revisiting, retracing time, and memorability were measured. Overall, our results indicate that orientation-supporting tools improve task completion times and revisiting behavior. The trail and heatmap tools were particularly useful for speed-focused tasks, minimal revisiting, and space coverage. The minimap increased memorability and especially supported retracing tasks. These results suggest that virtual reality systems should provide orientation aid tailored to the specific tasks of the users.}
}
@inproceedings{merino2020ismar,
    title           = {Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009-2019)},
    author          = {Leonel Merino, Magdalena Schwarzl, Matthias Kraus, Michael Sedlmair, Dieter Schmalstieg, Daniel Weiskopf},
    year            = {2020},
    month           = {12},
    booktitle       = {2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
    pages           = {438--451},
    doi             = {https://doi.org/10.1109/ISMAR50242.2020.00069},
    issn            = {1554-7868},
    venue           = {ISMAR},
    abstract        = {We present a systematic review of 45S papers that report on evaluations in mixed and augmented reality (MR/AR) published in ISMAR, CHI, IEEE VR, and UIST over a span of 11 years (2009-2019). Our goal is to provide guidance for future evaluations of MR/AR approaches. To this end, we characterize publications by paper type (e.g., technique, design study), research topic (e.g., tracking, rendering), evaluation scenario (e.g., algorithm performance, user performance), cognitive aspects (e.g., perception, emotion), and the context in which evaluations were conducted (e.g., lab vs. in-thewild). We found a strong coupling of types, topics, and scenarios. We observe two groups: (a) technology-centric performance evaluations of algorithms that focus on improving tracking, displays, reconstruction, rendering, and calibration, and (b) human-centric studies that analyze implications of applications and design, human factors on perception, usability, decision making, emotion, and attention. Amongst the 458 papers, we identified 248 user studies that involved 5,761 participants in total, of whom only 1,619 were identified as female. We identified 43 data collection methods used to analyze 10 cognitive aspects. We found nine objective methods, and eight methods that support qualitative analysis. A majority (216/248) of user studies are conducted in a laboratory setting. Often (138/248), such studies involve participants in a static way. However, we also found a fair number (30/248) of in-the-wild studies that involve participants in a mobile fashion. We consider this paper to be relevant to academia and industry alike in presenting the state-of-the-art and guiding the steps to designing, conducting, and analyzing results of evaluations in MR/AR.}
}
@article{boukhelifa2020eviva,
    title           = {Challenges in Evaluating Interactive Visual Machine Learning Systems},
    author          = {Nadia Boukhelifa, Anastasia Bezerianos, Remco Chang, Chris Collins, Steven Drucker, Alex Endert, Jessica Hullman, Chris North, Michael Sedlmair},
    year            = {2020},
    month           = {11},
    journal         = {IEEE Computer Graphics and Applications},
    volume          = {40},
    number          = {6},
    pages           = {88--96},
    doi             = {https://doi.org/10.1109/MCG.2020.3017064},
    venue           = {CG&A},
    abstract        = {In interactive visual machine learning (IVML), humans and machine learning algorithms collaborate to achieve tasks mediated by interactive visual interfaces. This human-in-the-loop approach to machine learning brings forth not only numerous intelligibility, trust, and usability issues, but also many open questions with respect to the evaluation of the IVML system, both as separate components, and as a holistic entity that includes both human and machine intelligence. This article describes the challenges and research gaps identified in an IEEE VIS workshop on the evaluation of IVML systems.}
}
@inproceedings{hube2020comparing,
    title           = {Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion},
    author          = {Natalie Hube, Oliver Lenz, Lars Engeln, Rainer Groh, Michael Sedlmair},
    year            = {2020},
    month           = {11},
    booktitle       = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
    pages           = {30--35},
    doi             = {https://doi.org/10.1109/ISMAR-Adjunct51615.2020.00023},
    venue           = {ISMAR},
    abstract        = {We present a user study comparing a pre-evaluated mapping approach with a state-of-the-art direct mapping method of facial expressions for emotion judgment in an immersive setting. At its heart, the pre-evaluated approach leverages semiotics, a theory used in linguistic. In doing so, we want to compare pre-evaluation with an approach that seeks to directly map real facial expressions onto their virtual counterparts. To evaluate both approaches, we conduct a controlled lab study with 22 participants. The results show that users are significantly more accurate in judging virtual facial expressions with pre-evaluated mapping. Additionally, participants were slightly more confident when deciding on a presented emotion. We could not find any differences regarding potential Uncanny Valley effects. However, the pre-evaluated mapping shows potential to be more convenient in a conversational scenario.}
}
@inproceedings{yu2020perspective,
    title           = {Perspective Matters: Design Implications for Motion Guidance in Mixed Reality},
    author          = {Xingyao Yu, Katrin Angerbauer, Peter Mohr, Denis Kalkofen, Michael Sedlmair},
    year            = {2020},
    month           = {11},
    booktitle       = {2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
    pages           = {577--587},
    doi             = {https://doi.org/10.1109/ISMAR50242.2020.00085},
    venue           = {ISMAR},
    abstract        = {We investigate how Mixed Reality (MR) can be used to guide human body motions, such as in physiotherapy, dancing, or workout applications. While first MR prototypes have shown promising results, many dimensions of the design space behind such applications remain largely unexplored. To better understand this design space, we approach the topic from different angles by contributing three user studies. In particular, we take a closer look at the influence of the perspective, the characteristics of motions, and visual guidance on different user performance measures. Our results indicate that a first-person perspective performs best for all visible motions, whereas the type of visual instruction plays a minor role. From our results we compile a set of considerations that can guide future work on the design of instructions, evaluations, and the technical setup of MR motion guidance systems.}
}
@inproceedings{balestrucci2020beliv,
    title           = {Pipelines Bent, Pipelines Broken: Interdisciplinary Self-Reflection on the Impact of COVID-19 on Current and Future Research (Position Paper)},
    author          = {Priscilla Balestrucci, Katrin Angerbauer, Cristina Morariu, Robin Welsch, Lewis L Chuang, Daniel Weiskopf, Marc O Ernst, Michael Sedlmair},
    year            = {2020},
    month           = {10},
    booktitle       = {2020 IEEE Workshop on Evaluation and Beyond - Methodological Approaches to Visualization (BELIV)},
    pages           = {11--18},
    doi             = {https://doi.org/10.1109/BELIV51497.2020.00009},
    venue           = {BELIV},
    abstract        = {Among the many changes brought about by the COVID-19 pandemic, one of the most pressing for scientific research concerns user testing. For the researchers who conduct studies with human participants, the requirements for social distancing have created a need for reflecting on methodologies that previously seemed relatively straightforward. It has become clear from the emerging literature on the topic and from first-hand experiences of researchers that the restrictions due to the pandemic affect every aspect of the research pipeline. The current paper offers an initial reflection on user-based research, drawing on the authors' own experiences and on the results of a survey that was conducted among researchers in different disciplines, primarily psychology, human-computer interaction (HCI), and visualization communities. While this sampling of researchers is by no means comprehensive, the multi-disciplinary approach and the consideration of different aspects of the research pipeline allow us to examine current and future challenges for user-based research. Through an exploration of these issues, this paper also invites others in the VIS-as well as in the wider-research community, to reflect on and discuss the ways in which the current crisis might also present new and previously unexplored opportunities.}
}
@inproceedings{cutura2020druidjs,
    title           = {DRUIDJS — A JavaScript Library for Dimensionality Reduction},
    author          = {Rene Cutura, Christoph Kralj, Michael Sedlmair},
    year            = {2020},
    month           = {10},
    booktitle       = {2020 IEEE Visualization Conference (VIS)},
    pages           = {111--115},
    doi             = {https://doi.org/10.1109/VIS47514.2020.00029},
    organization    = {IEEE},
    video           = {https://www.youtube.com/embed/LyiqHl4rq34?si=pzbugplHOM3pWJJf},
    suppl           = {https://renecutura.eu/pdfs/Druid_Supp.pdf},
    acks            = {This work was supported by the BMVIT ICT of the Future program via the ViSciPub project (no. 867378) and handled by the FFG.},
    pdf             = {https://renecutura.eu/pdfs/Druid.pdf},
    venue           = {VIS},
    abstract        = {Dimensionality reduction (DR) is a widely used technique for visualization. Nowadays, many of these visualizations are developed for the web, most commonly using JavaScript as the underlying programming language. So far, only few DR methods have a JavaScript implementation though, necessitating developers to write wrappers around implementations in other languages. In addition, those DR methods that exist in JavaScript libraries, such as PCA, t-SNE, and UMAP, do not offer consistent programming interfaces, hampering the quick integration of different methods. Toward a coherent and comprehensive DR programming framework, we developed an open source JavaScript library named DruidJS. Our library contains implementations of ten different DR algorithms, as well as the required linear algebra techniques, tools, and utilities.}
}
@misc{heyen2020supporting,
    title           = {Supporting Music Education through Visualizations of MIDI Recordings},
    author          = {Frank Heyen, Michael Sedlmair},
    year            = {2020},
    month           = {10},
    note            = {IEEE Visualization Conference Poster},
    abstract        = {Musicians mostly have to rely on their ears when they want to analyze what they play, for example to detect errors. Since hearing is sequential, it is not possible to quickly grasp an overview over one or multiple recordings of a whole piece of music at once. We therefore propose various visualizations that allow analyzing errors and stylistic variance. Our current approach focuses on rhythm and uses MIDI data for simplicity.},
    pdf             = {https://fheyen.github.io/files/ieee_vis_2020_poster/IEEE_VIS_Heyen_2020_Supporting.pdf},
    venue           = {VIS}
}
@article{weiss2020revisited,
    title           = {Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes},
    author          = {Maximilian Weiß, Katrin Angerbauer, Alexandra Voit, Magdalena Schwarzl, Michael Sedlmair, Sven Mayer},
    year            = {2020},
    month           = {10},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    volume          = {27},
    number          = {2},
    pages           = {1204--1213},
    doi             = {https://doi.org/10.1109/TVCG.2020.3030400},
    venue           = {VIS},
    abstract        = {Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach's primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.}
}
@article{bu2021sinestream,
    title           = {SineStream: Improving the Readability of Streamgraphs by Minimizing Sine Illusion Effects},
    author          = {Chuan Bu, Quanjie Zhang, Qianwen Wang, Jian Zhang, Oliver Deussen, Michael Sedlmair, Yunhai Wang},
    year            = {2020},
    month           = {10},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    volume          = {27},
    number          = {2},
    pages           = {1634--1643},
    doi             = {https://doi.org/10.1109/TVCG.2020.3030404},
    venue           = {TVCG},
    abstract        = {In this paper, we propose SineStream, a new variant of streamgraphs that improves their readability by minimizing sine illusion effects. Such effects reflect the tendency of humans to take the orthogonal rather than the vertical distance between two curves as their distance. In SineStream, we connect the readability of streamgraphs with minimizing sine illusions and by doing so provide a perceptual foundation for their design. As the geometry of a streamgraph is controlled by its baseline (the bottom-most curve) and the ordering of the layers, we re-interpret baseline computation and layer ordering algorithms in terms of reducing sine illusion effects. For baseline computation, we improve previous methods by introducing a Gaussian weight to penalize layers with large thickness changes. For layer ordering, three design requirements are proposed and implemented through a hierarchical clustering algorithm. Quantitative experiments and user studies demonstrate that SineStream improves the readability and aesthetics of streamgraphs compared to state-of-the-art methods.}
}
@article{kecheng2021palettaoilor,
    title           = {Palettailor: Discriminable Colorization for Categorical Data},
    author          = {Kecheng Lu, Mi Feng, Xin Chen, Michael Sedlmair, Oliver Deussen, Dani Lischinski, Zhanglin Cheng, Yunhai Wang},
    year            = {2020},
    month           = {10},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    volume          = {27},
    number          = {2},
    pages           = {475--484},
    doi             = {https://doi.org/10.1109/TVCG.2020.3030406},
    venue           = {TVCG},
    abstract        = {We present an integrated approach for creating and assigning color palettes to different visualizations such as multi-class scatterplots, line, and bar charts. While other methods separate the creation of colors from their assignment, our approach takes data characteristics into account to produce color palettes, which are then assigned in a way that fosters better visual discrimination of classes. To do so, we use a customized optimization based on simulated annealing to maximize the combination of three carefully designed color scoring functions: point distinctness, name difference, and color discrimination. We compare our approach to state-of-the-art palettes with a controlled user study for scatterplots and line charts, furthermore we performed a case study. Our results show that Palettailor, as a fully-automated approach, generates color palettes with a higher discrimination quality than existing approaches. The efficiency of our optimization allows us also to incorporate user modifications into the color selection process.}
}
@inproceedings{achberger2020caarvida,
    title           = {Caarvida: Visual Analytics for Test Drive Videos},
    author          = {Alexander Achberger, Rene Cutura, Oguzhan Türksoy, Michael Sedlmair},
    year            = {2020},
    month           = {09},
    booktitle       = {Proceedings of the International Conference on Advanced Visual Interfaces},
    location        = {Salerno, Italy},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {AVI '20},
    doi             = {https://doi.org/10.1145/3399715.3399862},
    isbn            = {9781450375351},
    abstract        = {We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.},
    articleno       = {6},
    numpages        = {9},
    keywords        = {visual analytics, object detection, automotive, information visualization, human computer interaction},
    venue           = {AVI}
}
@inproceedings{cutura2020comparing,
    title           = {Comparing and Exploring High-Dimensional Data with Dimensionality Reduction Algorithms and Matrix Visualizations},
    author          = {Rene Cutura, Michaël Aupetit, Jean-Daniel Fekete, Michael Sedlmair},
    year            = {2020},
    month           = {09},
    booktitle       = {Proc. Intl. Conf. on Advanced Visual Interfaces (AVI)},
    pages           = {1--9},
    doi             = {https://doi.org/10.1145/3399715.3399875},
    video           = {https://www.youtube.com/embed/UPkH7rc0ulU?si=UbZgql-ZjVD9CSzy},
    acks            = {This work was supported by the BMVIT ICT of the Future program via the ViSciPub project (no. 867378) and handled by the FFG.},
    pdf             = {https://renecutura.eu/pdfs/Compadre.pdf},
    venue           = {AVI},
    abstract        = {We propose Compadre, a tool for visual analysis for comparing distances of high-dimensional (HD) data and their low-dimensional projections. At the heart is a matrix visualization to represent the discrepancy between distance matrices, linked side-by-side with 2D scatterplot projections of the data. Using different examples and datasets, we illustrate how this approach fosters (1) evaluating dimensionality reduction techniques w.r.t. how well they project the HD data, (2) comparing them to each other side-by-side, and (3) evaluate important data features through subspace comparison. We also present a case study, in which we analyze IEEE VIS authors from 1990 to 2018, and gain new insights on the relationships between coauthors, citations, and keywords. The coauthors are projected as accurately with UMAP as with t-SNE but the projections show different insights. The structure of the citation subspace is very different from the coauthor subspace. The keyword subspace is noisy yet consistent among the three IEEE VIS sub-conferences.}
}
@inproceedings{heyen2020clavis,
    title           = {ClaVis: An Interactive Visual Comparison System for Classifiers},
    author          = {Frank Heyen, Tanja Munz, Michael Neumann, Daniel Ortega, Ngoc Thang Vu, Daniel Weiskopf, Michael Sedlmair},
    year            = {2020},
    month           = {09},
    booktitle       = {Proceedings of the International Conference on Advanced Visual Interfaces},
    location        = {Salerno, Italy},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {AVI '20},
    doi             = {https://doi.org/10.1145/3399715.3399814},
    isbn            = {9781450375351},
    abstract        = {We propose ClaVis, a visual analytics system for comparative analysis of classification models. ClaVis allows users to visually compare the performance and behavior of tens to hundreds of classifiers trained with different hyperparameter configurations. Our approach is plugin-based and classifier-agnostic and allows users to add their own datasets and classifier implementations. It provides multiple visualizations, including a multivariate ranking, a similarity map, a scatterplot that reveals correlations between parameters and scores, and a training history chart. We demonstrate the effectivity of our approach in multiple case studies for training classification models in the domain of natural language processing.},
    articleno       = {9},
    numpages        = {9},
    keywords        = {visual analytics, Visualization, machine learning, classifier comparison},
    suppl           = {https://github.com/fheyen/clavis},
    acks            = {Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (A08) and under Germany’s Excellence Strategy – EXC-2075 – 39074001},
    venue           = {AVI}
}
@article{schatz2020scivis,
    title           = {2019 IEEE Scientific Visualization Contest Winner: Visual Analysis of Structure Formation in Cosmic Evolution},
    author          = {Karsten Schatz, Christoph Müller, Patrick Gralka, Moritz Heinemann, Alexander Straub, Christoph Schulz, Matthias Braun, Tobias Rau, Michael Becher, Steffen Frey,  Guido Reina, Michael Sedlmair, others},
    year            = {2020},
    month           = {06},
    journal         = {IEEE Computer Graphics and Applications (CG\&A)},
    volume          = {41},
    number          = {6},
    pages           = {101--110},
    doi             = {https://doi.org/10.1109/MCG.2020.3004613},
    venue           = {CG&A},
    abstract        = {Simulations of cosmic evolution are a means to explain the formation of the universe as we see it today. The resulting data of such simulations comprise numerous physical quantities, which turns their analysis into a complex task. Here, we analyze such high-dimensional and time-varying particle data using various visualization techniques from the fields of particle visualization, flow visualization, volume visualization, and information visualization. Our approach employs specialized filters to extract and highlight the development of so-called active galactic nuclei and filament structures formed by the particles. Additionally, we calculate X-ray emission of the evolving structures in a preprocessing step to complement visual analysis. Our approach is integrated into a single visual analytics framework to allow for analysis of star formation at interactive frame rates. Finally, we lay out the methodological aspects of our work that led to success at the 2019 IEEE SciVis Contest.}
}
@inproceedings{oeney2020etra,
    title           = {Evaluation of Gaze Depth Estimation from Eye Tracking in Augmented Reality},
    author          = {Seyda Öney, Nils Rodrigues, Michael Becher, Guido Reina, Thomas Ertl, Michael Sedlmair, Daniel Weiskopf},
    year            = {2020},
    month           = {06},
    booktitle       = {ACM Symposium on Eye Tracking Research and Applications},
    location        = {Stuttgart, Germany},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {ETRA '20 Short Papers},
    doi             = {https://doi.org/10.1145/3379156.3391835},
    isbn            = {9781450371346},
    abstract        = {Gaze tracking in 3D has the potential to improve interaction with objects and visualizations in augmented reality. However, previous research showed that subjective perception of distance varies between real and virtual surroundings. We wanted to determine whether objectively measured 3D gaze depth through eye tracking also exhibits differences between entirely real and augmented environments. To this end, we conducted an experiment (N = 25) in which we used Microsoft HoloLens with a binocular eye tracking add-on from Pupil Labs. Participants performed a task that required them to look at stationary real and virtual objects while wearing a HoloLens device. We were not able to find significant differences in the gaze depth measured by eye tracking. Finally, we discuss our findings and their implications for gaze interaction in immersive analytics, and the quality of the collected gaze data.},
    articleno       = {49},
    numpages        = {5},
    keywords        = {eye tracking, visualization, Augmented reality, depth perception, immersive analytics, user study},
    venue           = {ETRA}
}
@inproceedings{pathmanathan2020etra,
    title           = {Eye vs. Head: Comparing Gaze Methods for Interaction in AR},
    author          = {Nelusa Pathmanathan, Michael Becher, Nils Rodrigues, Guido Reina, Thomas Ertl, Daniel Weiskopf, Michael Sedlmair},
    year            = {2020},
    month           = {06},
    booktitle       = {ACM Symposium on Eye Tracking Research and Applications},
    location        = {Stuttgart, Germany},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {ETRA '20 Short Papers},
    doi             = {https://doi.org/10.1145/3379156.3391829},
    isbn            = {9781450371346},
    abstract        = {Visualization in virtual 3D environments can provide a natural way for users to explore data. Often, arm and short head movements are required for interaction in augmented reality, which can be tiring and strenuous though. In an effort toward more user-friendly interaction, we developed a prototype that allows users to manipulate virtual objects using a combination of eye gaze and an external clicker device. Using this prototype, we performed a user study comparing four different input methods of which head gaze plus clicker was preferred by most participants.},
    articleno       = {50},
    numpages        = {5},
    keywords        = {Immersive analytics, interaction, visualization, eye tracking, augmented reality},
    venue           = {ETRA}
}
@inproceedings{streichert2020etra,
    title           = {Comparing Input Modalities for Shape Drawing Tasks},
    author          = {Annalena Streichert, Katrin Angerbauer, Magdalena Schwarzl, Michael Sedlmair},
    year            = {2020},
    month           = {06},
    booktitle       = {ACM Symposium on Eye Tracking Research and Applications},
    location        = {Stuttgart, Germany},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {ETRA '20 Short Papers},
    doi             = {https://doi.org/10.1145/3379156.3391830},
    isbn            = {9781450371346},
    abstract        = {With the growing interest in Immersive Analytics, there is also a need for novel and suitable input modalities for such applications. We explore eye tracking, head tracking, hand motion tracking, and data gloves as input methods for a 2D tracing task and compare them to touch input as a baseline in an exploratory user study (N= 20). We compare these methods in terms of user experience, workload, accuracy, and time required for input. The results show that the input method has a significant influence on these measured variables. While touch input surpasses all other input methods in terms of user experience, workload, and accuracy, eye tracking shows promise in respect of the input time. The results form a starting point for future research investigating input methods.},
    articleno       = {51},
    numpages        = {5},
    keywords        = {Immersive analytics, input modalities, interaction},
    venue           = {ETVIS}
}
@inproceedings{merino2020toward,
    title           = {Toward Agile Situated Visualization: An Exploratory User Study},
    author          = {Leonel Merino, Boris Sotomayor-Gómez, Xingyao Yu, Ronie Salgado, Alexandre Bergel, Michael Sedlmair, Daniel Weiskopf},
    year            = {2020},
    month           = {04},
    booktitle       = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
    location        = {Honolulu, HI, USA},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI EA '20},
    pages           = {1–7},
    doi             = {https://doi.org/10.1145/3334480.3383017},
    isbn            = {9781450368193},
    abstract        = {We introduce AVAR, a prototypical implementation of an agile situated visualization (SV) toolkit targeting liveness, integration, and expressiveness. We report on results of an exploratory study with AVAR and seven expert users. In it, participants wore a Microsoft HoloLens device and used a Bluetooth keyboard to program a visualization script for a given dataset. To support our analysis, we (i) video recorded sessions, (ii) tracked users' interactions, and (iii) collected data of participants' impressions. Our prototype confirms that agile SV is feasible. That is, liveness boosted participants' engagement when programming an SV, and so, the sessions were highly interactive and participants were willing to spend much time using our toolkit (i.e., median ≥ 1.5 hours). Participants used our integrated toolkit to deal with data transformations, visual mappings, and view transformations without leaving the immersive environment. Finally, participants benefited from our expressive toolkit and employed multiple of the available features when programming an SV.},
    numpages        = {7},
    keywords        = {situated visualization, augmented reality, user study},
    pdf             = {https://dl.acm.org/doi/pdf/10.1145/3334480.3383017},
    venue           = {CHI}
}
@inproceedings{kraus2020chi,
    title           = {Assessing 2D and 3D Heatmaps for Comparative Analysis: An Empirical Study},
    author          = {Matthias Kraus, Katrin Angerbauer, Juri Buchmüller, Daniel Schweitzer, Daniel A Keim, Michael Sedlmair, Johannes Fuchs},
    year            = {2020},
    month           = {04},
    booktitle       = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
    location        = {Honolulu, HI, USA},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI '20},
    pages           = {1–14},
    doi             = {https://doi.org/10.1145/3313831.3376675},
    isbn            = {9781450367080},
    abstract        = {Heatmaps are a popular visualization technique that encode 2D density distributions using color or brightness. Experimental studies have shown though that both of these visual variables are inaccurate when reading and comparing numeric data values. A potential remedy might be to use 3D heatmaps by introducing height as a third dimension to encode the data. Encoding abstract data in 3D, however, poses many problems, too. To better understand this tradeoff, we conducted an empirical study (N=48) to evaluate the user performance of 2D and 3D heatmaps for comparative analysis tasks. We test our conditions on a conventional 2D screen, but also in a virtual reality environment to allow for real stereoscopic vision. Our main results show that 3D heatmaps are superior in terms of error rate when reading and comparing single data items. However, for overview tasks, the well-established 2D heatmap performs better.},
    numpages        = {14},
    keywords        = {heatmaps, virtual reality, visual analytics},
    venue           = {CHI}
}
@inproceedings{kurzhals2020chi,
    title           = {A View on the Viewer: Gaze-Adaptive Captions for Videos},
    author          = {Kuno Kurzhals, Fabian Göbel, Katrin Angerbauer, Michael Sedlmair, Martin Raubal},
    year            = {2020},
    month           = {04},
    booktitle       = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
    location        = {Honolulu, HI, USA},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI '20},
    pages           = {1–12},
    doi             = {https://doi.org/10.1145/3313831.3376266},
    isbn            = {9781450367080},
    abstract        = {Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.},
    numpages        = {12},
    keywords        = {eye tracking, gaze input, gaze-responsive display, multimedia, subtitles, video captions},
    venue           = {CHI}
}
@inproceedings{bernard2020eurova,
    title           = {SepEx: Visual Analysis of Class Separation Measures},
    author          = {Jürgen Bernard, Marco Hutter, Matthias Zeppelzauer, Michael Sedlmair, Tamara Munzner},
    year            = {2020},
    month           = {01},
    booktitle       = {EuroVis Workshop on Visual Analytics (EuroVA)},
    publisher       = {The Eurographics Association},
    doi             = {https://doi.org/10.2312/eurova.20201079},
    isbn            = {978-3-03868-116-8},
    issn            = {2664-4487},
    editor          = {Turkay, Cagatay and Vrotsou, Katerina},
    venue           = {EuroVA},
    abstract        = {Class separation is an important concept in machine learning and visual analytics. However, the comparison of class separation for datasets with varying dimensionality is non-trivial, given a) the various possible structural characteristics of datasets and b) the plethora of separation measures that exist. Building upon recent findings in visualization research about the qualitative and quantitative evaluation of class separation for 2D dimensionally reduced data using scatterplots, this research addresses the visual analysis of class separation measures for high-dimensional data. We present SepEx, an interactive visualization approach for the assessment and comparison of class separation measures for multiple datasets. SepEx supports analysts with the comparison of multiple separation measures over many high-dimensional datasets, the effect of dimensionality reduction on measure outputs by supporting nD to 2D comparison, and the comparison of the effect of different dimensionality reduction methods on measure outputs. We demonstrate SepEx in a scenario on 100 two-class 5D datasets with a linearly increasing amount of separation between the classes, illustrating both similarities and nonlinearities across 11 measures.}
}
@article{wang2020scag,
    title           = {Improving the Robustness of Scagnostics},
    author          = {Yunhai Wang, Zeyu Wang, Tingting Liu, Michael Correll, Zhanglin Cheng, Oliver Deussen, Michael Sedlmair},
    year            = {2020},
    month           = {01},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    volume          = {26},
    number          = {1},
    pages           = {759--769},
    doi             = {https://doi.org/10.1109/TVCG.2019.2934796},
    venue           = {TVCG},
    abstract        = {In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.}
}
@inproceedings{aupetit2019vis,
    title           = {Toward Perception-based Evaluation of Clustering Techniques for Visual Analytics},
    author          = {Michael Aupetit, Michael Sedlmair, Mostafa Abbas, Abdelkader Baggag, Halima Bensmail},
    year            = {2019},
    month           = {12},
    booktitle       = {2019 IEEE Visualization Conference (VIS)},
    pages           = {141--145},
    doi             = {https://doi.org/10.1109/VISUAL.2019.8933620},
    venue           = {VIS},
    abstract        = {Automatic clustering techniques play a central role in Visual Analytics by helping analysts to discover interesting patterns in high-dimensional data. Evaluating these clustering techniques, however, is difficult due to the lack of universal ground truth. Instead, clustering approaches are usually evaluated based on a subjective visual judgment of low-dimensional scatterplots of different datasets. As clustering is an inherent human-in-the-loop task, we propose a more systematic way of evaluating clustering algorithms based on quantification of human perception of clusters in 2D scatterplots. The core question we are asking is in how far existing clustering techniques align with clusters perceived by humans. To do so, we build on a dataset from a previous study [1], in which 34 human subjects la-beled 1000 synthetic scatterplots in terms of whether they could see one or more than one cluster. Here, we use this dataset to benchmark state-of-the-art clustering techniques in terms of how far they agree with these human judgments. More specifically, we assess 1437 variants of K-means, Gaussian Mixture Models, CLIQUE, DBSCAN, and Agglomerative Clustering techniques on these benchmarks data. We get unexpected results. For instance, CLIQUE and DBSCAN are at best in slight agreement on this basic cluster counting task, while model-agnostic Agglomerative clustering can be up to a substantial agreement with human subjects depending on the variants. We discuss how to extend this perception-based clustering benchmark approach, and how it could lead to the design of perception-based clustering techniques that would better support more trustworthy and explainable models of cluster patterns.}
}
@inbook{brandes2019netvis,
    title           = {Network Visualization},
    author          = {Ulrik Brandes, Michael Sedlmair},
    year            = {2019},
    month           = {11},
    booktitle       = {Network Science: An Aerial View},
    publisher       = {Springer International Publishing},
    address         = {Cham},
    pages           = {5--21},
    doi             = {https://doi.org/10.1007/978-3-030-26814-5_2},
    isbn            = {978-3-030-26814-5},
    chapter         = {1},
    editor          = {Biagini, Francesca and Kauermann, Göran and Meyer-Brandis, Thilo},
    abstract        = {Data visualization is the art and science of mapping data to graphical variables. In this context, networks give rise to unique difficulties because of inherent dependencies among their elements. We provide a high-level overview of the main challenges and common techniques to address them. They are illustrated with examples from two application domains, social networks and automotive engineering. The chapter concludes with opportunities for future work in network visualization.},
    venue           = {In Network Science - An Aerial View, Springer}
}
@article{sippl2019tr,
    title           = {Collecting and Structuring Information in the Information Collage},
    author          = {Sebastian Sippl, Michael Sedlmair, Manuela Waldner},
    year            = {2019},
    month           = {09},
    journal         = {CoRR},
    volume          = {abs/1909.00608},
    doi             = {https://doi.org/10.48550/arXiv.1909.00608},
    url             = {http://arxiv.org/abs/1909.00608},
    eprinttype      = {arXiv},
    eprint          = {1909.00608},
    venue           = {arXiv},
    abstract        = {Knowledge workers, such as scientists, journalists, or consultants, adaptively seek, gather, and consume information. These processes are often inefficient as existing user interfaces provide limited possibilities to combine information from various sources and different formats into a common knowledge representation. In this paper, we present the concept of an information collage (IC) - a web browser extension combining manual spatial organization of gathered information fragments and automatic text analysis for interactive content exploration and expressive visual summaries. We used IC for case studies with knowledge workers from different domains and longer-term field studies over a period of one month. We identified three different ways how users collect and structure information and provide design recommendations how to support these observed usage strategies.}
}
@inproceedings{sperrle2018speculative,
    title           = {Speculative Execution for Guided Visual Analytics},
    author          = {Fabian Sperrle, Jürgen Bernard, Michael Sedlmair, Daniel Keim, Mennatallah El-Assady},
    year            = {2018},
    month           = {10},
    booktitle       = {Workshop on Machine Learning from User Interaction for Visualization and Analytics},
    volume          = {abs/1908.02627},
    doi             = {https://doi.org/10.1109/MLUI52768.2018.10075559},
    url             = {http://arxiv.org/abs/1908.02627},
    venue           = {MLUI},
    abstract        = {We propose the concept of Speculative Execution for Visual Analytics and discuss its effectiveness for model exploration and optimization. Speculative Execution enables the automatic generation of alternative, competing model configurations that do not alter the current model state unless explicitly confirmed by the user. These alternatives are computed based on either user interactions or model quality measures and can be explored using delta-visualizations. By automatically proposing modeling alternatives, systems employing Speculative Execution can shorten the gap between users and models, reduce the confirmation bias and speed up optimization processes. In this paper, we have assembled five application scenarios showcasing the potential of Speculative Execution, as well as a potential for further research.}
}
@article{stoiber2019eurovis,
    title           = {netflower: Dynamic Network Visualization for Data Journalists},
    author          = {Christina Stoiber, Alexander Rind, Florian Grassinger, Robert Gutounig, Eva Goldgruber, Michael Sedlmair, Stefan Emrich, Wolfgang Aigner},
    year            = {2019},
    month           = {07},
    journal         = {Computer Graphics Forum},
    volume          = {38},
    number          = {3},
    pages           = {699--711},
    doi             = {https://doi.org/10.1111/cgf.13721},
    keywords        = {CCS Concepts, • Human-centered computing → Visualization design and evaluation methods},
    abstract        = {Journalists need visual interfaces that cater to the exploratory nature of their investigative activities. In this paper, we report on a four-year design study with data journalists. The main result is netflower, a visual exploration tool that supports journalists in investigating quantitative flows in dynamic network data for story-finding. The visual metaphor is based on Sankey diagrams and has been extended to make it capable of processing large amounts of input data as well as network change over time. We followed a structured, iterative design process including requirement analysis and multiple design and prototyping iterations in close cooperation with journalists. To validate our concept and prototype, a workshop series and two diary studies were conducted with journalists. Our findings indicate that the prototype can be picked up quickly by journalists and valuable insights can be achieved in a few hours. The prototype can be accessed at: http://netflower.fhstp.ac.at/},
    pdf             = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13721},
    venue           = {CGF}
}
@article{abbas2019eurovis,
    title           = {ClustMe: A Visual Quality Measure for Ranking Monochrome Scatterplots based on Cluster Patterns},
    author          = {Mostafa Abbas, Michael Aupetit, Michael Sedlmair, Halima Bensmail},
    year            = {2019},
    month           = {07},
    journal         = {Computer Graphics Forum},
    volume          = {38},
    number          = {3},
    pages           = {225--236},
    doi             = {https://doi.org/10.1111/cgf.13684},
    keywords        = {CCS Concepts, • Human-centered computing → Visual analytics; Empirical studies in visualization, • Computing methodologies → Cluster analysis; Mixture modeling},
    abstract        = {We propose ClustMe, a new visual quality measure to rank monochrome scatterplots based on cluster patterns. ClustMe is based on data collected from a human-subjects study, in which 34 participants judged synthetically generated cluster patterns in 1000 scatterplots. We generated these patterns by carefully varying the free parameters of a simple Gaussian Mixture Model with two components, and asked the participants to count the number of clusters they could see (1 or more than 1). Based on the results, we form ClustMe by selecting the model that best predicts these human judgments among 7 different state-of-the-art merging techniques (Demp). To quantitatively evaluate ClustMe, we conducted a second study, in which 31 human subjects ranked 435 pairs of scatterplots of real and synthetic data in terms of cluster patterns complexity. We use this data to compare ClustMe's performance to 4 other state-of-the-art clustering measures, including the well-known Clumpiness scagnostics. We found that of all measures, ClustMe is in strongest agreement with the human rankings.},
    venue           = {CGF}
}
@inproceedings{zhao2019barperception,
    title           = {Neighborhood Perception in Bar Charts},
    author          = {Mingqian Zhao, Huamin Qu, Michael Sedlmair},
    year            = {2019},
    month           = {05},
    booktitle       = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
    location        = {Glasgow, Scotland Uk},
    publisher       = {Association for Computing Machinery},
    address         = {New York, NY, USA},
    series          = {CHI '19},
    pages           = {1–12},
    doi             = {https://doi.org/10.1145/3290605.3300462},
    isbn            = {9781450359702},
    abstract        = {In this paper, we report three user experiments that investigate in how far the perception of a bar in a bar chart changes based on the height of its neighboring bars. We hypothesized that the perception of the very same bar, for instance, might differ when it is surrounded by the top highest vs. the top lowest bars. Our results show that such neighborhood effects exist: a target bar surrounded by high neighbor bars, is perceived to be lower as the same bar surrounded with low neighbors. Yet, the effect size of this neighborhood effect is small compared to other data-inherent effects: the judgment accuracy largely depends on the target bar rank, number of data items, and other data characteristics of the dataset. Based on the findings, we discuss design implications for perceptually optimizing bar charts.},
    numpages        = {12},
    keywords        = {visualization, empirical study that tells us about people},
    venue           = {CHI}
}
@inproceedings{schulz2019deficiencySim,
    title           = {A Framework for Pervasive Visual Deficiency Simulation},
    author          = {Christoph Schulz, Nils Rodrigues, Marco Amann, Daniel Baumgartner, Arman Mielke, Christian Baumann, Michael Sedlmair, Daniel Weiskopf},
    year            = {2019},
    month           = {03},
    booktitle       = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
    pages           = {1--6},
    doi             = {https://doi.org/10.1109/VR44988.2019.9044164},
    venue           = {VR},
    abstract        = {We present a framework for rapid prototyping of pervasive visual deficiency simulation in the context of graphical interfaces, virtual reality, and augmented reality. Our framework facilitates the emulation of various visual deficiencies for a wide range of applications, which allows users with normal vision to experience combinations of conditions such as myopia, hyperopia, presbyopia, cataract, nyctalopia, protanopia, deuteranopia, tritanopia, and achromatopsia. Our framework provides an infrastructure to encourage researchers to evaluate visualization and other display techniques regarding visual deficiencies, and opens up the field of visual disease simulation to a broader audience. The benefits of our framework are easy integration, configuration, fast prototyping, and portability to new emerging hardware. To demonstrate the applicability of our framework, we showcase a desktop application and an Android application that transform commodity hardware into glasses for visual deficiency simulation. We expect that this work promotes a greater understanding of visual impairments, leads to better product design for the visually impaired, and forms a basis for research to compensate for these impairments as everyday help.}
}
@inproceedings{bernard2019eurova,
    title           = {Visual Analysis of Degree-of-Interest Functions to Support Selection Strategies for Instance Labeling},
    author          = {Jürgen Bernard, Marco Hutter, Christian Ritter, Markus Lehmann, Michael Sedlmair, Matthias Zeppelzauer},
    year            = {2019},
    month           = {01},
    booktitle       = {EuroVis Workshop on Visual Analytics (EuroVA)},
    publisher       = {The Eurographics Association},
    doi             = {https://doi.org/10.2312/eurova.20191116},
    isbn            = {978-3-03868-087-1},
    editor          = {Landesberger, Tatiana von and Turkay, Cagatay},
    venue           = {EuroVA},
    abstract        = {Manually labeling data sets is a time-consuming and expensive task that can be accelerated by interactive machine learning and visual analytics approaches. At the core of these approaches are strategies for the selection of candidate instances to label. We introduce degree-of-interest (DOI) functions as atomic building blocks to formalize candidate selection strategies. We introduce a taxonomy of DOI functions and an approach for the visual analysis of DOI functions, which provide novel complementary views on labeling strategies and DOIs, support their in-depth analysis and facilitate their interpretation. Our method shall support the generation of novel and better explanation of existing labeling strategies in future.}
}
@inproceedings{elassady2018ltma,
    title           = {LTMA: Layered Topic Matching for the Comparative Exploration, Evaluation, and Refinement of Topic Modeling Results},
    author          = {Mennatallah El-Assady, Fabian Sperrle, Rita Sevastjanova, Michael Sedlmair, Daniel Keim},
    year            = {2018},
    month           = {11},
    booktitle       = {2018 International Symposium on Big Data Visual and Immersive Analytics (BDVA)},
    pages           = {1--10},
    doi             = {https://doi.org/10.1109/BDVA.2018.8534018},
    venue           = {BDVA},
    abstract        = {We present LTMA, a Layered Topic Matching approach for the unsupervised comparative analysis of topic modeling results. Due to the vast number of available modeling algorithms, an efficient and effective comparison of their results is detrimental to a data- and task-driven selection of a model. LTMA automates this comparative analysis by providing topic matching based on two layers (document-overlap and keyword-similarity), creating a novel topic-match data structure. This data structure builds a basis for model exploration and optimization, thus, allowing for an efficient evaluation of their performance in the context of a given type of text data and task. This is especially important for text types where an annotated gold standard dataset is not readily available and, therefore, quantitative evaluation methods are not applicable. We confirm the usefulness of our technique based on three use cases, namely: (1) the automatic comparative evaluation of topic models, (2) the visual exploration of topic modeling differences, and (3) the optimization of topic modeling results through combining matches.}
}
@inbook{calero2018bias,
    title           = {Studying Biases in Visualization Research: Framework and Methods},
    author          = {André Calero Valdez, Martina Ziefle, Michael Sedlmair},
    year            = {2018},
    month           = {09},
    booktitle       = {Cognitive Biases in Visualizations},
    publisher       = {Springer International Publishing},
    address         = {Cham},
    pages           = {13--27},
    doi             = {https://doi.org/10.1007/978-3-319-95831-6_2},
    isbn            = {978-3-319-95831-6},
    chapter         = {2},
    editor          = {Ellis, Geoffrey},
    abstract        = {In this chapter, we propose and discuss a lightweight framework to help organize research questions that arise around biases in visualization and visual analysis. We contrast our framework against the cognitive bias codex by Buster Benson. The framework is inspired by Norman’s Human Action Cycle and classifies biases into three levels: perceptual biases, action biases, and social biases. For each of the levels of cognitive processing, we discuss examples of biases from the cognitive science literature and speculate how they might also be important to the area of visualization. In addition, we put forward a methodological discussion on how biases might be studied on all three levels, and which pitfalls and threats to validity exist. We hope that the framework will help spark new ideas and guide researchers that study the important topic of biases in visualization.},
    venue           = {Cognitive Biases in Visualizations, Springer}
}
@article{wang2019fisheye,
    title           = {Structure-aware Fisheye Views for Efficient Large Graph Exploration},
    author          = {Yunhai Wang, Yanyan Wang, Haifeng Zhang, Yinqi Sun, Chi-Wing Fu, Michael Sedlmair, Baoquan Chen, Oliver Deussen},
    year            = {2018},
    month           = {08},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    volume          = {25},
    number          = {1},
    pages           = {566--575},
    doi             = {https://doi.org/10.1109/TVCG.2018.2864911},
    venue           = {TVCG},
    abstract        = {Traditional fisheye views for exploring large graphs introduce substantial distortions that often lead to a decreased readability of paths and other interesting structures. To overcome these problems, we propose a framework for structure-aware fisheye views. Using edge orientations as constraints for graph layout optimization allows us not only to reduce spatial and temporal distortions during fisheye zooms, but also to improve the readability of the graph structure. Furthermore, the framework enables us to optimize fisheye lenses towards specific tasks and design a family of new lenses: polyfocal, cluster, and path lenses. A GPU implementation lets us process large graphs with up to 15,000 nodes at interactive rates. A comprehensive evaluation, a user study, and two case studies demonstrate that our structure-aware fisheye views improve layout readability and user performance.}
}
@article{wang2019color,
    title           = {Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots},
    author          = {Yunhai Wang, Xin Chen, Tong Ge, Chen Bao, Michael Sedlmair, Chi-Wing Fu, Oliver Deussen, Baoquan Chen},
    year            = {2018},
    month           = {08},
    journal         = {IEEE Transactions on Visualization and Computer Graphics},
    volume          = {25},
    number          = {1},
    pages           = {820--829},
    doi             = {https://doi.org/10.1109/TVCG.2018.2864912},
    venue           = {TVCG},
    abstract        = {Appropriate choice of colors significantly aids viewers in understanding the structures in multiclass scatterplots and becomes more important with a growing number of data points and groups. An appropriate color mapping is also an important parameter for the creation of an aesthetically pleasing scatterplot. Currently, users of visualization software routinely rely on color mappings that have been pre-defined by the software. A default color mapping, however, cannot ensure an optimal perceptual separability between groups, and sometimes may even lead to a misinterpretation of the data. In this paper, we present an effective approach for color assignment based on a set of given colors that is designed to optimize the perception of scatterplots. Our approach takes into account the spatial relationships, density, degree of overlap between point clusters, and also the background color. For this purpose, we use a genetic algorithm that is able to efficiently find good color assignments. We implemented an interactive color assignment system with three extensions of the basic method that incorporates top K suggestions, user-defined color subsets, and classes of interest for the optimization. To demonstrate the effectiveness of our assignment technique, we conducted a numerical study and a controlled user study to compare our approach with default color assignments; our findings were verified by two expert studies. The results show that our approach is able to support users in distinguishing cluster numbers faster and more precisely than default assignment methods.}
}
@article{bernard2018eurovis,
    title           = {Towards User-Centered Active Learning Algorithms},
    author          = {Jürgen Bernard, Matthias Zeppelzauer, Markus Lehmann, Martin Müller, Michael Sedlmair},
    year            = {2018},
    month           = {07},
    journal         = {Computer Graphics Forum},
    volume          = {37},
    number          = {3},
    pages           = {121--132},
    doi             = {https://doi.org/10.1111/cgf.13406},
    keywords        = {Categories and Subject Descriptors (according to ACM CCS), I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation},
    eprint          = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13406},
    abstract        = {The labeling of data sets is a time-consuming task, which is, however, an important prerequisite for machine learning and visual analytics. Visual-interactive labeling (VIAL) provides users an active role in the process of labeling, with the goal to combine the potentials of humans and machines to make labeling more efficient. Recent experiments showed that users apply different strategies when selecting instances for labeling with visual-interactive interfaces. In this paper, we contribute a systematic quantitative analysis of such user strategies. We identify computational building blocks of user strategies, formalize them, and investigate their potentials for different machine learning tasks in systematic experiments. The core insights of our experiments are as follows. First, we identified that particular user strategies can be used to considerably mitigate the bootstrap (cold start) problem in early labeling phases. Second, we observed that they have the potential to outperform existing active learning strategies in later phases. Third, we analyzed the identified core building blocks, which can serve as the basis for novel selection strategies. Overall, we observed that data-based user strategies (clusters, dense areas) work considerably well in early phases, while model-based user strategies (e.g., class separation) perform better during later phases. The insights gained from this work can be applied to develop novel active learning approaches as well as to better guide users in visual interactive labeling.},
    venue           = {CGF}
}
@article{torsneyweir2018eurovis,
    title           = {Hypersliceplorer: Interactive Visualization of Shapes in Multiple Dimensions},
    author          = {Thomas Torsney-Weir, Torsten Möller, Michael Sedlmair, Mike Kirby},
    year            = {2018},
    month           = {07},
    journal         = {Computer Graphics Forum},
    volume          = {37},
    number          = {3},
    pages           = {229--240},
    doi             = {https://doi.org/10.1111/cgf.13415},
    keywords        = {Categories and Subject Descriptors (according to ACM CCS), I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation},
    eprint          = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13415},
    abstract        = {In this paper we present Hypersliceplorer, an algorithm for generating 2D slices of multi-dimensional shapes defined by a simplical mesh. Often, slices are generated by using a parametric form and then constraining parameters to view the slice. In our case, we developed an algorithm to slice a simplical mesh of any number of dimensions with a two-dimensional slice. In order to get a global appreciation of the multi-dimensional object, we show multiple slices by sampling a number of different slicing points and projecting the slices into a single view per dimension pair. These slices are shown in an interactive viewer which can switch between a global view (all slices) and a local view (single slice). We show how this method can be used to study regular polytopes, differences between spaces of polynomials, and multi-objective optimization surfaces.},
    venue           = {CGF}
}